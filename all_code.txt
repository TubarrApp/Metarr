package cfg

import (
	keys "metarr/internal/domain/keys"

	"github.com/spf13/viper"
)

// initFilesDirs initializes user flag settings for input files and directories
func initFilesDirs() {

	// Batch
	rootCmd.PersistentFlags().StringSlice(keys.BatchPairsInput, nil, "Pairs of video and JSON directories (e.g. '/videodir:/metadir')")
	viper.BindPFlag(keys.BatchPairsInput, rootCmd.PersistentFlags().Lookup(keys.BatchPairsInput))

	// Videos
	rootCmd.PersistentFlags().StringSliceP(keys.VideoDirs, "v", nil, "A directory containing videos")
	viper.BindPFlag(keys.VideoDirs, rootCmd.PersistentFlags().Lookup(keys.VideoDirs))

	rootCmd.PersistentFlags().StringSliceP(keys.VideoFiles, "V", nil, "A video file")
	viper.BindPFlag(keys.VideoFiles, rootCmd.PersistentFlags().Lookup(keys.VideoFiles))

	// JSON
	rootCmd.PersistentFlags().StringSliceP(keys.JsonDirs, "j", nil, "A directory containing videos")
	viper.BindPFlag(keys.JsonDirs, rootCmd.PersistentFlags().Lookup(keys.JsonDirs))

	rootCmd.PersistentFlags().StringSliceP(keys.JsonFiles, "J", nil, "A directory containing videos")
	viper.BindPFlag(keys.JsonFiles, rootCmd.PersistentFlags().Lookup(keys.JsonFiles))

	// Cookies
	rootCmd.PersistentFlags().String(keys.CookiePath, "", "Specify cookie location")
	viper.BindPFlag(keys.CookiePath, rootCmd.PersistentFlags().Lookup(keys.CookiePath))
}

// initResourceRelated initializes user flag settings for parameters related to system hardware
func initResourceRelated() {

	// Concurrency limit
	rootCmd.PersistentFlags().IntP(keys.Concurrency, "l", 5, "Max concurrency limit")
	viper.BindPFlag(keys.Concurrency, rootCmd.PersistentFlags().Lookup(keys.Concurrency))

	// CPU usage
	rootCmd.PersistentFlags().Float64P(keys.MaxCPU, "c", 100.0, "Max CPU usage")
	viper.BindPFlag(keys.MaxCPU, rootCmd.PersistentFlags().Lookup(keys.MaxCPU))

	// Min memory
	rootCmd.PersistentFlags().Uint64P(keys.MinMem, "m", 0, "Minimum RAM to start process")
	viper.BindPFlag(keys.MinMem, rootCmd.PersistentFlags().Lookup(keys.MinMem))

	// Hardware accelerated transcoding
	rootCmd.PersistentFlags().StringP(keys.GPU, "g", "none", "GPU acceleration type (nvidia, amd, intel, none)")
	viper.BindPFlag(keys.GPU, rootCmd.PersistentFlags().Lookup(keys.GPU))
}

// initAllFileTransformers initializes user flag settings for transformations applying to all files
func initAllFileTransformers() {

	// Prefix file with metafield
	rootCmd.PersistentFlags().StringSlice(keys.MFilenamePfx, nil, "Adds a specified metatag's value onto the start of the filename")
	viper.BindPFlag(keys.MFilenamePfx, rootCmd.PersistentFlags().Lookup(keys.MFilenamePfx))

	// Prefix files with date tag
	rootCmd.PersistentFlags().String(keys.InputFileDatePfx, "", "Looks for dates in metadata to prefix the video with. (date:format [e.g. Ymd for yyyy-mm-dd])")
	viper.BindPFlag(keys.InputFileDatePfx, rootCmd.PersistentFlags().Lookup(keys.InputFileDatePfx))

	// Rename convention
	rootCmd.PersistentFlags().StringP(keys.RenameStyle, "r", "skip", "Rename flag (spaces, underscores, fixes-only, or skip)")
	viper.BindPFlag(keys.RenameStyle, rootCmd.PersistentFlags().Lookup(keys.RenameStyle))

	// Replace filename suffix
	rootCmd.PersistentFlags().StringSliceVar(&filenameReplaceSuffixInput, keys.InputFilenameReplaceSfx, nil, "Replaces a specified suffix on filenames. (suffix:replacement)")
	viper.BindPFlag(keys.InputFilenameReplaceSfx, rootCmd.PersistentFlags().Lookup(keys.InputFilenameReplaceSfx))

	// Backup files by renaming original files
	rootCmd.PersistentFlags().BoolP(keys.NoFileOverwrite, "n", false, "Renames the original files to avoid overwriting")
	viper.BindPFlag(keys.NoFileOverwrite, rootCmd.PersistentFlags().Lookup(keys.NoFileOverwrite))

	// Output directory (can be external)
	rootCmd.PersistentFlags().StringP(keys.MoveOnComplete, "o", "", "Move files to given directory on program completion")
	viper.BindPFlag(keys.MoveOnComplete, rootCmd.PersistentFlags().Lookup(keys.MoveOnComplete))
}

// initMetaTransformers initializes user flag settings for manipulation of metadata
func initMetaTransformers() {

	// Metadata transformations
	rootCmd.PersistentFlags().StringSlice(keys.MetaOps, nil, "Metadata operations (field:operation:value) - e.g. title:set:New Title, description:prefix:Draft-, tags:append:newtag")
	viper.BindPFlag(keys.MetaOps, rootCmd.PersistentFlags().Lookup(keys.MetaOps))

	// Prefix or append description fields with dates
	rootCmd.PersistentFlags().Bool(keys.MDescDatePfx, false, "Adds the date to the start of the description field.")
	viper.BindPFlag(keys.MDescDatePfx, rootCmd.PersistentFlags().Lookup(keys.MDescDatePfx))

	rootCmd.PersistentFlags().Bool(keys.MDescDateSfx, false, "Adds the date to the end of the description field.")
	viper.BindPFlag(keys.MDescDateSfx, rootCmd.PersistentFlags().Lookup(keys.MDescDateSfx))

	// Overwrite or preserve metafields
	rootCmd.PersistentFlags().Bool(keys.MOverwrite, false, "When adding new metadata fields, automatically overwrite existing fields with your new values")
	viper.BindPFlag(keys.MOverwrite, rootCmd.PersistentFlags().Lookup(keys.MOverwrite))

	rootCmd.PersistentFlags().Bool(keys.MPreserve, false, "When adding new metadata fields, skip already existent fields")
	viper.BindPFlag(keys.MPreserve, rootCmd.PersistentFlags().Lookup(keys.MPreserve))

	rootCmd.PersistentFlags().String(keys.MetaPurge, "", "Delete metadata files (e.g. .json, .nfo) after the video is successfully processed")
	viper.BindPFlag(keys.MetaPurge, rootCmd.PersistentFlags().Lookup(keys.MetaPurge))
}

// initVideoTransformers initializes user flag settings for transformation of video files
func initVideoTransformers() {

	// Output extension type
	rootCmd.PersistentFlags().String(keys.OutputFiletypeInput, "", "File extension to output files as (mp4 works best for most media servers)")
	viper.BindPFlag(keys.OutputFiletypeInput, rootCmd.PersistentFlags().Lookup(keys.OutputFiletypeInput))
}

// initFiltering initializes user flag settings for filtering files to work with
func initFiltering() {

	// Video file extensions to convert
	rootCmd.PersistentFlags().StringSliceP(keys.InputVideoExts, "e", []string{"all"}, "File extensions to convert (all, mkv, mp4, webm)")
	viper.BindPFlag(keys.InputVideoExts, rootCmd.PersistentFlags().Lookup(keys.InputVideoExts))

	// Meta file extensions to convert
	rootCmd.PersistentFlags().StringSlice(keys.InputMetaExts, []string{"all"}, "File extensions to convert (all, json, nfo)")
	viper.BindPFlag(keys.InputMetaExts, rootCmd.PersistentFlags().Lookup(keys.InputMetaExts))

	// Only convert files with prefix
	rootCmd.PersistentFlags().StringSliceP(keys.FilePrefixes, "p", []string{""}, "Filters files by prefixes")
	viper.BindPFlag(keys.FilePrefixes, rootCmd.PersistentFlags().Lookup(keys.FilePrefixes))
}

// initProgramFunctions initializes user flag settings for miscellaneous program features such as debug level
func initProgramFunctions() {

	// Debugging level
	rootCmd.PersistentFlags().Int(keys.DebugLevel, 0, "Level of debugging (0 - 5)")
	viper.BindPFlag(keys.DebugLevel, rootCmd.PersistentFlags().Lookup(keys.DebugLevel))

	// Skip videos, only alter metafiles
	rootCmd.PersistentFlags().Bool(keys.SkipVideos, false, "Skips compiling/transcoding the videos and just edits the file names/JSON file fields")
	viper.BindPFlag(keys.SkipVideos, rootCmd.PersistentFlags().Lookup(keys.SkipVideos))

	// Preset configurations for sites
	rootCmd.PersistentFlags().String(keys.InputPreset, "", "Use a preset configuration (e.g. censoredtv)")
	viper.BindPFlag(keys.InputPreset, rootCmd.PersistentFlags().Lookup(keys.InputPreset))

	// Output benchmarking files
	rootCmd.PersistentFlags().Bool(keys.Benchmarking, false, "Benchmarks the program")
	viper.BindPFlag(keys.Benchmarking, rootCmd.PersistentFlags().Lookup(keys.Benchmarking))
}
package cfg

import (
	"fmt"
	"metarr/internal/domain/consts"
	"metarr/internal/domain/enums"
	"metarr/internal/domain/keys"
	"metarr/internal/models"
	"metarr/internal/utils/logging"
	"os"
	"strings"

	"github.com/shirou/gopsutil/mem"
	"github.com/spf13/cobra"
	"github.com/spf13/viper"
)

var rootCmd = &cobra.Command{
	Use:   "metarr",
	Short: "metarr is a video and metatagging tool",
	RunE: func(cmd *cobra.Command, args []string) error {
		if cmd.Flags().Lookup("help").Changed {
			return nil // Stop further execution if help is invoked
		}
		viper.Set("execute", true)
		return execute()
	},
}

// init sets the initial Viper settings
func init() {

	// Files and directories
	initFilesDirs()

	// System resource related
	initResourceRelated()

	// Filtering
	initFiltering()

	// All file transformations
	initAllFileTransformers()

	// Filename transformations
	initVideoTransformers()

	// Metadata and metafile manipulation
	initMetaTransformers()

	// Special functions
	initProgramFunctions()

	// Text replacement initialization
	initTextReplace()
}

// Execute is the primary initializer of Viper
func Execute() error {

	fmt.Println()

	err := rootCmd.Execute()
	if err != nil {
		logging.E(0, "Failed to execute cobra")
		return err

	}
	return nil
}

// execute more thoroughly handles settings created in the Viper init
func execute() error {

	// Parse GPU settings and set commands
	verifyHWAcceleration()

	// Concurrency
	verifyConcurrencyLimit()

	// Resource usage limits (CPU and memory)
	verifyResourceLimits()

	// File extension settings
	verifyInputFiletypes()

	// File prefix filter settings
	verifyFilePrefixes()

	// Debugging level
	verifyDebugLevel()

	// Filetype to output as
	verifyOutputFiletype()

	// Meta overwrite and preserve flags
	verifyMetaOverwritePreserve()

	// Verify user metafile purge settings
	verifyPurgeMetafiles()

	// Ensure no video and metadata location conflicts
	if err := checkFileDirs(); err != nil {
		return err
	}

	logging.D(1, "Initializing text replace")
	if err := initTextReplace(); err != nil {
		return err
	}

	if err := initDateReplaceFormat(); err != nil {
		return err
	}

	return nil
}

// checkFileDirConflicts ensures no conflicts in the file and directories entered by the user
func checkFileDirs() error {

	var (
		videoFiles, videoDirs,
		jsonFiles, jsonDirs []string
	)

	videoFileSet := viper.IsSet(keys.VideoFiles)
	videoDirSet := viper.IsSet(keys.VideoDirs)
	jsonFileSet := viper.IsSet(keys.JsonFiles)
	jsonDirSet := viper.IsSet(keys.JsonDirs)

	if videoFileSet {
		videoFiles = viper.GetStringSlice(keys.VideoFiles)
	}

	if videoDirSet {
		videoDirs = viper.GetStringSlice(keys.VideoDirs)
	}

	if jsonFileSet {
		jsonFiles = viper.GetStringSlice(keys.JsonFiles)
	}

	if jsonDirSet {
		jsonDirs = viper.GetStringSlice(keys.JsonDirs)
	}

	if len(videoDirs) > len(jsonDirs) || len(videoFiles) > len(jsonFiles) {
		return fmt.Errorf("invalid configuration, please enter a meta directory/file for each video directory/file")
	}

	var tasks []models.Batch

	vDirCount := 0
	vFileCount := 0

	logging.I("Finding video and JSON directories...")

	// Make directory batches
	if len(videoDirs) > 0 {
		for i := range videoDirs {
			vInfo, err := os.Stat(videoDirs[i])
			if err != nil {
				return err
			}
			if !vInfo.IsDir() {
				return fmt.Errorf("file %q entered instead of directory", vInfo.Name())
			}

			jInfo, err := os.Stat(jsonDirs[i])
			if err != nil {
				return err
			}
			if !jInfo.IsDir() {
				return fmt.Errorf("file %q entered instead of directory", jInfo.Name())
			}

			tasks = append(tasks, models.Batch{
				Video:  videoDirs[i],
				Json:   jsonDirs[i],
				IsDirs: true,
			})
			vDirCount++
		}
	}

	logging.I("Got %d directory pairs to process, %d singular JSON directories", vDirCount, len(jsonDirs)-vDirCount)

	// Remnant JSON directories
	if len(jsonDirs) > vDirCount {
		j := jsonDirs[vDirCount:]

		for i := range j {
			jInfo, err := os.Stat(j[i])
			if err != nil {
				return err
			}
			if !jInfo.IsDir() {
				return fmt.Errorf("file %q entered instead of directory", jInfo.Name())
			}

			tasks = append(tasks, models.Batch{
				Json:       j[i],
				IsDirs:     true,
				SkipVideos: true,
			})
		}
	}

	logging.I("Finding video and JSON files...")

	// Make file batches
	if len(videoFiles) > 0 {
		for i := range videoFiles {
			vInfo, err := os.Stat(videoFiles[i])
			if err != nil {
				return err
			}
			if vInfo.IsDir() {
				return fmt.Errorf("directory %q entered instead of file", vInfo.Name())
			}

			jInfo, err := os.Stat(jsonFiles[i])
			if err != nil {
				return err
			}
			if jInfo.IsDir() {
				return fmt.Errorf("directory %q entered instead of file", jInfo.Name())
			}

			tasks = append(tasks, models.Batch{
				Video:  videoFiles[i],
				Json:   jsonFiles[i],
				IsDirs: false,
			})
			vFileCount++
		}

		logging.I("Got %d file pairs to process, %d singular JSON files", vFileCount, len(jsonFiles)-len(videoFiles))

		// Remnant JSON files
		if len(jsonFiles) > vFileCount {
			j := jsonFiles[vFileCount-1:]

			for i := range j {
				jInfo, err := os.Stat(j[i])
				if err != nil {
					return err
				}
				if jInfo.IsDir() {
					return fmt.Errorf("directory %q entered instead of file", jInfo.Name())
				}

				tasks = append(tasks, models.Batch{
					Json:       j[i],
					IsDirs:     false,
					SkipVideos: true,
				})
			}
		}
	}

	logging.I("Got %d batch jobs to perform.", len(tasks))
	viper.Set(keys.BatchPairs, tasks)

	return nil
}

// verifyFilePrefixes checks and sets the file prefix filters
func verifyFilePrefixes() {
	if !viper.IsSet(keys.FilePrefixes) {
		return
	}

	argsInputPrefixes := viper.GetStringSlice(keys.FilePrefixes)
	filePrefixes := make([]string, 0, len(argsInputPrefixes))

	for _, arg := range argsInputPrefixes {
		if arg != "" {
			filePrefixes = append(filePrefixes, arg)
		}
	}
	if len(filePrefixes) > 0 {
		viper.Set(keys.FilePrefixes, filePrefixes)
	}
}

// verifyMetaOverwritePreserve checks if the entered meta overwrite and preserve flags are valid
func verifyMetaOverwritePreserve() {
	if GetBool(keys.MOverwrite) && GetBool(keys.MPreserve) {
		logging.E(0, "Cannot enter both meta preserve AND meta overwrite, exiting...")
		os.Exit(1)
	}
}

// verifyDebugLevel checks and sets the debugging level to use
func verifyDebugLevel() {
	debugLevel := viper.GetInt(keys.DebugLevel)
	if debugLevel > 5 {
		debugLevel = 5
	} else if debugLevel < 0 {
		debugLevel = 0
	}
	logging.I("Debugging level: %v", debugLevel)
	viper.Set(keys.DebugLevel, debugLevel)
	logging.Level = int(debugLevel)
}

// verifyInputFiletypes checks that the inputted filetypes are accepted
func verifyInputFiletypes() {
	argsVInputExts := viper.GetStringSlice(keys.InputVideoExts)
	inputVExts := make([]enums.ConvertFromFiletype, 0, len(argsVInputExts))

	for _, data := range argsVInputExts {
		switch data {
		case "mkv":
			inputVExts = append(inputVExts, enums.VID_EXTS_MKV)
		case "mp4":
			inputVExts = append(inputVExts, enums.VID_EXTS_MP4)
		case "webm":
			inputVExts = append(inputVExts, enums.VID_EXTS_WEBM)
		default:
			inputVExts = append(inputVExts, enums.VID_EXTS_ALL)
		}
	}
	if len(inputVExts) == 0 {
		inputVExts = append(inputVExts, enums.VID_EXTS_ALL)
	}
	logging.D(2, "Received video input extension filter: %v", inputVExts)
	viper.Set(keys.InputVExtsEnum, inputVExts)

	argsMInputExts := viper.GetStringSlice(keys.InputMetaExts)
	inputMExts := make([]enums.MetaFiletypeFilter, 0, len(argsMInputExts))

	for _, data := range argsMInputExts {
		switch data {
		case "json":
			inputMExts = append(inputMExts, enums.META_EXTS_JSON)
		case "nfo":
			inputMExts = append(inputMExts, enums.META_EXTS_NFO)
		default:
			inputMExts = append(inputMExts, enums.META_EXTS_ALL)
		}
	}
	if len(inputMExts) == 0 {
		inputMExts = append(inputMExts, enums.META_EXTS_ALL)
	}
	logging.D(2, "Received meta input extension filter: %v", inputMExts)
	viper.Set(keys.InputMExtsEnum, inputMExts)
}

// verifyHWAcceleration checks and sets HW acceleration to use
func verifyHWAcceleration() {
	switch viper.GetString(keys.GPU) {
	case "nvidia":
		viper.Set(keys.GPUEnum, enums.GPU_NVIDIA)
		logging.P("GPU acceleration selected by user: %v", keys.GPU)
	case "amd":
		viper.Set(keys.GPUEnum, enums.GPU_AMD)
		logging.P("GPU acceleration selected by user: %v", keys.GPU)
	case "intel":
		viper.Set(keys.GPUEnum, enums.GPU_INTEL)
		logging.P("GPU acceleration selected by user: %v", keys.GPU)
	default:
		viper.Set(keys.GPUEnum, enums.GPU_NO_HW_ACCEL)
	}
}

// verifyConcurrencyLimit checks and ensures correct concurrency limit input
func verifyConcurrencyLimit() {
	maxConcurrentProcesses := viper.GetInt(keys.Concurrency)

	switch {
	case maxConcurrentProcesses < 1:
		maxConcurrentProcesses = 1
		logging.E(2, "Max concurrency set too low, set to minimum value: %d", maxConcurrentProcesses)
	default:
		logging.I("Max concurrency: %d", maxConcurrentProcesses)
	}
	viper.Set(keys.Concurrency, maxConcurrentProcesses)
}

// verifyCPUUsage verifies the value used to limit the CPU needed to spawn a new routine
func verifyResourceLimits() {
	MinMemUsage := viper.GetUint64(keys.MinMem)
	MinMemUsage *= 1024 * 1024 // Convert input to MB

	currentAvailableMem, err := mem.VirtualMemory()
	if err != nil {
		logging.E(0, "Could not get system memory, using default max RAM requirements: %v", err)
		currentAvailableMem.Available = 1024
	}
	if MinMemUsage > currentAvailableMem.Available {
		MinMemUsage = currentAvailableMem.Available
	}

	if MinMemUsage > 0 {
		logging.I("Min RAM to spawn process: %v", MinMemUsage)
	}
	viper.Set(keys.MinMemMB, MinMemUsage)

	maxCPUUsage := viper.GetFloat64(keys.MaxCPU)
	switch {
	case maxCPUUsage > 100.0:
		maxCPUUsage = 100.0
		logging.E(2, "Max CPU usage entered too high, setting to default max: %.2f%%", maxCPUUsage)

	case maxCPUUsage < 1.0:
		maxCPUUsage = 10.0
		logging.E(0, "Max CPU usage entered too low, setting to default low: %.2f%%", maxCPUUsage)
	}
	if maxCPUUsage != 100.0 {
		logging.I("Max CPU usage: %.2f%%", maxCPUUsage)
	}
	viper.Set(keys.MaxCPU, maxCPUUsage)
}

// Verify the output filetype is valid for FFmpeg
func verifyOutputFiletype() {
	if !viper.IsSet(keys.OutputFiletypeInput) {
		return
	}

	o := GetString(keys.OutputFiletypeInput)
	o = strings.TrimSpace(o)

	if !strings.HasPrefix(o, ".") {
		o = "." + o
		viper.Set(keys.OutputFiletype, o)
	}

	valid := false
	for ext := range consts.AllVidExtensions {
		if o != ext {
			continue
		} else {
			valid = true
			break
		}
	}

	if valid {
		logging.I("Outputting files as %s", o)
	}
}

// verifyPurgeMetafiles checks and sets the type of metafile purge to perform
func verifyPurgeMetafiles() {
	if !viper.IsSet(keys.MetaPurge) {
		return
	}

	var e enums.PurgeMetafiles
	purgeType := viper.GetString(keys.MetaPurge)

	purgeType = strings.TrimSpace(purgeType)
	purgeType = strings.ToLower(purgeType)
	purgeType = strings.ReplaceAll(purgeType, ".", "")

	switch purgeType {
	case "all":
		e = enums.PURGEMETA_ALL
	case "json":
		e = enums.PURGEMETA_JSON
	case "nfo":
		e = enums.PURGEMETA_NFO
	default:
		e = enums.PURGEMETA_NONE
	}

	viper.Set(keys.MetaPurgeEnum, e)
}
package cfg

import (
	"fmt"
	enums "metarr/internal/domain/enums"
	keys "metarr/internal/domain/keys"
	"metarr/internal/models"
	logging "metarr/internal/utils/logging"
	"strings"

	"github.com/spf13/viper"
)

var (
	filenameReplaceSuffixInput []string
)

type metaOpsLen struct {
	newLen,
	apndLen,
	pfxLen,
	trimSfxLen,
	trimPfxLen,
	replaceLen,
	dTagLen,
	delDTagLen,
	copyToFieldLen,
	pasteFromFieldLen int
}

// initTextReplace initializes text replacement functions
func initTextReplace() error {

	// Parse rename flag
	setRenameFlag()

	logging.D(1, "About to validate meta operations")
	// Meta operations
	if err := validateMetaOps(); err != nil {
		return err
	}

	// Replace filename suffixes
	if err := validateFilenameSuffixReplace(); err != nil {
		return err
	}

	return nil
}

// validateMetaOps parses the meta transformation operations
func validateMetaOps() error {

	logging.D(1, "Validating meta operations...")

	metaOpsInput := viper.GetStringSlice(keys.MetaOps)
	if len(metaOpsInput) == 0 {
		logging.D(2, "No metadata operations passed in")
		return nil
	}

	m := metaOpsMapLength(metaOpsInput, metaOpsLen{})

	// Add new field
	newField := make([]models.MetaNewField, 0, m.newLen)
	models.SetOverrideMap = make(map[enums.OverrideMetaType]string, m.newLen)

	// Replacements
	replace := make([]models.MetaReplace, 0, m.replaceLen)
	models.ReplaceOverrideMap = make(map[enums.OverrideMetaType]models.MOverrideReplacePair, m.replaceLen)

	// Append
	apnd := make([]models.MetaAppend, 0, m.apndLen)
	models.AppendOverrideMap = make(map[enums.OverrideMetaType]string, m.apndLen)

	// Prefix
	pfx := make([]models.MetaPrefix, 0, m.pfxLen)

	// Trim prefix/suffix
	trimSfx := make([]models.MetaTrimSuffix, 0, m.trimSfxLen)
	trimPfx := make([]models.MetaTrimPrefix, 0, m.trimPfxLen)

	// Date tagging ops
	dateTag := make(map[string]models.MetaDateTag, m.dTagLen)
	delDateTag := make(map[string]models.MetaDateTag, m.delDTagLen)

	// Copy to and from fields
	copyToField := make([]models.CopyToField, 0, m.copyToFieldLen)
	pasteFromField := make([]models.PasteFromField, 0, m.pasteFromFieldLen)

	for _, op := range metaOpsInput {

		// Check validity
		parts := strings.Split(op, ":")

		if len(parts) < 3 || len(parts) > 4 {
			return fmt.Errorf("malformed input meta-ops, each entry must be at least 3 parts, split by : (e.g. 'title:add:Video Title')")
		}

		field := parts[0]
		operation := parts[1]
		value := parts[2]

		switch strings.ToLower(operation) {
		case "set":
			switch field {
			case "all-credits", "credits-all":
				models.SetOverrideMap[enums.OVERRIDE_META_CREDITS] = value
			}

			newFieldModel := models.MetaNewField{
				Field: field,
				Value: value,
			}
			newField = append(newField, newFieldModel)
			fmt.Println()
			logging.D(3, "Added new field op:\nField: %s\nValue: %s", newFieldModel.Field, newFieldModel.Value)
			fmt.Println()

		case "append":
			switch field {
			case "all-credits", "credits-all":
				models.AppendOverrideMap[enums.OVERRIDE_META_CREDITS] = value
			}

			apndModel := models.MetaAppend{
				Field:  field,
				Suffix: value,
			}
			apnd = append(apnd, apndModel)
			fmt.Println()
			logging.D(3, "Added new append op:\nField: %s\nAppend: %s", apndModel.Field, apndModel.Suffix)
			fmt.Println()

		case "prefix":
			pfxModel := models.MetaPrefix{
				Field:  field,
				Prefix: value,
			}
			pfx = append(pfx, pfxModel)
			fmt.Println()
			logging.D(3, "Added new prefix op:\nField: %s\nPrefix: %s", pfxModel.Field, pfxModel.Prefix)
			fmt.Println()

		case "trim-suffix":
			tSfxModel := models.MetaTrimSuffix{
				Field:  field,
				Suffix: value,
			}
			trimSfx = append(trimSfx, tSfxModel)
			fmt.Println()
			logging.D(3, "Added new suffix trim op:\nField: %s\nSuffix: %s", tSfxModel.Field, tSfxModel.Suffix)
			fmt.Println()

		case "trim-prefix":
			tPfxModel := models.MetaTrimPrefix{
				Field:  field,
				Prefix: value,
			}
			trimPfx = append(trimPfx, tPfxModel)
			fmt.Println()
			logging.D(3, "Added new prefix trim op:\nField: %s\nPrefix: %s", tPfxModel.Field, tPfxModel.Prefix)
			fmt.Println()

		case "copy-to":
			c := models.CopyToField{
				Field: field,
				Dest:  value,
			}
			copyToField = append(copyToField, c)
			fmt.Println()
			logging.D(3, "Added new copy/paste op:\nField: %s\nCopy To: %s", c.Field, c.Dest)
			fmt.Println()

		case "paste-from":
			p := models.PasteFromField{
				Field:  field,
				Origin: value,
			}
			pasteFromField = append(pasteFromField, p)
			fmt.Println()
			logging.D(3, "Added new copy/paste op:\nField: %s\nPaste From: %s", p.Field, p.Origin)
			fmt.Println()

		case "replace":
			if len(parts) != 4 {
				return fmt.Errorf("replacement should be in format 'field:replace:text:replacement'")
			}

			switch field {
			case "all-credits", "credits-all":
				models.ReplaceOverrideMap[enums.OVERRIDE_META_CREDITS] = models.MOverrideReplacePair{
					Value:       value,
					Replacement: parts[3],
				}
			}
			rModel := models.MetaReplace{
				Field:       field,
				Value:       value,
				Replacement: parts[3],
			}

			replace = append(replace, rModel)
			fmt.Println()
			logging.D(3, "Added new replace operation:\nField: %s\nValue: %s\nReplacement: %s\n", rModel.Field, rModel.Value, rModel.Replacement)
			fmt.Println()

		case "date-tag":
			if len(parts) != 4 {
				return fmt.Errorf("date-tag should be in format 'field:date-tag:location:format' (Ymd is yyyy-mm-dd, ymd is yy-mm-dd)")
			}
			var loc enums.MetaDateTagLocation

			switch strings.ToLower(value) {
			case "prefix":
				loc = enums.DATE_TAG_LOC_PFX
			case "suffix":
				loc = enums.DATE_TAG_LOC_SFX
			default:
				return fmt.Errorf("date tag location must be prefix, or suffix")
			}
			if e, err := dateEnum(parts[3]); err != nil {
				return err
			} else {
				dateTag[field] = models.MetaDateTag{
					Loc:    loc,
					Format: e,
				}
				fmt.Println()
				logging.D(3, "Added new date tag operation:\nField: %s\nLocation: %s\nReplacement: %s\n", field, value, parts[3])
				fmt.Println()
			}

		case "delete-date-tag":
			if len(parts) != 4 {
				return fmt.Errorf("date-tag should be in format 'field:date-tag:location:format' (Ymd is yyyy-mm-dd, ymd is yy-mm-dd)")
			}
			var loc enums.MetaDateTagLocation

			switch strings.ToLower(value) {
			case "prefix":
				loc = enums.DATE_TAG_LOC_PFX
			case "suffix":
				loc = enums.DATE_TAG_LOC_SFX
			default:
				return fmt.Errorf("date tag location must be prefix, or suffix")
			}
			if e, err := dateEnum(parts[3]); err != nil {
				return err
			} else {
				delDateTag[field] = models.MetaDateTag{
					Loc:    loc,
					Format: e,
				}
				fmt.Println()
				logging.D(3, "Added delete date tag operation:\nField: %s\nLocation: %s\nReplacement: %s\n", field, value, parts[3])
				fmt.Println()
			}

		default:
			return fmt.Errorf("unrecognized meta operation %q (valid operations: add, append, prefix, trim-suffix, trim-prefix, replace, date-tag, delete-date-tag, copy-to, copy-from)", parts[1])
		}
	}

	if len(apnd) > 0 {
		logging.I("Appending: %v", apnd)
		viper.Set(keys.MAppend, apnd)
	}

	if len(newField) > 0 {
		logging.I("New meta fields: %v", newField)
		viper.Set(keys.MNewField, newField)
	}

	if len(pfx) > 0 {
		logging.I("Prefixing: %v", apnd)
		viper.Set(keys.MPrefix, pfx)
	}

	if len(trimPfx) > 0 {
		logging.I("Trimming prefix: %v", trimPfx)
		viper.Set(keys.MTrimPrefix, trimPfx)
	}

	if len(trimSfx) > 0 {
		logging.I("Trimming suffix: %v", trimSfx)
		viper.Set(keys.MTrimSuffix, trimSfx)
	}

	if len(replace) > 0 {
		logging.I("Replacing text: %v", replace)
		viper.Set(keys.MReplaceText, replace)
	}

	if len(copyToField) > 0 {
		logging.I("Copying to fields: %v", copyToField)
		viper.Set(keys.MCopyToField, copyToField)
	}

	if len(pasteFromField) > 0 {
		logging.I("Pasting from fields: %v", pasteFromField)
		viper.Set(keys.MPasteFromField, pasteFromField)
	}

	if len(dateTag) > 0 {
		logging.I("Adding date tags: %v", dateTag)
		viper.Set(keys.MDateTagMap, dateTag)
	}

	if len(delDateTag) > 0 {
		logging.I("Deleting date tags: %v", delDateTag)
		viper.Set(keys.MDelDateTagMap, delDateTag)
	}

	return nil
}

// metaOpsMapLength quickly grabs the lengths needed for each map
func metaOpsMapLength(metaOpsInput []string, m metaOpsLen) metaOpsLen {

	for _, op := range metaOpsInput {
		if i := strings.IndexByte(op, ':'); i >= 0 {
			if j := strings.IndexByte(op[i+1:], ':'); j >= 0 {
				op = op[i+1 : i+1+j]

				switch op {
				case "set":
					m.newLen++
				case "append":
					m.apndLen++
				case "prefix":
					m.pfxLen++
				case "trim-suffix":
					m.trimSfxLen++
				case "trim-prefix":
					m.trimPfxLen++
				case "replace":
					m.replaceLen++
				case "date-tag":
					m.dTagLen++
				case "delete-date-tag":
					m.delDTagLen++
				case "copy-to":
					m.copyToFieldLen++
				case "paste-from":
					m.pasteFromFieldLen++
				}
			}
		}
	}
	fmt.Println()
	logging.D(2, "Meta additions: %d\nMeta appends: %d\nMeta prefix: %d\nMeta suffix trim: %d\nMeta prefix trim: %d\nMeta replacements: %d\nDate tags: %d\nDelete date tags: %d\nCopy operations: %d\nPaste operations: %d", m.newLen, m.apndLen, m.pfxLen, m.trimSfxLen, m.trimPfxLen, m.replaceLen, m.dTagLen, m.delDTagLen, m.copyToFieldLen, m.pasteFromFieldLen)
	fmt.Println()
	return m
}

// validateFilenameSuffixReplace checks if the input format for filename suffix replacement is valid
func validateFilenameSuffixReplace() error {
	filenameReplaceSuffix := make([]models.FilenameReplaceSuffix, 0, len(filenameReplaceSuffixInput))

	for _, pair := range filenameReplaceSuffixInput {
		parts := strings.SplitN(pair, ":", 2)
		if len(parts) < 3 {
			return fmt.Errorf("invalid use of filename-replace-suffix, values must be written as (suffix:replacement)")
		}
		filenameReplaceSuffix = append(filenameReplaceSuffix, models.FilenameReplaceSuffix{
			Suffix:      parts[0],
			Replacement: parts[1],
		})
	}
	if len(filenameReplaceSuffix) > 0 {
		logging.I("Meta replace suffixes: %v", filenameReplaceSuffix)
		viper.Set(keys.FilenameReplaceSfx, filenameReplaceSuffix)
	}
	return nil
}

// setRenameFlag sets the rename style to apply
func setRenameFlag() {

	var renameFlag enums.ReplaceToStyle
	argRenameFlag := viper.GetString(keys.RenameStyle)

	// Trim whitespace for more robust validation
	argRenameFlag = strings.TrimSpace(argRenameFlag)
	argRenameFlag = strings.ToLower(argRenameFlag)

	switch argRenameFlag {
	case "spaces", "space":
		renameFlag = enums.RENAMING_SPACES
		logging.P("Rename style selected: %v", argRenameFlag)

	case "underscores", "underscore":
		renameFlag = enums.RENAMING_UNDERSCORES
		logging.P("Rename style selected: %v", argRenameFlag)

	case "fixes", "fix", "fixes-only", "fixesonly":
		renameFlag = enums.RENAMING_FIXES_ONLY
		logging.P("Rename style selected: %v", argRenameFlag)

	default:
		logging.D(1, "'Spaces', 'underscores' or 'fixes-only' not selected for renaming style, skipping these modifications.")
		renameFlag = enums.RENAMING_SKIP
	}
	viper.Set(keys.Rename, renameFlag)
}

// initDateReplaceFormat initializes the user's preferred format for dates
func initDateReplaceFormat() error {

	if viper.IsSet(keys.InputFileDatePfx) {
		dateFmt := viper.GetString(keys.InputFileDatePfx)

		// Trim whitespace for more robust validation
		dateFmt = strings.TrimSpace(dateFmt)

		formatEnum, err := dateEnum(dateFmt)
		if err != nil {
			return err
		}

		viper.Set(keys.FileDateFmt, formatEnum)
		logging.D(1, "Set file date format to %v", formatEnum)
	}
	return nil
}

// dateEnum returns the date format enum type
func dateEnum(dateFmt string) (formatEnum enums.DateFormat, err error) {

	if len(dateFmt) < 2 {
		return enums.DATEFMT_SKIP, fmt.Errorf("invalid date format entered as %q, please enter up to three characters (where 'Y' is yyyy and 'y' is yy)", dateFmt)
	} else {
		switch dateFmt {
		case "Ymd":
			return enums.DATEFMT_YYYY_MM_DD, nil
		case "ymd":
			return enums.DATEFMT_YY_MM_DD, nil
		case "Ydm":
			return enums.DATEFMT_YYYY_DD_MM, nil
		case "ydm":
			return enums.DATEFMT_YY_DD_MM, nil
		case "dmY":
			return enums.DATEFMT_DD_MM_YYYY, nil
		case "dmy":
			return enums.DATEFMT_DD_MM_YY, nil
		case "mdY":
			return enums.DATEFMT_MM_DD_YYYY, nil
		case "mdy":
			return enums.DATEFMT_MM_DD_YY, nil
		case "md":
			return enums.DATEFMT_MM_DD, nil
		case "dm":
			return enums.DATEFMT_DD_MM, nil
		}
	}
	return enums.DATEFMT_SKIP, fmt.Errorf("invalid date format entered as %q, please enter up to three ymd characters (where capital Y is yyyy and y is yy)", dateFmt)
}
package cfg

import (
	"github.com/spf13/viper"
)

// Set sets the value for the key in the override register. Set is case-insensitive for a key. Will be used instead of values obtained via flags, config file, ENV, default, or key/value store.
func Set(key string, value any) {

	viper.Set(key, value)
}

// Get can retrieve any value given the key to use. Get is case-insensitive for a key. Get has the behavior of returning the value associated with the first place from where it is set. Viper will check in the following order: override, flag, env, config file, key/value store, default
// Get returns an interface. For a specific value use one of the Get____ methods.
func Get(key string) any {
	return viper.Get(key)
}

// GetBool returns the value associated with the key as a boolean.
func GetBool(key string) bool {
	return viper.GetBool(key)
}

// GetInt returns the value associated with the key as an integer.
func GetInt(key string) int {
	return viper.GetInt(key)
}

// GetUint64 returns the value associated with the key as an unsigned integer.
func GetUint64(key string) uint64 {
	return viper.GetUint64(key)
}

// GetFloat64 returns the value associated with the key as a float64.
func GetFloat64(key string) float64 {
	return viper.GetFloat64(key)
}

// GetString returns the value associated with the key as a string.
func GetString(key string) string {
	return viper.GetString(key)
}

// GetStringSlice returns the value associated with the key as a slice of strings.
func GetStringSlice(key string) []string {
	return viper.GetStringSlice(key)
}

// IsSet checks to see if the key has been set in any of the data locations.
// IsSet is case-insensitive for a key.
func IsSet(key string) bool {
	return viper.IsSet(key)
}
package dates

import (
	"fmt"
	enums "metarr/internal/domain/enums"
	logging "metarr/internal/utils/logging"
	"strconv"
	"strings"
)

// ParseDateComponents extracts and validates year, month, and day from the date string
func ParseDateComponents(date string, dateFmt enums.DateFormat) (year, month, day string, err error) {
	date = strings.ReplaceAll(date, "-", "")
	date = strings.TrimSpace(date)

	year, month, day, err = getYearMonthDay(date, dateFmt)
	if err != nil {
		return "", "", "", err
	}

	return validateDateComponents(year, month, day)
}

// dayStringSwitch appends a numerical day with the appropriate suffix (e.g. '1st', '2nd', '3rd')
func dayStringSwitch(day string) string {
	var b strings.Builder
	b.Grow(len(day) + 2)

	num, err := strconv.Atoi(day)
	if err != nil {
		logging.E(0, "Failed to convert date string to number")
		return day
	}

	b.WriteString(day)

	if num > 10 && num < 20 {
		b.WriteString("th")
		return b.String()
	}

	switch num % 10 {
	case 1:
		b.WriteString("st")
	case 2:
		b.WriteString("nd")
	case 3:
		b.WriteString("rd")
	default:
		b.WriteString("th")
	}

	return b.String()
}

// Convert a numerical month to a word
func monthStringSwitch(month string) string {
	var monthStr string
	switch month {
	case "01":
		monthStr = "Jan"
	case "02":
		monthStr = "Feb"
	case "03":
		monthStr = "Mar"
	case "04":
		monthStr = "Apr"
	case "05":
		monthStr = "May"
	case "06":
		monthStr = "Jun"
	case "07":
		monthStr = "Jul"
	case "08":
		monthStr = "Aug"
	case "09":
		monthStr = "Sep"
	case "10":
		monthStr = "Oct"
	case "11":
		monthStr = "Nov"
	case "12":
		monthStr = "Dec"
	default:
		logging.E(0, "Failed to make month string from month number %q", month)
		monthStr = "Jan"
	}
	return monthStr
}

// joinNonEmpty joins non-empty strings from an array with hyphens
func joinNonEmpty(parts [3]string) string {
	nonEmpty := make([]string, 0, len(parts))
	for _, p := range parts {
		if p != "" {
			nonEmpty = append(nonEmpty, p)
		}
	}
	if len(nonEmpty) == 0 {
		return ""
	}
	return strings.Join(nonEmpty, "-")
}

// getYear returns the year digits from the date string
func getYearMonthDay(d string, dateFmt enums.DateFormat) (year, month, day string, err error) {
	d = strings.ReplaceAll(d, "-", "")
	d = strings.TrimSpace(d)

	if len(d) >= 8 {
		switch dateFmt {
		case enums.DATEFMT_DD_MM_YY, enums.DATEFMT_MM_DD_YY, enums.DATEFMT_YY_DD_MM, enums.DATEFMT_YY_MM_DD:
			year = d[2:4]
		default:
			year = d[:4]
		}
		month = d[4:6]
		day = d[6:8]

		return year, month, day, nil
	}
	if len(d) >= 6 {
		year = d[:2]
		month = d[2:4]
		day = d[4:6]

		return year, month, day, nil
	}
	if len(d) == 4 { // Guess year or month-day

		i, err := strconv.Atoi(d[:2])
		if err != nil {
			return "", "", "", fmt.Errorf("invalid date string %q threw error: %w", d, err)
		}
		j, err := strconv.Atoi(d[2:4])
		if err != nil {
			return "", "", "", fmt.Errorf("invalid date string %q threw error: %w", d, err)
		}

		if (i == 20 || i == 19) && j > 12 { // First guess year
			logging.I("Guessing date string %q as year", d)
			switch dateFmt {
			case enums.DATEFMT_DD_MM_YY, enums.DATEFMT_MM_DD_YY, enums.DATEFMT_YY_DD_MM, enums.DATEFMT_YY_MM_DD:
				return d[2:4], "", "", nil
			default:
				return d[:4], "", "", nil
			}
		} else { // Second guess, month-date
			if ddmm, mmdd := maybeDayMonth(i, j); ddmm || mmdd {
				if ddmm {
					logging.I("Guessing date string %q as day-month", d)
					day = d[:2]
					month = d[2:4]

				} else if mmdd {
					logging.I("Guessing date string %q as month-day", d)
					day = d[2:4]
					month = d[:2]
				}
				return "", month, day, nil
			} else if i == 20 || i == 19 { // Final guess year
				logging.I("Guessing date string %q as year after failed day-month check", d)
				switch dateFmt {
				case enums.DATEFMT_DD_MM_YY, enums.DATEFMT_MM_DD_YY, enums.DATEFMT_YY_DD_MM, enums.DATEFMT_YY_MM_DD:
					return d[2:4], "", "", nil
				default:
					return d[:4], "", "", nil
				}
			}
		}
	}

	return "", "", "", fmt.Errorf("failed to parse year, month, and day from %q", d)
}

// validateDateComponents attempts to fix faulty date arrangements
func validateDateComponents(year, month, day string) (y, m, d string, err error) {

	if isValidMonth(month) && isValidDay(day, month, year) {
		return year, month, day, nil
	}

	// Attempt swapping day and month
	if isValidMonth(day) && isValidDay(month, day, year) {
		return year, month, day, nil
	}

	// Fail check:
	return "", "", "", fmt.Errorf("invalid date components: year=%s, month=%s, day=%s", year, month, day)
}

// isValidMonth checks if the month inputted is a valid month
func isValidMonth(month string) bool {
	m, err := strconv.Atoi(month)
	if err != nil {
		return false
	}
	return m >= 1 && m <= 12
}

// isValidDay checks if the day inputted is a valid day
func isValidDay(day, month, year string) bool {
	d, err := strconv.Atoi(day)
	if err != nil {
		return false
	}

	m, err := strconv.Atoi(month)
	if err != nil {
		return false
	}

	y, err := strconv.Atoi(year)
	if err != nil {
		return false
	}

	if d < 1 || d > 31 {
		return false
	}

	// Months with 30 days
	if m == 4 || m == 6 || m == 9 || m == 11 {
		return d <= 30
	}

	// February
	if m == 2 {
		// Leap year check
		isLeap := y%4 == 0 && (y%100 != 0 || y%400 == 0)
		if isLeap {
			return d <= 29
		}
		return d <= 28
	}

	return true
}

// maybeDayMonth guesses if the input is a DD-MM or MM-DD format
func maybeDayMonth(i, j int) (ddmm, mmdd bool) {
	if i == 0 || i >= 31 || j == 0 || j >= 31 {
		return false, false
	}

	switch {
	case i <= 31 && j <= 12:
		return ddmm, false
	case j <= 31 && i <= 12:
		return false, mmdd
	default:
		return false, false
	}
}
package dates

import (
	"fmt"
	enums "metarr/internal/domain/enums"
	"metarr/internal/models"
	logging "metarr/internal/utils/logging"
	"strings"

	"github.com/araddon/dateparse"
)

// ParseWordDate parses and formats the inputted word date (e.g. Jan 2nd)
func ParseWordDate(dateString string) (string, error) {

	t, err := dateparse.ParseAny(dateString)
	if err != nil {
		return "", fmt.Errorf("unable to parse date: %s", dateString)
	}

	return t.Format("2006-01-02"), nil
}

// ParseAndFormatDate parses and formats the inputted date string
func ParseNumDate(dateNum string) (string, error) {

	t, err := dateparse.ParseAny(dateNum)
	if err != nil {
		return "", fmt.Errorf("unable to parse date %q to word date", dateNum)
	}
	time := t.Format("01022006")
	if time == "" {
		time = t.Format("010206")
	}

	var day, month, year, dateStr string

	if len(time) < 6 {
		return dateNum, fmt.Errorf("unable to parse date, date %q is too short", time)
	}

	if len(time) >= 8 {
		day = time[2:4]
		month = time[:2]
		year = time[4:8]
	} else if len(time) >= 6 {
		day = time[2:4]
		month = time[:2]
		year = time[4:6]
	}

	month = monthStringSwitch(month)
	day = dayStringSwitch(day)

	dateStr = fmt.Sprintf("%s %s, %s", month, day, year)
	logging.S(1, "Made string form date: %q", dateStr)

	return dateStr, nil
}

// YyyyMmDd converts inputted date strings into the user's defined format
func YyyyMmDd(date string) (string, bool) {

	var t string = ""
	if tIdx := strings.Index(date, "T"); tIdx != -1 {
		t = date[tIdx:]
	}

	date = strings.ReplaceAll(date, "-", "")

	if len(date) >= 8 {
		formatted := fmt.Sprintf("%s-%s-%s%s", date[:4], date[4:6], date[6:8], t)
		logging.S(2, "Made date %s", formatted)
		return formatted, true

	} else if len(date) >= 6 {
		formatted := fmt.Sprintf("%s-%s-%s%s", date[:2], date[2:4], date[4:6], t)
		logging.S(2, "Made date %s", formatted)
		return formatted, true
	}
	logging.D(3, "Returning empty or short date element (%s) without formatting", date)
	return date, false
}

// formatDateString formats the date as a hyphenated string
func FormatDateString(year, month, day string, dateFmt enums.DateFormat) (string, error) {
	var parts [3]string

	switch dateFmt {
	case enums.DATEFMT_YYYY_MM_DD, enums.DATEFMT_YY_MM_DD:
		parts = [3]string{year, month, day}
	case enums.DATEFMT_YYYY_DD_MM, enums.DATEFMT_YY_DD_MM:
		parts = [3]string{year, day, month}
	case enums.DATEFMT_DD_MM_YYYY, enums.DATEFMT_DD_MM_YY:
		parts = [3]string{day, month, year}
	case enums.DATEFMT_MM_DD_YYYY, enums.DATEFMT_MM_DD_YY:
		parts = [3]string{month, day, year}
	}

	result := joinNonEmpty(parts)
	if result == "" {
		return "", fmt.Errorf("no valid date components found")
	}
	return result, nil
}

// FormatAllDates formats timestamps into a hyphenated form
func FormatAllDates(fd *models.FileData) string {

	var (
		result string
		ok     bool
	)

	d := fd.MDates

	if !ok && d.Originally_Available_At != "" {
		logging.D(2, "Attempting to format originally available date: %v", d.Originally_Available_At)
		result, ok = YyyyMmDd(d.Originally_Available_At)
	}
	if !ok && d.ReleaseDate != "" {
		logging.D(2, "Attempting to format release date: %v", d.ReleaseDate)
		result, ok = YyyyMmDd(d.ReleaseDate)
	}
	if !ok && d.Date != "" {
		logging.D(2, "Attempting to format date: %v", d.Date)
		result, ok = YyyyMmDd(d.Date)
	}
	if !ok && d.UploadDate != "" {
		logging.D(2, "Attempting to format upload date: %v", d.UploadDate)
		result, ok = YyyyMmDd(d.UploadDate)
	}
	if !ok && d.Creation_Time != "" {
		logging.D(3, "Attempting to format creation time: %v", d.Creation_Time)
		result, ok = YyyyMmDd(d.Creation_Time)
	}
	if !ok {
		logging.E(0, "Failed to format dates")
		return ""
	} else {
		logging.D(2, "Exiting with formatted date: %v", result)

		d.FormattedDate = result

		logging.D(2, "Got formatted date %q and entering parse to string function...", result)

		var err error
		d.StringDate, err = ParseNumDate(d.FormattedDate)
		if err != nil {
			logging.E(0, err.Error())
		}

		return result
	}
}
package consts

const (
	BrowserChrome  = "chrome"
	BrowserEdge    = "edge"
	BrowserFirefox = "firefox"
	BrowserSafari  = "safari"
)
package consts

// Colors
const (
	ColorReset  = "\033[0m"
	ColorRed    = "\033[91m"
	ColorGreen  = "\033[92m"
	ColorYellow = "\033[93m"
	ColorBlue   = "\033[34m"
	ColorPurple = "\033[35m"
	ColorCyan   = "\033[96m"
	ColorWhite  = "\033[37m"
)

const (
	RedError     string = ColorRed + "[ERROR] " + ColorReset
	GreenSuccess string = ColorGreen + "[Success] " + ColorReset
	YellowDebug  string = ColorYellow + "[Debug] " + ColorReset
	BlueInfo     string = ColorCyan + "[Info] " + ColorReset
)
package consts

// File prefix and suffix
const (
	BackupTag = "_metarrbackup"
	TempTag   = "tmp_"
)

const TimeSfx = "T00:00:00Z"

var (
	AllVidExtensions = map[string]bool{
		Ext3G2: false, Ext3GP: false, ExtAVI: false, ExtF4V: false, ExtFLV: false,
		ExtM4V: false, ExtMKV: false, ExtMOV: false, ExtMP4: false, ExtMPEG: false,
		ExtMPG: false, ExtMTS: false, ExtOGM: false, ExtOGV: false, ExtRM: false,
		ExtRMVB: false, ExtTS: false, ExtVOB: false, ExtWEBM: false, ExtWMV: false,
		ExtASF: false,
	}
)

var (
	AllMetaExtensions = map[string]bool{
		MExtJSON: false, MExtNFO: false,
	}
)

// Webpage tags
var (
	// Ensure lengths match
	WebDateTags        = [...]string{"release-date", "upload-date", "date", "date-text", "text-date"}
	WebDescriptionTags = [...]string{"description", "longdescription", "long-description", "summary", "synopsis",
		"check-for-urls"}

	// Credits tags, and nested elements
	WebCreditsTags      = [...]string{"creator", "uploader", "uploaded-by", "uploaded_by", "channel-name", "claim-preview__title"}
	WebCreditsSelectors = map[string]string{
		"claim-preview__title":               "truncated-text",
		`script[type="application/ld+json"]`: "author.name",
	}

	WebTitleTags = [...]string{"video-title", "video-name"}
)
package consts

var (
	ContractionsSpaced = map[string]string{
		"ain t":     "aint",
		"can t":     "cant",
		"don t":     "dont",
		"didn t":    "didnt",
		"hasn t":    "hasnt",
		"haven t":   "havent",
		"won t":     "wont",
		"wouldn t":  "wouldnt",
		"shouldn t": "shouldnt",
		"couldn t":  "couldnt",
		"wasn t":    "wasnt",
		"weren t":   "werent",
		"let s":     "lets",
		"hadn t":    "hadnt",
		"who s":     "whos",
		"what s":    "whats",
		"when s":    "whens",
		"where s":   "wheres",
		"why s":     "whys",
		"how s":     "hows",
		"there s":   "theres",
		"that s":    "thats",
		"it d":      "itd",
		"she d":     "shed",
		"she s":     "shes",
		"he d":      "hed",
		"he s":      "hes",
		"it ll":     "itll",
		"should ve": "shouldve",
		"could ve":  "couldve",
		"would ve":  "wouldve",
	}

	ContractionsUnderscored = map[string]string{
		"ain_t":     "aint",
		"can_t":     "cant",
		"don_t":     "dont",
		"didn_t":    "didnt",
		"hasn_t":    "hasnt",
		"haven_t":   "havent",
		"won_t":     "wont",
		"wouldn_t":  "wouldnt",
		"shouldn_t": "shouldnt",
		"couldn_t":  "couldnt",
		"wasn_t":    "wasnt",
		"weren_t":   "werent",
		"let_s":     "lets",
		"hadn_t":    "hadnt",
		"who_s":     "whos",
		"what_s":    "whats",
		"when_s":    "whens",
		"where_s":   "wheres",
		"why_s":     "whys",
		"how_s":     "hows",
		"there_s":   "theres",
		"that_s":    "thats",
		"it_d":      "itd",
		"she_d":     "shed",
		"she_s":     "shes",
		"he_d":      "hed",
		"he_s":      "hes",
		"it_ll":     "itll",
		"should_ve": "shouldve",
		"could_ve":  "couldve",
		"would_ve":  "wouldve",
	}
)
package consts

// Video extensions
const (
	Ext3GP  = ".3gp"
	Ext3G2  = ".3g2"
	ExtASF  = ".asf"
	ExtAVI  = ".avi"
	ExtF4V  = ".f4v"
	ExtFLV  = ".flv"
	ExtOGM  = ".ogm"
	ExtOGV  = ".ogv"
	ExtM4V  = ".m4v"
	ExtMKV  = ".mkv"
	ExtMOV  = ".mov"
	ExtMP4  = ".mp4"
	ExtMPEG = ".mpeg"
	ExtMPG  = ".mpg"
	ExtMTS  = ".mts"
	ExtRM   = ".rm"
	ExtRMVB = ".rmvb"
	ExtTS   = ".ts"
	ExtVOB  = ".vob"
	ExtWEBM = ".webm"
	ExtWMV  = ".wmv"
)

// Metafile extensions
const (
	MExtJSON = ".json"
	MExtNFO  = ".nfo"
)
package consts

// AV codec copy
var (
	AVCodecCopy = [...]string{"-codec", "copy"}
)

// Audio flags
var (
	AudioCodecCopy = [...]string{"-c:a", "copy"}
	AudioToAAC     = [...]string{"-c:a", "aac"}
	AudioBitrate   = [...]string{"-b:a", "256k"}
)

// Video flags
var (
	VideoCodecCopy      = [...]string{"-c:v", "copy"}
	VideoToH264Balanced = [...]string{"-c:v", "libx264", "-crf", "23", "-profile:v", "main"}
	PixelFmtYuv420p     = [...]string{"-pix_fmt", "yuv420p"}
	KeyframeBalanced    = [...]string{"-g", "50", "-keyint_min", "30"}
)

// GPU hardware flags
var (
	NvidiaAccel = [...]string{"-hwaccel", "nvdec"}
	AMDAccel    = [...]string{"-hwaccel", "vulkan"}
	IntelAccel  = [...]string{"-hwaccel", "qsv"}
)
package consts

const (
	JOverrideCredits = "all-credits"
	JActor           = "actor"
	JAuthor          = "author"
	JArtist          = "artist"
	JChannel         = "channel"
	JComposer        = "composer"
	JCreator         = "creator"
	JDirector        = "director"
	JPerformer       = "performer"
	JProducer        = "producer"
	JPublisher       = "publisher"
	JStudio          = "studio"
	JUploader        = "uploader"
	JWriter          = "writer"
)

const (
	JComment          = "comment"
	JDescription      = "description"
	JFulltitle        = "fulltitle"
	JLongDescription  = "longdescription"
	JLong_Description = "long_description"
	JSubtitle         = "subtitle"
	JSummary          = "summary"
	JSynopsis         = "synopsis"
	JTitle            = "title"
)

const (
	JCreationTime        = "creation_time"
	JDate                = "date"
	JFormattedDate       = "formatted_date"
	JOriginallyAvailable = "originally_available_at"
	JReleaseDate         = "release_date"
	JUploadDate          = "upload_date"
	JYear                = "year"
	JReleaseYear         = "release_year"
)

const (
	JDomain        = "domain"
	JReferer       = "referer"
	JURL           = "url"
	JWebpageDomain = "webpage_url_domain"
	JWebpageURL    = "webpage_url"
)
package consts

// Log file keys
const (
	LogFinished = "FINISHED: "
	LogError    = "ERROR: "
	LogFailure  = "FAILED: "
	LogSuccess  = "Success: "
	LogInfo     = "Info: "
	LogWarning  = "Warning: "
	LogBasic    = ""
)
package consts

const (
	// Core Descriptive Metadata
	NTitle         = "title"
	NOriginalTitle = "originaltitle"
	NSortTitle     = "sorttitle"
	NTagline       = "tagline"
	NDescription   = "description"
	NPlot          = "plot"
	NOutline       = "outline"
	NShowTitle     = "showtitle"
	NSubtitle      = "subtitle"

	// Cast and Crew Metadata
	NActors   = "actor"
	NDirector = "director"
	NWriter   = "writer"
	NComposer = "composer"
	NProducer = "producer"

	// Genre, Category, and Rating Metadata
	NGenre      = "genre"
	NMood       = "mood"
	NMPAA       = "mpaa"
	NVotes      = "votes"
	NRatingsURL = "ratingurl"

	// Date and Release Metadata
	NAired        = "aired"
	NPremiereDate = "premiered"
	NYear         = "year"

	// Episodic Metadata
	NSeason       = "season"
	NEpisode      = "episode"
	NEpisodeTitle = "episodetitle"

	// Technical Information
	NCountry   = "country"
	NLanguage  = "language"
	NRated     = "rated"
	NEncodedBy = "encodedby"
	NRuntime   = "runtime"
	NRating    = "rating"

	// Production Metadata
	NProductionCompany = "productioncompany"
	NStudio            = "studio"
	NCoverArtist       = "coverartist"
	NPublisher         = "publisher"
	NCompilation       = "compilation"

	// Artwork, Media Assets, and Related Links
	NThumb    = "thumb"
	NFanart   = "fanart"
	NTrailer  = "trailer"
	NCoverArt = "cover_art"

	// Sorting and Alternate Display Titles
	NShowSortTitle = "showsorttitle"

	// Miscellaneous
	NComment = "comment"
	NTop250  = "top250"
	NTrack   = "track"
	NAlbum   = "album"
	NLicence = "license"
	NRights  = "rights"
	NURL     = "url"
)
package enums

// User selection of filetypes to convert from
type ConvertFromFiletype int

const (
	VID_EXTS_ALL ConvertFromFiletype = iota
	VID_EXTS_MKV
	VID_EXTS_MP4
	VID_EXTS_WEBM
)

// MetaFiletypeFilter filters the metadata files to read from
type MetaFiletypeFilter int

const (
	META_EXTS_ALL MetaFiletypeFilter = iota
	META_EXTS_JSON
	META_EXTS_NFO
)

// OverrideMetaType holds the value for the type of metafield to override all values of (e.g. "credits")
type OverrideMetaType int

const (
	OVERRIDE_META_NONE OverrideMetaType = iota
	OVERRIDE_META_CREDITS
)

type MetaOpType int

const (
	METAOPS_NONE MetaOpType = iota
	METAOPS_SET
)

// User system graphics hardware for transcoding
type SysGPU int

const (
	GPU_NO_HW_ACCEL SysGPU = iota
	GPU_AMD
	GPU_INTEL
	GPU_NVIDIA
)

// Naming syle
type ReplaceToStyle int

const (
	RENAMING_SKIP ReplaceToStyle = iota
	RENAMING_UNDERSCORES
	RENAMING_FIXES_ONLY
	RENAMING_SPACES
)

// Date formats
type DateFormat int

const (
	DATEFMT_SKIP DateFormat = iota
	DATEFMT_YYYY_MM_DD
	DATEFMT_YY_MM_DD
	DATEFMT_YYYY_DD_MM
	DATEFMT_YY_DD_MM
	DATEFMT_DD_MM_YYYY
	DATEFMT_DD_MM_YY
	DATEFMT_MM_DD_YYYY
	DATEFMT_MM_DD_YY
	DATEFMT_DD_MM
	DATEFMT_MM_DD
)

// Date tag location
type MetaDateTagLocation int

const (
	DATE_TAG_LOC_PFX MetaDateTagLocation = iota
	DATE_TAG_LOC_SFX
)

type MetaDateTaggingType int

const (
	DATE_TAG_ADD_OP MetaDateTaggingType = iota
	DATE_TAG_DEL_OP
)

// Web tags
type MetaFiletypeFound int

const (
	METAFILE_JSON MetaFiletypeFound = iota
	METAFILE_NFO
)

// Viper variable types
type ViperVarTypes int

const (
	VIPER_ANY ViperVarTypes = iota
	VIPER_BOOL
	VIPER_INT
	VIPER_STRING
	VIPER_STRING_SLICE
)

// Purge metafile types
type PurgeMetafiles int

const (
	PURGEMETA_ALL PurgeMetafiles = iota
	PURGEMETA_JSON
	PURGEMETA_NFO
	PURGEMETA_NONE
)

// Web tags
type WebClassTags int

const (
	WEBCLASS_DATE WebClassTags = iota
	WEBCLASS_TITLE
	WEBCLASS_DESCRIPTION
	WEBCLASS_CREDITS
	WEBCLASS_WEBINFO
)

// Presets
type SitePresets int

const (
	PRESET_CENSOREDTV SitePresets = iota
)
package keys

// Terminal keys
const (
	BatchPairsInput string = "batch-pairs"
	// VideoDir        string = "video-dir"
	// VideoFile       string = "video-file"

	VideoDirs  string = "video-dir"
	VideoFiles string = "video-file"
	JsonDirs   string = "json-dir"
	JsonFiles  string = "json-file"

	// JsonDir   string = "json-dir"
	// JsonFile  string = "json-file"
	MetaPurge string = "purge-metafile"

	CookiePath string = "cookie-dir"

	InputMetaExts  string = "input-meta-exts"
	InputVideoExts string = "input-video-exts"
	FilePrefixes   string = "prefix"

	Concurrency string = "concurrency"
	GPU         string = "gpu"
	MaxCPU      string = "max-cpu"
	MinMem      string = "min-mem"
	MinMemMB    string = "min-mem-mb"

	InputFilenameReplaceSfx string = "filename-replace-suffix"
	InputFileDatePfx        string = "filename-date-tag"
	RenameStyle             string = "rename-style"
	MFilenamePfx            string = "metadata-filename-prefix"

	MetaOps      string = "meta-ops"
	MDescDatePfx string = "desc-date-prefix"
	MDescDateSfx string = "desc-date-suffix"

	DebugLevel      string = "debug"
	SkipVideos      string = "skip-videos"
	NoFileOverwrite string = "no-file-overwrite"

	Benchmarking        string = "benchmark"
	OutputFiletypeInput string = "ext"
	MoveOnComplete      string = "output-directory"
	InputPreset         string = "preset"
)

// Primary program
const (
	WaitGroup  string = "WaitGroup"
	SingleFile string = "SingleFile"
)

// Files and directories
const (
	BatchPairs     string = "batchPairs"
	OpenVideo      string = "openVideo"
	OpenJson       string = "openJson"
	MetaPurgeEnum  string = "metaPurgeEnum"
	OutputFiletype string = "outputFiletype"
)

// Filter for files
const (
	InputVExtsEnum string = "inputVideoExtsEnum"
	InputMExtsEnum string = "inputMetaExtsEnum"
)

// Performance
const (
	GPUEnum string = "gpuEnum"
)

// Filename edits
const (
	Rename             string = "Rename"
	FileDateFmt        string = "filenameDateTag"
	FilenameReplaceSfx string = "filenameReplaceSfx"
)

// Meta edits
const (
	MOverwrite string = "meta-overwrite"
	MPreserve  string = "meta-preserve"

	MCopyToField    string = "copyToField"
	MPasteFromField string = "pasteFromField"

	MAppend      string = "metaAppend"
	MNewField    string = "metaNewField"
	MPrefix      string = "metaPrefix"
	MReplaceText string = "metaReplaceText"
	MTrimPrefix  string = "metaTrimPrefix"
	MTrimSuffix  string = "metaTrimSuffix"

	MDateTagMap    string = "metaDateTagMap"
	MDelDateTagMap string = "metaDelDateTagMap"
)

// Contains the fields which accept multiple entries as string arrays
var MultiEntryFields = []string{
	InputVideoExts,
	InputMetaExts,
	FilePrefixes,
	FilenameReplaceSfx,
}
package regex

import (
	"metarr/internal/domain/consts"
	"metarr/internal/models"
	"regexp"
	"sync"
)

var (
	AnsiEscape                *regexp.Regexp
	ExtraSpaces               *regexp.Regexp
	InvalidChars              *regexp.Regexp
	SpecialChars              *regexp.Regexp
	ContractionMapSpaced      map[string]models.ContractionPattern
	ContractionMapUnderscored map[string]models.ContractionPattern
	ContractionMapAll         map[string]models.ContractionPattern

	// Initialize sync.Once for each compilation
	ansiEscapeOnce              sync.Once
	extraSpacesOnce             sync.Once
	invalidCharsOnce            sync.Once
	specialCharsOnce            sync.Once
	contractionsSpacedOnce      sync.Once
	contractionsUnderscoredOnce sync.Once
	contractionsAllOnce         sync.Once
	contractionMu               sync.RWMutex
)

// ContractionMapAllCompile compiles the regex pattern for spaced AND underscored contractions and returns
// a model containing the regex and the replacement
func ContractionMapAllCompile() map[string]models.ContractionPattern {
	contractionsAllOnce.Do(func() {
		contractionMu.Lock()
		defer contractionMu.Unlock()

		ContractionMapAll = make(map[string]models.ContractionPattern, len(consts.ContractionsSpaced)+len(consts.ContractionsUnderscored))
		// Spaced map
		for contraction, replacement := range consts.ContractionsSpaced {
			ContractionMapAll[contraction] = models.ContractionPattern{
				Regexp:      regexp.MustCompile(`\b` + regexp.QuoteMeta(contraction) + `\b`),
				Replacement: replacement,
			}
		}
		// Underscored map
		for contraction, replacement := range consts.ContractionsUnderscored {
			ContractionMapAll[contraction] = models.ContractionPattern{
				Regexp:      regexp.MustCompile(`\b` + regexp.QuoteMeta(contraction) + `\b`),
				Replacement: replacement,
			}
		}
	})
	return ContractionMapAll
}

// ContractionMapSpacesCompile compiles the regex pattern for spaced contractions and returns
// a model containing the regex and the replacement
func ContractionMapSpacesCompile() map[string]models.ContractionPattern {
	contractionsSpacedOnce.Do(func() {
		contractionMu.Lock()
		defer contractionMu.Unlock()

		ContractionMapSpaced = make(map[string]models.ContractionPattern, len(consts.ContractionsSpaced))
		for contraction, replacement := range consts.ContractionsSpaced {
			ContractionMapSpaced[contraction] = models.ContractionPattern{
				Regexp:      regexp.MustCompile(`\b` + regexp.QuoteMeta(contraction) + `\b`),
				Replacement: replacement,
			}
		}
	})
	return ContractionMapSpaced
}

// ContractionMapUnderscoresCompile compiles the regex pattern for underscored contractions and returns
// a model containing the regex and the replacement
func ContractionMapUnderscoresCompile() map[string]models.ContractionPattern {
	contractionsUnderscoredOnce.Do(func() {
		contractionMu.Lock()
		defer contractionMu.Unlock()

		ContractionMapUnderscored = make(map[string]models.ContractionPattern, len(consts.ContractionsUnderscored))
		for contraction, replacement := range consts.ContractionsUnderscored {
			ContractionMapUnderscored[contraction] = models.ContractionPattern{
				Regexp:      regexp.MustCompile(`\b` + regexp.QuoteMeta(contraction) + `\b`),
				Replacement: replacement,
			}
		}
	})
	return ContractionMapUnderscored
}

// AnsiEscapeCompile compiles regex for ANSI escape codes
func AnsiEscapeCompile() *regexp.Regexp {
	ansiEscapeOnce.Do(func() {
		contractionMu.Lock()
		defer contractionMu.Unlock()

		AnsiEscape = regexp.MustCompile(`\x1b\[[0-9;]*m`)
	})
	return AnsiEscape
}

// ExtraSpacesCompile compiles regex for extra spaces
func ExtraSpacesCompile() *regexp.Regexp {
	extraSpacesOnce.Do(func() {
		contractionMu.Lock()
		defer contractionMu.Unlock()

		ExtraSpaces = regexp.MustCompile(`\s+`)
	})
	return ExtraSpaces
}

// InvalidCharsCompile compiles regex for invalid characters
func InvalidCharsCompile() *regexp.Regexp {
	invalidCharsOnce.Do(func() {
		contractionMu.Lock()
		defer contractionMu.Unlock()

		InvalidChars = regexp.MustCompile(`[<>:"/\\|?*\x00-\x1F]`)
	})
	return InvalidChars
}

// SpecialCharsCompile compiles regex for special characters
func SpecialCharsCompile() *regexp.Regexp {
	specialCharsOnce.Do(func() {
		contractionMu.Lock()
		defer contractionMu.Unlock()

		SpecialChars = regexp.MustCompile(`[^\w\s-]`)
	})
	return SpecialChars
}
package templates

const (
	Year  = "year"
	Month = "month"
	Day   = "day"
)

const (
	Author   = "author"
	Director = "director"
)

const (
	Domain = "domain"
)
package ffmpeg

import (
	"metarr/internal/cfg"
	"metarr/internal/domain/consts"
	"metarr/internal/domain/enums"
	"metarr/internal/domain/keys"
	"metarr/internal/models"
	"metarr/internal/utils/logging"
	"path/filepath"
	"strings"
)

// ffCommandBuilder handles FFmpeg command construction
type ffCommandBuilder struct {
	inputFile   string
	outputFile  string
	formatFlags []string
	gpuAccel    []string
	metadataMap map[string]string
	builder     *strings.Builder
}

// newFfCommandBuilder creates a new FFmpeg command builder
func newFfCommandBuilder(fd *models.FileData, outputFile string) *ffCommandBuilder {
	return &ffCommandBuilder{
		builder:     &strings.Builder{},
		inputFile:   fd.OriginalVideoPath,
		outputFile:  outputFile,
		metadataMap: make(map[string]string),
	}
}

// buildCommand constructs the complete FFmpeg command
func (b *ffCommandBuilder) buildCommand(fd *models.FileData, outExt string) ([]string, error) {

	b.setGPUAcceleration()
	b.addAllMetadata(fd)
	b.setFormatFlags(outExt)

	// Return the fully appended argument string
	return b.buildFinalCommand()
}

// addAllMetadata combines all metadata into a single map
func (b *ffCommandBuilder) addAllMetadata(fd *models.FileData) {

	b.addTitlesDescs(fd.MTitleDesc)
	b.addCredits(fd.MCredits)
	b.addDates(fd.MDates)
	b.addShowInfo(fd.MShowData)
	b.addOtherMetadata(fd.MOther)
}

// addTitlesDescs adds all title/description-related metadata
func (b *ffCommandBuilder) addTitlesDescs(t *models.MetadataTitlesDescs) {

	// Prefer fulltitle if possible (also exists in the JSON processing func)
	if t.Title == "" && t.Fulltitle != "" {
		t.Title = t.Fulltitle
	}

	if t.LongDescription == "" && t.Long_Description != "" {
		t.LongDescription = t.Long_Description
	}

	fields := map[string]string{
		consts.JTitle:           t.Title,
		consts.JSubtitle:        t.Subtitle,
		consts.JDescription:     t.Description,
		consts.JLongDescription: t.LongDescription,
		consts.JSummary:         t.Summary,
		consts.JSynopsis:        t.Synopsis,
	}

	for field, value := range fields {
		if value != "" {
			b.metadataMap[field] = value
		}
	}
}

// addCredits adds all credit-related metadata
func (b *ffCommandBuilder) addCredits(c *models.MetadataCredits) {

	// Single value credits
	fields := map[string]string{
		consts.JActor:     c.Actor,
		consts.JAuthor:    c.Author,
		consts.JArtist:    c.Artist,
		consts.JCreator:   c.Creator,
		consts.JStudio:    c.Studio,
		consts.JPublisher: c.Publisher,
		consts.JProducer:  c.Producer,
		consts.JPerformer: c.Performer,
		consts.JComposer:  c.Composer,
		consts.JDirector:  c.Director,
		consts.JWriter:    c.Writer,
	}

	for field, value := range fields {
		if value != "" {
			b.metadataMap[field] = value
		}
	}

	// Array credits (length already checked in function)
	b.addArrayMetadata(consts.JActor, c.Actors)
	b.addArrayMetadata(consts.JComposer, c.Composers)
	b.addArrayMetadata(consts.JArtist, c.Artists)
	b.addArrayMetadata(consts.JStudio, c.Studios)
	b.addArrayMetadata(consts.JPerformer, c.Performers)
	b.addArrayMetadata(consts.JProducer, c.Producers)
	b.addArrayMetadata(consts.JPublisher, c.Publishers)
	b.addArrayMetadata(consts.JDirector, c.Directors)
	b.addArrayMetadata(consts.JWriter, c.Writers)
}

// addDates adds all date-related metadata
func (b *ffCommandBuilder) addDates(d *models.MetadataDates) {

	fields := map[string]string{
		consts.JCreationTime:        d.Creation_Time,
		consts.JDate:                d.Date,
		consts.JOriginallyAvailable: d.Originally_Available_At,
		consts.JReleaseDate:         d.ReleaseDate,
		consts.JUploadDate:          d.UploadDate,
		consts.JYear:                d.Year,
	}

	for field, value := range fields {
		if value != "" {
			b.metadataMap[field] = value
		}
	}
}

// addShowInfo adds all show info related metadata
func (b *ffCommandBuilder) addShowInfo(s *models.MetadataShowData) {

	fields := map[string]string{
		"episode_id":    s.Episode_ID,
		"episode_sort":  s.Episode_Sort,
		"season_number": s.Season_Number,
		"season_title":  s.Season_Title,
		"show":          s.Show,
	}

	for field, value := range fields {
		if value != "" {
			b.metadataMap[field] = value
		}
	}
}

// addOtherMetadata adds other related metadata
func (b *ffCommandBuilder) addOtherMetadata(o *models.MetadataOtherData) {

	fields := map[string]string{
		"genre":    o.Genre,
		"hd_video": o.HD_Video,
		"language": o.Language,
	}

	for field, value := range fields {
		if value != "" {
			b.metadataMap[field] = value
		}
	}
}

// addArrayMetadata combines array values with existing metadata
func (b *ffCommandBuilder) addArrayMetadata(key string, values []string) {
	if len(values) == 0 {
		return
	}

	existing, exists := b.metadataMap[key]
	newValue := strings.Join(values, "; ")

	b.builder.Reset()
	if exists && existing != "" {
		for i, v := range values {
			if i > 0 {
				b.builder.WriteString("; ")
			}
			b.builder.WriteString(v)
		}
	} else {
		b.metadataMap[key] = newValue
	}
}

// setGPUAcceleration sets appropriate GPU acceleration flags
func (b *ffCommandBuilder) setGPUAcceleration() {
	if cfg.IsSet(keys.GPUEnum) {
		gpuFlag, ok := cfg.Get(keys.GPUEnum).(enums.SysGPU)
		if ok {
			switch gpuFlag {
			case enums.GPU_NVIDIA:
				b.gpuAccel = consts.NvidiaAccel[:]
			case enums.GPU_AMD:
				b.gpuAccel = consts.AMDAccel[:]
			case enums.GPU_INTEL:
				b.gpuAccel = consts.IntelAccel[:]
			}
		}
	}
}

// setFormatFlags adds commands specific for the extension input and output
func (b *ffCommandBuilder) setFormatFlags(outExt string) {
	inExt := strings.ToLower(filepath.Ext(b.inputFile))
	outExt = strings.ToLower(outExt)

	if outExt == "" || strings.EqualFold(inExt, outExt) {
		b.formatFlags = copyPreset.flags
		return
	}

	logging.I("Input extension: %q, output extension: %q, File: %s",
		inExt, outExt, b.inputFile)

	// Get format preset from map
	if presets, exists := formatMap[outExt]; exists {
		// Try exact input format match
		if preset, exists := presets[inExt]; exists {
			b.formatFlags = preset.flags
			return
		}
		// Fall back to default preset for this output format
		if preset, exists := presets["*"]; exists {
			b.formatFlags = preset.flags
			return
		}
	}

	// Fall back to copy preset if no mapping found
	b.formatFlags = copyPreset.flags
	logging.D(1, "No format mapping found for %s to %s conversion, using copy preset",
		inExt, outExt)
}

// buildFinalCommand assembles the final FFmpeg command
func (b *ffCommandBuilder) buildFinalCommand() ([]string, error) {

	args := make([]string, 0, calculateCommandCapacity(b))

	if len(b.gpuAccel) > 0 {
		args = append(args, b.gpuAccel...)
	}

	if b.inputFile != "" {
		args = append(args, "-y", "-i", b.inputFile)
	}

	// Add all -metadata commands
	for key, value := range b.metadataMap {

		// Reset builder
		b.builder.Reset()
		b.builder.WriteString(key)
		b.builder.WriteByte('=')
		b.builder.WriteString(strings.TrimSpace(value))

		// Write argument
		logging.I("Adding metadata argument: '-metadata %s", b.builder.String())
		args = append(args, "-metadata", b.builder.String())
	}

	if len(b.formatFlags) > 0 {
		args = append(args, b.formatFlags...)
	}

	if b.outputFile != "" {
		args = append(args, b.outputFile)
	}

	return args, nil
}

// calculateCommandCapacity determines the total length needed for the command
func calculateCommandCapacity(b *ffCommandBuilder) int {
	const (
		inputFlags   = 2 // "-y", "-i"
		inputFile    = 1 // input file
		formatFlag   = 1 // "-codec"
		outputFile   = 1 // output file
		metadataFlag = 1 // "-metadata" for each metadata entry
		keyValuePair = 1 // "key=value" for each metadata entry
	)

	totalCapacity := len(b.gpuAccel) + // GPU acceleration flags if any
		inputFlags + inputFile + // Input related flags and file
		(len(b.metadataMap) * (metadataFlag + keyValuePair)) + // Metadata entries
		len(b.formatFlags) + // Format flags (like -codec copy)
		outputFile

	logging.D(3, "Total command capacity calculated as: %d", totalCapacity)
	return totalCapacity
}
package ffmpeg

import (
	"metarr/internal/domain/consts"
	"metarr/internal/utils/logging"
)

// formatPreset holds a pre-calculated set of ffmpeg flags
type formatPreset struct {
	flags []string
}

var (
	// Direct copy preset
	copyPreset = formatPreset{
		flags: consts.AVCodecCopy[:],
	}

	// Standard h264 conversion
	h264Preset = formatPreset{
		flags: concat(
			consts.VideoToH264Balanced[:],
			consts.PixelFmtYuv420p[:],
			consts.AudioToAAC[:],
			consts.AudioBitrate[:],
		),
	}

	// Video copy with AAC audio
	videoCopyAACPreset = formatPreset{
		flags: concat(
			consts.VideoCodecCopy[:],
			consts.AudioToAAC[:],
			consts.AudioBitrate[:],
		),
	}

	// Full webm conversion preset
	webmPreset = formatPreset{
		flags: concat(
			consts.VideoToH264Balanced[:],
			consts.PixelFmtYuv420p[:],
			consts.KeyframeBalanced[:],
			consts.AudioToAAC[:],
			consts.AudioBitrate[:],
		),
	}
)

var formatMap = map[string]map[string]formatPreset{
	consts.ExtAVI: {
		consts.ExtAVI:  copyPreset,
		consts.ExtMP4:  videoCopyAACPreset,
		consts.ExtM4V:  videoCopyAACPreset,
		consts.ExtMOV:  videoCopyAACPreset,
		consts.ExtRM:   webmPreset,
		consts.ExtRMVB: webmPreset,
		"*":            h264Preset, // default preset
	},
	consts.ExtMP4: {
		consts.ExtMP4:  copyPreset,
		consts.ExtMKV:  videoCopyAACPreset,
		consts.ExtWEBM: webmPreset,
		"*":            h264Preset,
	},
	consts.ExtMKV: {
		consts.ExtMKV: copyPreset,
		consts.ExtMP4: videoCopyAACPreset,
		consts.ExtM4V: videoCopyAACPreset,
		"*":           h264Preset,
	},
	consts.ExtWEBM: {
		consts.ExtWEBM: copyPreset,
		"*":            webmPreset,
	},
}

// concat combines multiple string slices into one
func concat(slices ...[]string) []string {
	var totalLen int
	for _, s := range slices {
		totalLen += len(s)
	}

	result := make([]string, 0, totalLen)
	for _, s := range slices {
		result = append(result, s...)
	}

	logging.D(2, "Made format flag array %v", result)
	return result
}
package ffmpeg

import (
	"context"
	"fmt"
	"metarr/internal/cfg"
	"metarr/internal/domain/consts"
	"metarr/internal/domain/keys"
	"metarr/internal/models"
	backup "metarr/internal/utils/fs/backup"
	"metarr/internal/utils/logging"
	validate "metarr/internal/utils/validation"
	"os"
	"os/exec"
	"path/filepath"
	"strings"
)

// executeVideo writes metadata to a single video file
func ExecuteVideo(ctx context.Context, fd *models.FileData) error {
	var (
		tmpOutPath, outExt string
	)

	origPath := fd.OriginalVideoPath
	origExt := filepath.Ext(origPath)

	// Extension validation - now checks length and format immediately
	if cfg.IsSet(keys.OutputFiletype) {
		if outExt = validate.ValidateExtension(cfg.GetString(keys.OutputFiletype)); outExt == "" {
			logging.E(0, "Grabbed output extension but extension was empty/invalid, reverting to original: %s", origExt)
			outExt = origExt
		}
	} else {
		outExt = origExt
	}

	logging.I("Will execute video from extension %q to extension %q", origExt, outExt)

	if dontProcess(fd, outExt) {
		return nil
	}

	fmt.Printf("\nWriting metadata for file: %s\n", origPath)

	dir := fd.VideoDirectory
	fileBase := strings.TrimSuffix(filepath.Base(origPath), origExt)

	// Make temp output path
	tmpOutPath = filepath.Join(dir, consts.TempTag+fileBase+origExt+outExt)
	logging.D(3, "Orig ext: %q, Out ext: %q", origExt, outExt)

	// Add temp path to data struct
	fd.TempOutputFilePath = tmpOutPath

	defer func() {
		if _, err := os.Stat(tmpOutPath); err == nil {
			os.Remove(tmpOutPath)
		}
	}()

	// Build FFmpeg command
	builder := newFfCommandBuilder(fd, tmpOutPath)
	args, err := builder.buildCommand(fd, outExt)
	if err != nil {
		return err
	}

	command := exec.CommandContext(ctx, "ffmpeg", args...)
	logging.I("%sConstructed FFmpeg command for%s %q:\n\n%v\n", consts.ColorCyan, consts.ColorReset, fd.OriginalVideoPath, command.String())

	command.Stdout = os.Stdout
	command.Stderr = os.Stderr

	// Set final video path and base name in model
	fd.FinalVideoBaseName = strings.TrimSuffix(filepath.Base(origPath), filepath.Ext(origPath))
	fd.FinalVideoPath = filepath.Join(fd.VideoDirectory, fd.FinalVideoBaseName) + outExt

	logging.I("Video file path data:\n\nOriginal Video Path: %s\nMetadata File Path: %s\nFinal Video Path: %s\n\nTemp Output Path: %s", origPath,
		fd.JSONFilePath,
		fd.FinalVideoPath,
		fd.TempOutputFilePath)

	// Run the ffmpeg command
	logging.P("%s!!! Starting FFmpeg command for %q...\n%s", consts.ColorCyan, fd.FinalVideoBaseName, consts.ColorReset)
	if err := command.Run(); err != nil {
		logging.ErrorArray = append(logging.ErrorArray, err)
		return fmt.Errorf("failed to run FFmpeg command: %w", err)
	}

	// Rename temporary file to overwrite the original video file
	if filepath.Ext(origPath) != filepath.Ext(fd.FinalVideoPath) {
		logging.I("Original file not type %s, removing %q", outExt, origPath)

	} else if cfg.GetBool(keys.NoFileOverwrite) && origPath == fd.FinalVideoPath {
		if err := makeBackup(origPath); err != nil {
			return err
		}
	}

	// Delete original after potential backup ops
	err = os.Remove(origPath)
	if err != nil {
		logging.ErrorArray = append(logging.ErrorArray, err)
		return fmt.Errorf("failed to remove original file (%s). Error: %v", origPath, err)
	}

	//
	err = os.Rename(tmpOutPath, fd.FinalVideoPath)
	if err != nil {
		return fmt.Errorf("failed to rename temp file: %w", err)
	}

	logging.S(0, "Successfully processed video:\n\nOriginal file: %s\nNew file: %s\n\nTitle: %s", origPath,
		fd.FinalVideoPath,
		fd.MTitleDesc.Title)

	return nil
}

// dontProcess determines whether the program should process this video (meta already exists and file extensions are unchanged)
func dontProcess(fd *models.FileData, outExt string) (dontProcess bool) {
	if fd.MetaAlreadyExists {

		logging.I("Metadata already exists in the file, skipping processing...")
		origPath := fd.OriginalVideoPath
		fd.FinalVideoBaseName = strings.TrimSuffix(filepath.Base(origPath), filepath.Ext(origPath))

		// Save final video path into model
		fd.FinalVideoPath = filepath.Join(fd.VideoDirectory, fd.FinalVideoBaseName) + outExt
		return true
	}
	return dontProcess
}

// makeBackup performs the backup
func makeBackup(origPath string) error {

	origInfo, err := os.Stat(origPath)
	if os.IsNotExist(err) {
		logging.I("File does not exist, safe to proceed overwriting: %s", origPath)
		return nil
	}

	backupPath, err := backup.RenameToBackup(origPath)
	if err != nil {
		return fmt.Errorf("failed to rename original file and preserve file is on, aborting: %w", err)
	}

	backInfo, err := os.Stat(backupPath)
	if os.IsNotExist(err) {
		return fmt.Errorf("backup file was not created, aborting")
	}

	if origInfo.Size() != backInfo.Size() {
		return fmt.Errorf("backup file size does not match original, aborting")
	}

	return nil
}
package metadata

import (
	"metarr/internal/domain/consts"
	"metarr/internal/domain/enums"
	"metarr/internal/models"
	browser "metarr/internal/utils/browser"
	"metarr/internal/utils/logging"
	"metarr/internal/utils/printout"
	"strings"
)

// fillCredits fills in the metadator for credits (e.g. actor, director, uploader)
func fillCredits(fd *models.FileData, json map[string]any) (map[string]any, bool) {
	var (
		filled, overriden bool
	)

	c := fd.MCredits
	w := fd.MWebData

	// Order by importance
	fieldMap := map[string]*string{
		consts.JCreator:   &c.Creator,
		consts.JPerformer: &c.Performer,
		consts.JAuthor:    &c.Author,
		consts.JArtist:    &c.Artist, // May be alias for "author" in some systems
		consts.JChannel:   &c.Channel,
		consts.JDirector:  &c.Director,
		consts.JActor:     &c.Actor,
		consts.JStudio:    &c.Studio,
		consts.JProducer:  &c.Producer,
		consts.JWriter:    &c.Writer,
		consts.JUploader:  &c.Uploader,
		consts.JPublisher: &c.Publisher,
		consts.JComposer:  &c.Composer,
	}

	var printMap map[string]string
	if logging.Level > 1 {
		printMap = make(map[string]string, len(fieldMap))
		defer func() {
			if len(printMap) > 0 {
				printout.PrintGrabbedFields("credits", printMap)
			}
		}()
	}

	// Set using override will fill all values anyway
	if len(models.SetOverrideMap) == 0 {
		if filled = unpackJSON(fieldMap, json); filled {
			logging.D(2, "Decoded credits JSON into field map")
		}

		// Find highest priority filled element
		fillWith := c.Override
		if fillWith == "" {
			for _, ptr := range fieldMap {
				if ptr == nil {
					continue
				}
				if *ptr != "" {
					fillWith = *ptr
					break
				}
			}
		}

		// Check if filled
		for k, ptr := range fieldMap {
			if ptr == nil {
				logging.E(0, "Unexpected nil pointer in credits fieldMap")
				continue
			}

			if *ptr != "" {
				if logging.Level > 1 {
					printMap[k] = *ptr
				}
				filled = true
				continue
			}
			logging.D(2, "Value for %q is empty, attempting to fill by inference...", k)

			*ptr = fillWith
			if logging.Level > 1 {
				printMap[k] = *ptr
			}
			logging.D(2, "Set value to %q", *ptr)
		}
	}

	if printMap, overriden = overrideAll(fieldMap, printMap); overriden {
		if !filled {
			filled = overriden
		}
	}

	// Return if data filled or no web data, else scrape
	switch {
	case filled:
		rtn, err := fd.JSONFileRW.WriteJSON(fieldMap)
		if err != nil {
			logging.E(0, "Failed to write into JSON file %q: %v", fd.JSONFilePath, err)
			return json, true
		}

		if rtn != nil {
			return rtn, true
		}
		return json, true

	case w.WebpageURL == "":
		logging.I("Page URL not found in metadata, so cannot scrape for missing credits in %q", fd.JSONFilePath)
		return json, false
	}

	// Scrape for missing data (write back to file if found)
	credits := browser.ScrapeMeta(w, enums.WEBCLASS_CREDITS)
	if credits != "" {
		for _, value := range fieldMap {
			if *value == "" {
				*value = credits
			}
		}

		rtn, err := fd.JSONFileRW.WriteJSON(fieldMap)
		if err != nil {
			logging.E(0, "Failed to write new metadata (%s) into JSON file %q: %v", credits, fd.JSONFilePath, err)
			return json, true
		}

		if rtn != nil {
			json = rtn
			return json, true
		}
	}
	return json, false
}

// overrideAll makes override replacements if existent
func overrideAll(fieldMap map[string]*string, printMap map[string]string) (map[string]string, bool) {
	logging.D(2, "Checking credits field overrides...")
	if fieldMap == nil {
		logging.E(0, "fieldMap passed in null")
		return printMap, false
	}

	filled := false

	// Note order of operations
	if len(models.ReplaceOverrideMap) > 0 {
		logging.I("Overriding credits with text replacements...")
		if m, exists := models.ReplaceOverrideMap[enums.OVERRIDE_META_CREDITS]; exists {
			for k, ptr := range fieldMap {
				if ptr == nil {
					logging.E(0, "Entry is nil in fieldMap %v", fieldMap)
					continue
				}
				logging.I("Overriding old %q by replacing %q with %q", *ptr, m.Value, m.Replacement)
				*ptr = strings.ReplaceAll(*ptr, m.Value, m.Replacement)

				if logging.Level > 1 {
					printMap[k] = *ptr
				}

				filled = true
			}
		}
	}

	if len(models.SetOverrideMap) > 0 {
		logging.I("Overriding credits with new values...")
		if val, exists := models.SetOverrideMap[enums.OVERRIDE_META_CREDITS]; exists {
			for k, ptr := range fieldMap {
				if ptr == nil {
					logging.E(0, "Entry is nil in fieldMap %v", fieldMap)
					continue
				}
				logging.I("Overriding old %q to %q", *ptr, val)
				*ptr = val

				if logging.Level > 1 {
					printMap[k] = *ptr
				}

				filled = true
			}
		}
	}

	if len(models.AppendOverrideMap) > 0 {
		logging.I("Overriding credits with appends...")
		if val, exists := models.AppendOverrideMap[enums.OVERRIDE_META_CREDITS]; exists {
			for k, ptr := range fieldMap {
				if ptr == nil {
					logging.E(0, "Entry is nil in fieldMap %v", fieldMap)
					continue
				}

				logging.I("Overriding old %q by appending it with %q", *ptr, val)
				*ptr += val

				if logging.Level > 1 {
					printMap[k] = *ptr
				}

				filled = true
			}
		}
	}

	return printMap, filled
}
package metadata

import (
	"metarr/internal/dates"
	"metarr/internal/domain/consts"
	"metarr/internal/domain/enums"
	"metarr/internal/models"
	browser "metarr/internal/utils/browser"
	"metarr/internal/utils/logging"
	"metarr/internal/utils/printout"
	"strings"
)

// fillTimestamps grabs timestamp metadata from JSON
func FillTimestamps(fd *models.FileData, json map[string]any) bool {

	t := fd.MDates
	w := fd.MWebData

	fieldMap := map[string]*string{ // Order by importance
		consts.JReleaseDate:         &t.ReleaseDate,
		consts.JOriginallyAvailable: &t.Originally_Available_At,
		consts.JDate:                &t.Date,
		consts.JUploadDate:          &t.UploadDate,
		consts.JReleaseYear:         &t.Year,
		consts.JYear:                &t.Year,
		consts.JCreationTime:        &t.Creation_Time,
	}

	if ok := unpackJSON(fieldMap, json); !ok {
		logging.E(1, "Failed to unpack date JSON, no dates currently exist in file?")
	}

	var printMap map[string]string
	if logging.Level > 1 {
		printMap = make(map[string]string, len(fieldMap))
		defer func() {
			if len(printMap) > 0 {
				printout.PrintGrabbedFields("dates", printMap)
			}
		}()
	}

	var gotDate bool
	for k, ptr := range fieldMap {
		if ptr == nil {
			logging.E(0, "Unexpected nil pointer in fieldMap")
			continue
		}

		v, exists := json[k]
		if !exists {
			continue
		}

		val, ok := v.(string)
		if !ok {
			continue
		}

		var finalVal string
		if len(val) >= 6 {
			if formatted, ok := dates.YyyyMmDd(val); ok {
				finalVal = formatted
			} else {
				finalVal = val
			}
		} else {
			finalVal = val
		}

		*ptr = finalVal
		if logging.Level > 1 {
			printMap[k] = finalVal
		}
		gotDate = true
	}

	var b strings.Builder
	b.Grow(len(consts.TimeSfx) + 10)

	if fillEmptyTimestamps(t, &b) {
		gotDate = true
	}

	var err error
	switch {
	case gotDate:
		logging.D(3, "Got a relevant date, proceeding...")

		if t.FormattedDate == "" {
			dates.FormatAllDates(fd)
		} else if t.StringDate, err = dates.ParseNumDate(t.FormattedDate); err != nil {
			logging.E(0, err.Error())
		}

		if _, err := fd.JSONFileRW.WriteJSON(fieldMap); err != nil {
			logging.E(0, "Failed to write into JSON file %q: %v", fd.JSONFilePath, err)
		}

		return true

	case w.WebpageURL == "":

		logging.I("Page URL not found in metadata, so cannot scrape for missing date in %q", fd.JSONFilePath)
		return false
	}

	scrapedDate := browser.ScrapeMeta(w, enums.WEBCLASS_DATE)
	logging.D(1, "Scraped date: %s", scrapedDate)

	var date string

	if scrapedDate != "" {
		date, err = dates.ParseWordDate(scrapedDate)
		if err != nil || date == "" {
			logging.E(0, "Failed to parse date %q: %v", scrapedDate, err)
			return false
		}
		if t.ReleaseDate == "" {
			t.ReleaseDate = date
		}
		if t.Date == "" {
			t.Date = date
		}
		if !strings.ContainsRune(t.Creation_Time, 'T') {
			t.Creation_Time = formatTimeStamp(date, &b)
		}
		if t.UploadDate == "" {
			t.UploadDate = date
		}
		if t.Originally_Available_At == "" {
			t.Originally_Available_At = date
		}
		if t.FormattedDate == "" {
			t.FormattedDate = date
		}
		if len(date) >= 4 {
			t.Year = date[:4]
		}

		if logging.Level > 1 {
			printMap[consts.JReleaseDate] = t.ReleaseDate
			printMap[consts.JDate] = t.Date
			printMap[consts.JYear] = t.Year
		}

		if t.FormattedDate == "" {
			dates.FormatAllDates(fd)
		}
		if _, err := fd.JSONFileRW.WriteJSON(fieldMap); err != nil {
			logging.E(0, "Failed to write new metadata (%s) into JSON file %q: %v", date, fd.JSONFilePath, err)
		}
		return true
	}
	return false
}

// fillEmptyTimestamps attempts to infer missing timestamps
func fillEmptyTimestamps(t *models.MetadataDates, b *strings.Builder) bool {

	gotDate := false

	// Infer from originally available date
	if t.Originally_Available_At != "" && len(t.Originally_Available_At) >= 6 {
		gotDate = true

		if !strings.ContainsRune(t.Creation_Time, 'T') {
			processDateField(t.Originally_Available_At, &t.Creation_Time, t)
			t.Creation_Time = formatTimeStamp(t.Creation_Time, b)
		}
	}

	// Infer from release date
	if t.ReleaseDate != "" && len(t.ReleaseDate) >= 6 {
		gotDate = true

		if !strings.ContainsRune(t.Creation_Time, 'T') {
			processDateField(t.ReleaseDate, &t.Creation_Time, t)
			t.Creation_Time = formatTimeStamp(t.Creation_Time, b)
		}

		if t.Originally_Available_At == "" {
			processDateField(t.ReleaseDate, &t.Originally_Available_At, t)
		}
	}

	// Infer from date
	if t.Date != "" && len(t.Date) >= 6 {
		gotDate = true

		if !strings.ContainsRune(t.Creation_Time, 'T') {
			processDateField(t.Date, &t.Creation_Time, t)
			t.Creation_Time = formatTimeStamp(t.Creation_Time, b)
		}

		if t.Originally_Available_At == "" {
			processDateField(t.Date, &t.Originally_Available_At, t)
		}
	}

	// Infer from upload date
	if t.UploadDate != "" && len(t.UploadDate) >= 6 {

		if !strings.ContainsRune(t.Creation_Time, 'T') {
			processDateField(t.UploadDate, &t.Creation_Time, t)
			t.Creation_Time = formatTimeStamp(t.Creation_Time, b)
		}

		if t.Originally_Available_At == "" {
			processDateField(t.UploadDate, &t.Originally_Available_At, t)
		}
	}

	// Fill empty date
	if t.Date == "" {
		switch {
		case t.ReleaseDate != "":
			t.Date = t.ReleaseDate
			t.Originally_Available_At = t.ReleaseDate

		case t.UploadDate != "":
			t.Date = t.UploadDate
			t.Originally_Available_At = t.UploadDate

		case t.FormattedDate != "":
			t.Date = t.FormattedDate
		}
	}
	// Fill empty year
	if t.Year == "" {
		switch {
		case t.Date != "" && len(t.Date) >= 4:
			t.Year = t.Date[:4]

		case t.UploadDate != "" && len(t.UploadDate) >= 4:
			t.Year = t.UploadDate[:4]

		case t.FormattedDate != "" && len(t.FormattedDate) >= 4:
			t.Year = t.FormattedDate[:4]
		}
	}
	if len(t.Year) > 4 {
		t.Year = t.Year[:4]
	}

	// Try to fix accidentally using upload date if another date is available
	if len(t.Year) == 4 && !strings.HasPrefix(t.Creation_Time, t.Year) && len(t.Creation_Time) >= 4 {

		logging.D(1, "Creation time does not match year tag, seeing if other dates are available...")

		switch {
		case strings.HasPrefix(t.Originally_Available_At, t.Year):
			if !strings.ContainsRune(t.Creation_Time, 'T') {
				t.Creation_Time = formatTimeStamp(t.Originally_Available_At, b)
			}

			logging.D(1, "Set creation time to %s", t.Originally_Available_At)

		case strings.HasPrefix(t.ReleaseDate, t.Year):
			if !strings.ContainsRune(t.Creation_Time, 'T') {
				t.Creation_Time = formatTimeStamp(t.ReleaseDate, b)
			}

			logging.D(1, "Set creation time to %s", t.ReleaseDate)

		case strings.HasPrefix(t.Date, t.Year):
			if !strings.ContainsRune(t.Creation_Time, 'T') {
				t.Creation_Time = formatTimeStamp(t.Date, b)
			}

			logging.D(1, "Set creation time to %s", t.Date)

		case strings.HasPrefix(t.FormattedDate, t.Year):
			if !strings.ContainsRune(t.Creation_Time, 'T') {
				t.Creation_Time = formatTimeStamp(t.FormattedDate, b)
			}

			logging.D(1, "Set creation time to %s", t.FormattedDate)

		default:
			logging.D(1, "Could not find a match, directly altering t.Creation_Time for year (month and day may therefore be wrong)")
			t.Creation_Time = t.Year + t.Creation_Time[4:]
			logging.D(1, "Set creation time's year only. Got %q", t.Creation_Time)
		}
	}
	return gotDate
}

// formatTimeStamp takes an input date and appends the T time string
func formatTimeStamp(date string, b *strings.Builder) string {
	if b == nil {
		b = &strings.Builder{}
		b.Grow(len(consts.TimeSfx) + 10)
	}

	b.Reset()
	b.WriteString(date)
	b.WriteString(consts.TimeSfx)
	return b.String()
}

// processDateField takes in a filled date, and fills the target with it
func processDateField(date string, target *string, t *models.MetadataDates) {
	if formatted, ok := dates.YyyyMmDd(date); ok {
		if !strings.ContainsRune(formatted, 'T') {
			*target = formatted
			t.FormattedDate = formatted
		} else {
			*target = formatted
			t.FormattedDate, _, _ = strings.Cut(formatted, "T")
		}
	} else {
		*target = date
	}
}
package metadata

import (
	"fmt"
	"metarr/internal/cfg"
	"metarr/internal/domain/consts"
	"metarr/internal/domain/enums"
	"metarr/internal/domain/keys"
	"metarr/internal/models"
	browser "metarr/internal/utils/browser"
	"metarr/internal/utils/logging"
	"metarr/internal/utils/printout"
	"strings"
)

// fillDescriptions grabs description data from JSON
func fillDescriptions(fd *models.FileData, data map[string]any) (map[string]any, bool) {

	d := fd.MTitleDesc
	w := fd.MWebData
	t := fd.MDates

	fieldMap := map[string]*string{ // Order by importance
		consts.JLongDescription:  &d.LongDescription,
		consts.JLong_Description: &d.Long_Description,
		consts.JDescription:      &d.Description,
		consts.JSynopsis:         &d.Synopsis,
		consts.JSummary:          &d.Summary,
		consts.JComment:          &d.Comment,
	}
	filled := unpackJSON(fieldMap, data)

	datePfx := cfg.GetBool(keys.MDescDatePfx)
	dateSfx := cfg.GetBool(keys.MDescDateSfx)

	if (datePfx || dateSfx) && t.StringDate != "" {
		for _, ptr := range fieldMap {
			if ptr == nil {
				logging.E(0, "Unexpected nil pointer in descriptions fieldMap")
				continue
			}

			if !datePfx && !dateSfx {
				logging.D(1, "Unknown issue appending date to description. Condition should be impossible? (reached: %s)", *ptr)
				continue
			}

			if datePfx && !strings.HasPrefix(*ptr, t.StringDate) {
				*ptr = fmt.Sprintf("%s\n\n%s", t.StringDate, *ptr) // Prefix string date
			}

			if dateSfx && !strings.HasSuffix(*ptr, t.StringDate) {
				*ptr = fmt.Sprintf("%s\n\n%s", *ptr, t.StringDate) // Suffix string date
			}
		}
	}

	var printMap map[string]string
	if logging.Level > 1 {
		printMap = make(map[string]string, len(fieldMap))
		defer func() {
			if len(printMap) > 0 {
				printout.PrintGrabbedFields("descriptions", printMap)
			}
		}()
	}

	// Fill with by inference
	var fillWith string
	for _, ptr := range fieldMap {
		if ptr == nil {
			continue
		}
		if *ptr != "" {
			fillWith = *ptr
			break
		}
	}

	// Attempt to fill empty description fields by inference
	for k, ptr := range fieldMap {
		if ptr == nil {
			logging.E(0, "Unexpected nil pointer in descriptions fieldMap")
			continue
		}

		if *ptr == "" {
			*ptr = fillWith
			filled = true
			if logging.Level > 1 {
				printMap[k] = *ptr
			}
		} else {
			filled = true
			if logging.Level > 1 {
				printMap[k] = *ptr
			}
		}
	}

	if filled {

		rtn, err := fd.JSONFileRW.WriteJSON(fieldMap)
		if err != nil {
			logging.E(0, "Failed to write into JSON file %q: %v", fd.JSONFilePath, err)
		}

		if len(rtn) == 0 {
			logging.E(0, "Length of return value is 0, returning original data from descriptions functions")
			return data, true
		}

		data = rtn
		return data, true
	}

	if w.WebpageURL == "" {
		logging.I("Page URL not found in data, so cannot scrape for missing description in %q", fd.JSONFilePath)
		return data, false
	}

	description := browser.ScrapeMeta(w, enums.WEBCLASS_DESCRIPTION)

	// Infer remaining fields from description
	if description != "" {

		for _, ptr := range fieldMap {
			if ptr == nil {
				logging.E(0, "Unexpected nil in descriptions fieldMap")
				continue
			}

			if *ptr == "" {
				*ptr = description
			}
		}

		// Insert new scraped fields into file
		rtn, err := fd.JSONFileRW.WriteJSON(fieldMap)
		if err != nil {
			logging.E(0, "Failed to insert new data (%s) into JSON file %q: %v", description, fd.JSONFilePath, err)
		} else if rtn != nil {

			data = rtn
			return data, true
		}

		logging.D(1, "No descriptions were grabbed from scrape, returning original data map")
		return data, false
	} else {
		return data, false
	}
}

// // fillEmptyDescriptions fills empty description fields by inference
// func fillEmptyDescriptions(s *string, d *models.MetadataTitlesDescs) bool {

// 	// Nil check and empty value check should be done in caller
// 	filled := false
// 	switch {
// 	case d.LongDescription != "":
// 		*s = d.LongDescription
// 		filled = true

// 	case d.Long_Description != "":
// 		*s = d.Long_Description
// 		filled = true

// 	case d.Description != "":
// 		*s = d.Description
// 		filled = true

// 	case d.Synopsis != "":
// 		*s = d.Synopsis
// 		filled = true

// 	case d.Summary != "":
// 		*s = d.Summary
// 		filled = true

// 	case d.Comment != "":
// 		*s = d.Comment
// 		filled = true
// 	}

// 	return filled
// }
package metadata

import (
	"metarr/internal/models"
	logging "metarr/internal/utils/logging"
)

// Primary function to fill out meta fields before writing
func FillJSONFields(fd *models.FileData, json map[string]any) (map[string]any, bool) {

	allFilled := true
	if meta, ok := fillTitles(fd, json); !ok {
		logging.I("No title metadata found")
		allFilled = false
	} else if meta != nil {
		json = meta
	}

	if meta, ok := fillCredits(fd, json); !ok {
		logging.I("No credits metadata found")
		allFilled = false
	} else if meta != nil {
		json = meta
	}

	if meta, ok := fillDescriptions(fd, json); !ok {
		logging.I("No description metadata found")
		allFilled = false
	} else if meta != nil {
		json = meta
	}
	return json, allFilled
}

// unpackJSON decodes JSON for metafields
func unpackJSON(fmap map[string]*string, json map[string]any) bool {

	filled := false
	pmap := make(map[string]string, len(fmap))

	// Match decoded JSON to field map
	for k, ptr := range fmap {
		if ptr == nil {
			logging.E(0, "fieldMap entry pointer unexpectedly nil")
			continue
		}

		v, exists := json[k]
		if !exists {
			continue
		}

		val, ok := v.(string)
		if !ok {
			continue
		}

		if *ptr == "" {
			*ptr = val
			if logging.Level > 1 {
				pmap[k] = val
			}
			filled = true
		}
	}

	return filled
}
package metadata

import (
	"metarr/internal/domain/consts"
	"metarr/internal/domain/enums"
	"metarr/internal/models"
	browser "metarr/internal/utils/browser"
	"metarr/internal/utils/logging"
	"metarr/internal/utils/printout"
)

// fillTitles grabs the fulltitle ("title")
func fillTitles(fd *models.FileData, json map[string]any) (map[string]any, bool) {

	t := fd.MTitleDesc
	w := fd.MWebData

	fieldMap := map[string]*string{
		consts.JTitle:     &t.Title,
		consts.JFulltitle: &t.Fulltitle,
		consts.JSubtitle:  &t.Subtitle,
	}

	var printMap map[string]string
	if logging.Level > 1 {
		printMap = make(map[string]string, len(fieldMap))
		defer func() {
			if len(printMap) > 0 {
				printout.PrintGrabbedFields("titles", printMap)
			}
		}()
	}

	if filled := unpackJSON(fieldMap, json); filled {
		logging.D(2, "Decoded titles JSON into field map")
	}

	// Fill fieldMap entries
	for k, ptr := range fieldMap {
		if ptr == nil {
			logging.E(0, "fieldMap entry pointer unexpectedly nil")
			continue
		}

		v, exists := json[k]
		if !exists {
			continue
		}

		val, ok := v.(string)
		if !ok {
			continue
		}

		if *ptr == "" {
			*ptr = val
		}

		if logging.Level > 1 {
			printMap[k] = val
		}
	}

	if t.Title == "" && t.Fulltitle != "" {
		t.Title = t.Fulltitle
	}

	if t.Fulltitle == "" && t.Title != "" {
		t.Fulltitle = t.Title
	}

	if t.Title == "" {
		logging.I("Title is blank, scraping web for missing title data...")

		title := browser.ScrapeMeta(w, enums.WEBCLASS_TITLE)
		if title != "" {
			t.Title = title
		}
	}

	data, err := fd.JSONFileRW.WriteJSON(fieldMap)
	if err != nil {
		logging.E(0, err.Error())
		return data, false
	}

	return data, true
}
package metadata

import (
	"metarr/internal/domain/consts"
	"metarr/internal/models"
	"metarr/internal/utils/logging"
	"metarr/internal/utils/printout"
)

// Grabs details necessary to scrape the web for missing metafields
func FillWebpageDetails(fd *models.FileData, data map[string]any) bool {

	var isFilled bool
	w := fd.MWebData

	priorityMap := [...]string{consts.JWebpageURL,
		consts.JURL,
		consts.JReferer,
		consts.JWebpageDomain,
		consts.JDomain}

	if w.TryURLs == nil {
		w.TryURLs = make([]string, 0, len(priorityMap))
	}

	var printMap map[string]string
	if logging.Level > 1 {
		printMap = make(map[string]string, len(priorityMap))
		defer func() {
			if len(printMap) > 0 {
				printout.PrintGrabbedFields("web info", printMap)
			}
		}()
	}

	// Fill model using priorityMap keys
	for _, k := range priorityMap {
		v, exists := data[k]
		if !exists {
			continue
		}

		val, ok := v.(string)
		if !ok {
			continue
		}

		switch {
		case k == consts.JWebpageURL:
			if webInfoFill(&w.WebpageURL, val, w) {
				isFilled = true
			}

		case k == consts.JURL:
			if webInfoFill(&w.VideoURL, val, w) {
				isFilled = true
			}

		case k == consts.JReferer:
			if webInfoFill(&w.Referer, val, w) {
				isFilled = true
			}

		case k == consts.JWebpageDomain, k == consts.JDomain:

			if webInfoFill(&w.Domain, val, w) {
				isFilled = true
			}

		default:
			continue
		}

		if logging.Level > 1 && val != "" {
			printMap[k] = val
		}

	}
	logging.D(2, "Stored URLs for scraping missing fields: %v", w.TryURLs)

	return isFilled
}

// webInfoFill fills web info data into the model
func webInfoFill(s *string, val string, w *models.MetadataWebData) (filled bool) {
	if s == nil {
		logging.E(0, "String passed in null")
		return false
	}
	logging.D(3, "Got URL: %s", val)
	if *s == "" {
		*s = val
	}

	w.TryURLs = append(w.TryURLs, val)
	return *s != ""
}
package metadata

import (
	"metarr/internal/domain/consts"
	"metarr/internal/models"
	"metarr/internal/utils/logging"
	"strings"
)

// fillNFODescriptions attempts to fill in title info from NFO
func fillNFOCredits(fd *models.FileData) bool {

	c := fd.MCredits
	n := fd.NFOData

	fieldMap := map[string]*string{
		consts.NActors:            &c.Actor,
		consts.NDirector:          &c.Director,
		consts.NProductionCompany: &c.Publisher,
		consts.NStudio:            &c.Studio,
		consts.NWriter:            &c.Writer,
		consts.NProducer:          &c.Producer,
	}

	// Post-unmarshal clean
	cleanEmptyFields(fieldMap)

	if n.Actors != nil {
		for _, actor := range n.Actors {
			c.Actors = append(c.Actors, actor.Name)
		}
		fillSingleCredits(c.Actors, &c.Actor)
	}
	if n.Directors != nil {
		c.Directors = append(c.Directors, n.Directors...)
		fillSingleCredits(c.Directors, &c.Director)
	}
	if n.Producers != nil {
		c.Producers = append(c.Producers, n.Producers...)
		fillSingleCredits(c.Producers, &c.Producer)
	}
	if n.Writers != nil {
		c.Writers = append(c.Writers, n.Writers...)
		fillSingleCredits(c.Writers, &c.Writer)
	}
	if n.Publishers != nil {
		c.Publishers = append(c.Publishers, n.Publishers...)
		fillSingleCredits(c.Publishers, &c.Publisher)
	}
	if n.Studios != nil {
		c.Studios = append(c.Studios, n.Studios...)
		fillSingleCredits(c.Studios, &c.Studio)
	}

	return true
}

// fillSingleCredits fills empty singular credits fields from
// filled arrays
func fillSingleCredits(entries []string, target *string) {

	if target == nil {
		logging.D(1, "Target string is nil, skipping...")
		return
	}

	if *target != "" {
		logging.D(1, "Target string is not empty, skipping...")
		return
	}

	filtered := make([]string, 0, len(entries))
	for _, entry := range entries {
		if entry != "" {
			filtered = append(filtered, entry)
		}
	}

	*target = strings.Join(filtered, ", ")
}

func unpackCredits(fd *models.FileData, creditsData map[string]any) bool {
	c := fd.MCredits
	filled := false

	// Recursive helper to search for "role" within nested maps
	var findRoles func(data map[string]any)
	findRoles = func(data map[string]any) {
		// Check each key-value pair within the actor data
		for k, v := range data {

			switch value := v.(type) {
			case string:
				if k == "role" {
					if role, ok := v.(string); ok {
						logging.D(3, "Adding role %q to actors", role)
						c.Actors = append(c.Actors, role)
						filled = true
					}
				}
			case map[string]any: // Regular map
				findRoles(value)

			case []any: // Nested map
				for _, item := range value {
					if nestedMap, ok := item.(map[string]any); ok {
						findRoles(nestedMap)
					}
				}
			}
		}
	}

	// Access the "cast" data to find "actor" entries
	if castData, ok := creditsData["cast"].(map[string]any); ok {
		if actorsData, ok := castData["actor"].([]any); ok {
			for _, actorData := range actorsData {
				if actorMap, ok := actorData.(map[string]any); ok {
					if name, ok := actorMap["name"].(string); ok {
						logging.D(3, "Adding actor name %q", name)
						c.Actors = append(c.Actors, name)
						filled = true
					}
					if role, ok := actorMap["role"].(string); ok {
						logging.D(3, "Adding actor role %q", role)
						filled = true
					}
				}
			}
		} else {
			logging.D(1, "'actor' key is present but not a valid structure")
		}
	} else {
		logging.D(1, "'cast' key is missing or not a map")
	}

	return filled
}
package metadata

import (
	"fmt"
	"metarr/internal/dates"
	"metarr/internal/domain/consts"
	"metarr/internal/domain/enums"
	"metarr/internal/models"
	browser "metarr/internal/utils/browser"
	"metarr/internal/utils/logging"
	"metarr/internal/utils/printout"
)

func fillNFOTimestamps(fd *models.FileData) bool {

	t := fd.MDates
	w := fd.MWebData
	n := fd.NFOData

	fieldMap := map[string]*string{
		consts.NAired:        &t.Date,
		consts.NPremiereDate: &t.ReleaseDate,
		consts.NYear:         &t.Year,
	}

	cleanEmptyFields(fieldMap)

	gotRelevantDate := false
	printMap := make(map[string]string, len(fieldMap))

	defer func() {
		printout.PrintGrabbedFields("time and date", printMap)
	}()

	if n.Premiered != "" {
		if rtn, ok := dates.YyyyMmDd(n.Premiered); ok && rtn != "" {
			if t.FormattedDate == "" {
				t.FormattedDate = rtn
			}
		}
		printMap[consts.NPremiereDate] = n.Premiered
		gotRelevantDate = true
	}
	if n.ReleaseDate != "" {
		if rtn, ok := dates.YyyyMmDd(n.ReleaseDate); ok && rtn != "" {
			if t.FormattedDate == "" {
				t.FormattedDate = rtn
			}
		}
		printMap[consts.NAired] = n.Premiered
		gotRelevantDate = true
	}
	if n.Year != "" {
		t.Year = n.Year
		printMap[consts.NYear] = n.Year
	}

	if t.FormattedDate != "" {
		if t.Date == "" {
			t.Date = t.FormattedDate
		}
		if t.ReleaseDate == "" {
			t.ReleaseDate = t.FormattedDate
		}
		if t.Creation_Time == "" {
			t.Creation_Time = fmt.Sprintf("%sT00:00:00Z", t.FormattedDate)
		}
		gotRelevantDate = true
	}

	switch {
	case gotRelevantDate:

		var err error

		logging.D(3, "Got a relevant date, proceeding...")
		if t.FormattedDate == "" {
			dates.FormatAllDates(fd)
		} else {
			t.StringDate, err = dates.ParseNumDate(t.FormattedDate)
			if err != nil {
				logging.E(0, err.Error())
			}
		}

	case w.WebpageURL == "":

		logging.I("Page URL not found in metadata, so cannot scrape for missing date in %q", fd.JSONFilePath)
		return false
	}

	scrapedDate := browser.ScrapeMeta(w, enums.WEBCLASS_DATE)
	logging.D(1, "Scraped date: %s", scrapedDate)

	logging.D(3, "Passed web scrape attempt for date.")

	var (
		date string
		err  error
	)
	if scrapedDate != "" {
		date, err = dates.ParseWordDate(scrapedDate)
		if err != nil || date == "" {
			logging.E(0, "Failed to parse date %q: %v", scrapedDate, err)
			return false
		} else {
			if t.ReleaseDate == "" {
				t.ReleaseDate = date
			}
			if t.Date == "" {
				t.Date = date
			}
			if t.Creation_Time == "" {
				t.Creation_Time = fmt.Sprintf("%sT00:00:00Z", date)
			}
			if t.UploadDate == "" {
				t.UploadDate = date
			}
			if t.Originally_Available_At == "" {
				t.Originally_Available_At = date
			}
			if t.FormattedDate == "" {
				t.FormattedDate = date
			}
			if len(date) >= 4 {
				t.Year = date[:4]
			}

			printMap[consts.NPremiereDate] = t.ReleaseDate
			printMap[consts.NAired] = t.Date
			printMap[consts.NYear] = t.Year

			if t.FormattedDate == "" {
				dates.FormatAllDates(fd)
			}
		}
	}

	return true
}
package metadata

import (
	"metarr/internal/domain/consts"
	"metarr/internal/models"
	"metarr/internal/utils/printout"
)

// fillNFODescriptions attempts to fill in title info from NFO
func fillNFODescriptions(fd *models.FileData) bool {

	d := fd.MTitleDesc
	n := fd.NFOData

	fieldMap := map[string]*string{
		consts.NDescription: &d.Description,
		consts.NPlot:        &d.LongDescription,
	}

	// Post-unmarshal clean
	cleanEmptyFields(fieldMap)

	if n.Description != "" {
		if d.Description == "" {
			d.Description = n.Description
		}
		if d.LongDescription == "" {
			d.LongDescription = n.Description
		}
	}
	if n.Plot != "" {
		if d.Description == "" {
			d.Description = n.Plot
		}
		if d.LongDescription == "" {
			d.LongDescription = n.Plot
		}
	}

	if d.Synopsis == "" {
		switch {
		case n.Plot != "":
			d.Synopsis = n.Plot
		case n.Description != "":
			d.Summary = n.Description
		case d.LongDescription != "":
			d.Synopsis = d.LongDescription
		case d.Description != "":
			d.Synopsis = d.Description
		}
	}
	if d.Summary == "" {
		switch {
		case n.Plot != "":
			d.Summary = n.Plot
		case n.Description != "":
			d.Summary = n.Description
		case d.LongDescription != "":
			d.Summary = d.LongDescription
		case d.Description != "":
			d.Summary = d.Description
		}
	}
	if d.Comment == "" {
		switch {
		case n.Plot != "":
			d.Comment = n.Plot
		case n.Description != "":
			d.Comment = n.Description
		case d.LongDescription != "":
			d.Comment = d.LongDescription
		case d.Description != "":
			d.Comment = d.Description
		}
	}

	printout.CreateModelPrintout(fd, fd.NFOFilePath, "Parsing NFO descriptions")
	return true
}
package metadata

import (
	"fmt"
	"metarr/internal/models"
	"metarr/internal/utils/logging"
	"metarr/internal/utils/printout"
	"strings"
)

// FillNFO is the primary entrypoint for filling NFO metadata
// from an open file's read content
func FillNFO(fd *models.FileData) bool {

	var filled bool

	if ok := fillNFOTimestamps(fd); ok {
		filled = true
	}

	if ok := fillNFOTitles(fd); ok {
		filled = true
	}

	if ok := fillNFODescriptions(fd); ok {
		filled = true
	}

	if ok := fillNFOCredits(fd); ok {
		filled = true
	}

	if ok := fillNFOWebData(fd); ok {
		filled = true
	}

	printout.CreateModelPrintout(fd, fd.NFOBaseName, "Fill metadata from NFO for file %q", fd.NFOFilePath)

	return filled
}

// Clean up empty fields from fieldmap
func cleanEmptyFields(fieldMap map[string]*string) {
	for _, value := range fieldMap {
		if strings.TrimSpace(*value) == "" {
			*value = ""
		}
	}
}

// nestedLoop parses content recursively and returns a nested map
func nestedLoop(content string) map[string]any {
	nested := make(map[string]any)

	logging.D(2, "Parsing content in nestedLoop: %s", content)

	for content != "" {
		if strings.HasPrefix(content, "<?xml") || strings.HasPrefix(content, "<?") {
			endIdx := strings.Index(content, "?>")
			if endIdx == -1 {
				logging.E(0, "Malformed XML declaration in content: %s", content)
				break
			}
			content = content[endIdx+2:]
			logging.D(2, "Skipping XML declaration, remaining content: %s", content)
			continue
		}

		// Find the opening tag
		openIdx := strings.Index(content, "<")
		if openIdx == -1 {
			break // No more tags
		}

		openIdxClose := strings.Index(content, ">")
		if openIdxClose == -1 {
			logging.E(0, "No valid tag close bracket for entry beginning %s", content[openIdx:])
			break // No closing tag bracket
		}

		// Get the tag name and check if it is self-closing
		tag := content[openIdx+1 : openIdxClose]
		isSelfClosing := strings.HasSuffix(tag, "/")
		tag = strings.TrimSuffix(tag, "/") // Remove trailing / if present

		if isSelfClosing {
			// Self-closing tag; skip over and move to the next
			content = content[openIdxClose+1:]
			logging.D(2, "Skipping self-closing tag: %s", tag)
			continue
		}

		// Look for the corresponding closing tag
		closeTag := fmt.Sprintf("</%s>", tag)
		closeIdx := strings.Index(content, closeTag)
		if closeIdx == -1 {
			// No closing tag; skip this tag and continue
			content = content[openIdxClose+1:]
			logging.D(2, "Skipping tag without end tag: %s", tag)
			continue
		}

		// Extract the inner content between tags
		innerContent := content[openIdxClose+1 : closeIdx]
		logging.D(2, "Found inner content for tag %q: %s", tag, innerContent)

		// Recursive call if innerContent contains nested tags
		if strings.Contains(innerContent, "<") && strings.Contains(innerContent, ">") {
			logging.D(2, "Recursively parsing nested content for tag %q", tag)
			nested[tag] = nestedLoop(innerContent)
		} else {
			logging.D(2, "Assigning inner content to tag %q: %s", tag, innerContent)
			nested[tag] = innerContent
		}

		// Move past the processed tag
		content = content[closeIdx+len(closeTag):]
		logging.D(2, "Remaining content after parsing tag %q: %s", tag, content)
	}

	logging.D(2, "Final parsed structure from nestedLoop: %v", nested)
	return nested
}

// unpackNFO unpacks an NFO map back to the model
func unpackNFO(fd *models.FileData, data map[string]any, fieldMap map[string]*string) {
	logging.D(3, "Unpacking NFO map...")

	// Access the top-level "movie" key
	movieData, ok := data["movie"].(map[string]any)
	if !ok {
		logging.E(0, "Missing 'movie' key in data, unable to unpack")
		return
	}

	for field, fieldVal := range fieldMap {
		if fieldVal == nil {
			logging.E(0, "Field value is null, continuing...")
			continue
		}

		// Look for the field in the movie data
		val, exists := movieData[field]
		if !exists {
			continue // Field does not exist in this map
		}

		switch v := val.(type) {
		case string:
			logging.D(3, "Setting field %q to %q", field, v)
			*fieldVal = v
		case map[string]any:
			switch field {

			case "title":
				logging.D(3, "Unpacking nested 'title' map...")
				unpackTitle(fd, v)
			case "cast":
				logging.D(3, "Unpacking nested 'cast' map...")
				unpackCredits(fd, v)
			}
		default:
			logging.D(1, "Unknown field type for %q, skipping...", field)
		}
	}
}
package metadata

import (
	"metarr/internal/domain/consts"
	"metarr/internal/models"
	"metarr/internal/utils/logging"
)

// fillNFOTitles attempts to fill in title info from NFO
func fillNFOTitles(fd *models.FileData) bool {

	t := fd.MTitleDesc
	n := fd.NFOData

	fieldMap := map[string]*string{
		consts.NTitle:         &t.Title,
		consts.NOriginalTitle: &t.Fulltitle,
		consts.NTagline:       &t.Subtitle,
	}

	// Post-unmarshal clean
	cleanEmptyFields(fieldMap)

	logging.I("Grab NFO metadata: %v", t)

	if n.Title.Main != "" {
		if t.Title == "" {
			t.Title = n.Title.Main
		}
	}
	if n.Title.Original != "" {
		if t.Fulltitle == "" {
			t.Fulltitle = n.Title.Original
		}
		if t.Title == "" {
			t.Title = n.Title.Original
		}
	}
	if n.Title.Sub != "" {
		if t.Subtitle == "" {
			t.Subtitle = n.Title.Sub
		}
	}
	if n.Title.PlainText != "" {
		if t.Title == "" {
			t.Title = n.Title.PlainText
		}
	}
	return true
}

// unpackTitle unpacks common nested title elements to the model.
func unpackTitle(fd *models.FileData, titleData map[string]any) bool {
	t := fd.MTitleDesc
	filled := false

	for key, value := range titleData {
		switch key {
		case "main":
			if strVal, ok := value.(string); ok {
				logging.D(3, "Setting main title to %q", strVal)
				t.Title = strVal
				filled = true
			}
		case "sub":
			if strVal, ok := value.(string); ok {
				logging.D(3, "Setting subtitle to %q", strVal)
				t.Subtitle = strVal
				filled = true
			}
		default:
			logging.D(1, "Unknown nested title element %q, skipping...", key)
		}
	}
	return filled
}
package metadata

import (
	"metarr/internal/domain/consts"
	"metarr/internal/models"
	"metarr/internal/utils/printout"
)

// fillNFODescriptions attempts to fill in title info from NFO
func fillNFOWebData(fd *models.FileData) bool {

	w := fd.MWebData
	nw := fd.NFOData.WebpageInfo

	fieldMap := map[string]*string{
		consts.NURL: &w.WebpageURL,
	}

	// Post-unmarshal clean
	cleanEmptyFields(fieldMap)

	if nw.URL != "" {
		if w.WebpageURL == "" {
			w.WebpageURL = nw.URL
		}
	}

	printout.CreateModelPrintout(fd, fd.NFOFilePath, "Parsing NFO descriptions")
	return true
}
package metadata

import (
	logging "metarr/internal/utils/logging"
	"strings"
)

type ffprobeFormat struct {
	Tags ffprobeTags `json:"tags"`
}

type ffprobeOutput struct {
	Format ffprobeFormat `json:"format"`
}

type ffprobeTags struct {
	Description  string `json:"description"`
	Synopsis     string `json:"synopsis"`
	Title        string `json:"title"`
	CreationTime string `json:"creation_time"`
	Date         string `json:"date"`
	Artist       string `json:"artist"`
	Composer     string `json:"composer"`
}

// safeGetDatePart safely extracts the date part before 'T' if it exists
func safeGetDatePart(timeStr string) string {
	timeStr = strings.TrimSpace(timeStr)
	if parts := strings.Split(timeStr, "T"); len(parts) > 0 {
		return parts[0]
	}
	return timeStr
}

func printArray(s []string) {
	str := strings.Join(s, ", ")
	logging.I("FFprobe captured %s", str)
}
package metadata

import (
	"context"
	"encoding/json"
	"fmt"
	"metarr/internal/domain/consts"
	"metarr/internal/models"
	"metarr/internal/utils/logging"
	"os/exec"
	"strings"
)

// MP4MetaMatches checks FFprobe captured metadata from the video against the metafile.
func MP4MetaMatches(ctx context.Context, fd *models.FileData) bool {

	c := fd.MCredits
	d := fd.MDates
	t := fd.MTitleDesc

	// FFprobe command fetches metadata from the video file
	command := exec.CommandContext(
		ctx,
		"ffprobe",
		"-v", "quiet",
		"-print_format", "json",
		"-show_format",
		fd.OriginalVideoPath,
	)

	logging.I("Made command for FFprobe:\n\n%v", command.String())

	output, err := command.Output()
	if err != nil {
		logging.E(0, "Error running FFprobe command: %v. Will process video.", err)
		return false
	}

	// Parse JSON output
	var ffData ffprobeOutput

	if err := json.Unmarshal(output, &ffData); err != nil {
		logging.E(0, "Error parsing FFprobe output: %v. Will process video.", err)
		return false
	}

	// Map of metadata to check
	metaCheckMap := map[string]struct {
		existing string
		new      string
	}{
		consts.JDescription: {
			existing: strings.TrimSpace(ffData.Format.Tags.Description),
			new:      strings.TrimSpace(t.Description),
		},
		consts.JSynopsis: {
			existing: strings.TrimSpace(ffData.Format.Tags.Synopsis),
			new:      strings.TrimSpace(t.Synopsis),
		},
		consts.JTitle: {
			existing: strings.TrimSpace(ffData.Format.Tags.Title),
			new:      strings.TrimSpace(t.Title),
		},
		consts.JCreationTime: {
			existing: safeGetDatePart(ffData.Format.Tags.CreationTime),
			new:      safeGetDatePart(d.Creation_Time),
		},
		consts.JDate: {
			existing: strings.TrimSpace(ffData.Format.Tags.Date),
			new:      strings.TrimSpace(d.Date),
		},
		consts.JArtist: {
			existing: strings.TrimSpace(ffData.Format.Tags.Artist),
			new:      strings.TrimSpace(c.Artist),
		},
		consts.JComposer: {
			existing: strings.TrimSpace(ffData.Format.Tags.Composer),
			new:      strings.TrimSpace(c.Composer),
		},
	}

	// Collect all metadata for logging
	var ffContent []string
	matches := true

	// Check each field
	for key, values := range metaCheckMap {
		printVals := fmt.Sprintf("Currently in video: Key=%s, Value=%s, New Value=%s", key, values.existing, values.new)
		ffContent = append(ffContent, printVals)

		if values.new != values.existing {
			logging.D(2, "======== Mismatched meta in file: %q ========\nMismatch in key %q:\nNew value: %q\nIn video as: %q. Will process video.",
				fd.OriginalVideoBaseName, key, values.new, values.existing)
			matches = false
		} else {
			logging.D(2, "Detected key %q as being the same.\nFFprobe: %q\nMetafile: %q", key, values.existing, values.new)
		}
	}

	// Print all captured metadata
	printArray(ffContent)

	return matches
}
package metadata

import (
	"context"
	"fmt"
	"metarr/internal/cfg"
	"metarr/internal/dates"
	"metarr/internal/domain/consts"
	"metarr/internal/domain/enums"
	"metarr/internal/domain/keys"
	process "metarr/internal/metadata/process/json"
	check "metarr/internal/metadata/reader/check_existing"
	tags "metarr/internal/metadata/tags"
	jsonRw "metarr/internal/metadata/writer/json"
	"metarr/internal/models"
	"metarr/internal/transformations"
	"metarr/internal/utils/logging"
	"metarr/internal/utils/printout"
	"os"
	"path/filepath"
	"strings"
	"sync"
)

var (
	mu sync.Mutex
)

// ProcessJSONFile reads a single JSON file and fills in the metadata.
func ProcessJSONFile(ctx context.Context, fd *models.FileData) (*models.FileData, error) {
	if fd == nil {
		return nil, fmt.Errorf("model passed in null")
	}

	logging.D(2, "Beginning JSON file processing...")

	// Function mutex
	mu.Lock()
	defer mu.Unlock()

	filePath := fd.JSONFilePath

	// Open the file
	file, err := os.OpenFile(filePath, os.O_RDWR, 0o644)
	if err != nil {
		logging.ErrorArray = append(logging.ErrorArray, err)
		return nil, fmt.Errorf("failed to open file: %w", err)
	}
	defer file.Close()

	// Grab and store metadata reader/writer
	jsonRW := jsonRw.NewJSONFileRW(file)
	if jsonRW != nil {
		fd.JSONFileRW = jsonRW
	}

	// Decode metadata from file
	data, err := fd.JSONFileRW.DecodeJSON(file)
	if err != nil {
		return nil, err
	}

	if data == nil {
		return nil, fmt.Errorf("json decoded nil")
	}

	logging.D(3, "%v", data)

	var ok bool

	// Get web data first (before MakeMetaEdits in case of transformation presets)
	if ok = process.FillWebpageDetails(fd, data); ok {
		logging.I("URLs grabbed: %s", fd.MWebData.TryURLs)
	}

	if len(fd.MWebData.TryURLs) > 0 {
		if match := transformations.TryTransPresets(fd.MWebData.TryURLs, fd); match == "" {
			logging.D(1, "No presets found for video %q URLs %v", fd.OriginalVideoBaseName, fd.MWebData.TryURLs)
		}
	}

	// Make metadata adjustments per user selection or transformation preset
	if edited, err := fd.JSONFileRW.MakeJSONEdits(file, fd); err != nil {
		return nil, err
	} else if edited {
		logging.D(2, "Refreshing JSON metadata after edits were made...")
		if data, err = fd.JSONFileRW.RefreshJSON(); err != nil {
			return nil, err
		}
	}

	// Fill timestamps and make/delete date tag ammendments
	if ok = process.FillTimestamps(fd, data); !ok {
		logging.I("No date metadata found")
	}

	if fd.MDates.FormattedDate == "" {
		dates.FormatAllDates(fd)
	}

	if cfg.IsSet(keys.MDateTagMap) || cfg.IsSet(keys.MDelDateTagMap) {
		ok, err = jsonRW.JSONDateTagEdits(file, fd)
		if err != nil {
			logging.E(0, err.Error())
		} else if !ok {
			logging.D(1, "Did not make date tag edits for metadata, tag already exists?")
		}
	} else {
		logging.D(4, "Skipping making metadata date tag edits, key not set")
	}

	// Must refresh JSON again after further edits
	data, err = jsonRW.RefreshJSON()
	if err != nil {
		return nil, err
	}

	// Fill other metafields
	if data, ok = process.FillJSONFields(fd, data); !ok {
		logging.D(2, "Some metafields were unfilled")
	}

	// Make filename date tag
	logging.D(3, "About to make date tag for: %v", file.Name())

	if cfg.IsSet(keys.FileDateFmt) {
		dateFmt, ok := cfg.Get(keys.FileDateFmt).(enums.DateFormat)

		switch {
		case !ok:
			logging.E(0, "Got null or wrong type for file date format. Got type %T", dateFmt)

		case dateFmt != enums.DATEFMT_SKIP:

			dateTag, err := tags.MakeDateTag(data, fd, dateFmt)
			if err != nil {
				logging.E(0, "Failed to make date tag: %v", err)
			}
			if !strings.Contains(file.Name(), dateTag) {
				fd.FilenameDateTag = dateTag
			}

		default:
			logging.D(1, "Set file date tag format to skip, not making date tag for %q", file.Name())
		}
	}

	if logging.Level > 3 {
		printout.CreateModelPrintout(fd, fd.OriginalVideoBaseName, "Printing model fields")
	}

	// Add new filename tag for files
	if cfg.IsSet(keys.MFilenamePfx) {
		logging.D(3, "About to make prefix tag for: %v", file.Name())
		fd.FilenameMetaPrefix = tags.MakeFilenameTag(data, file)
	}

	// Check if metadata is already existent in target file
	if filetypeMetaCheckSwitch(ctx, fd) {
		logging.I("Metadata already exists in target file %q, will skip processing", fd.OriginalVideoBaseName)
		fd.MetaAlreadyExists = true
	}

	return fd, nil
}

// filetypeMetaCheckSwitch checks metadata matches by file extension (different extensions store different fields).
func filetypeMetaCheckSwitch(ctx context.Context, fd *models.FileData) bool {

	logging.D(4, "Entering filetypeMetaCheckSwitch with %q", fd.OriginalVideoPath)

	var outExt string
	outFlagSet := cfg.IsSet(keys.OutputFiletype)

	if outFlagSet {
		outExt = cfg.GetString(keys.OutputFiletype)
	} else {
		outExt = filepath.Ext(fd.OriginalVideoPath)
		logging.D(2, "Got output extension as %s", outExt)
	}

	currentExt := filepath.Ext(fd.OriginalVideoPath)
	currentExt = strings.TrimSpace(currentExt)

	if outExt != "" && !strings.HasPrefix(outExt, ".") {
		outExt = "." + outExt

		logging.D(2, "Added dot to outExt: %s, currentExt is %s", outExt, currentExt)
	}

	if outFlagSet && outExt != "" && !strings.EqualFold(outExt, currentExt) {
		logging.I("Input format %q differs from output format %q, will not run metadata checks", currentExt, outExt)
		return false
	}

	// Run metadata checks in all other cases
	switch currentExt {
	case consts.ExtMP4:
		return check.MP4MetaMatches(ctx, fd)
	default:
		logging.I("Checks not currently implemented for this filetype")
		return false
	}
}
package metadata

import (
	"fmt"
	nfo "metarr/internal/metadata/process/nfo"
	nfoRw "metarr/internal/metadata/writer/nfo"
	"metarr/internal/models"
	logging "metarr/internal/utils/logging"
	"os"
)

// ProcessNFOFiles processes NFO files and sends data into the metadata model
func ProcessNFOFiles(fd *models.FileData) (*models.FileData, error) {
	if fd == nil {
		return nil, fmt.Errorf("model passed in null")
	}

	logging.D(2, "Beginning NFO file processing...")

	// Open the file
	file, err := os.OpenFile(fd.NFOFilePath, os.O_RDWR, 0o644)
	if err != nil {
		logging.ErrorArray = append(logging.ErrorArray, err)
		return nil, fmt.Errorf("failed to open file: %w", err)
	}
	defer file.Close()

	nfoRW := nfoRw.NewNFOFileRW(file)
	if nfoRW != nil {
		// Store NFO RW in model
		fd.NFOFileRW = nfoRW
	}

	data, err := nfoRW.DecodeMetadata(file)
	if err != nil || data == nil {
		logging.E(0, "Failed to decode metadata from file: %v", err)
	} else {
		// Store NFO data in model
		fd.NFOData = data
	}

	edited, err := nfoRW.MakeMetaEdits(nfoRW.Meta, file, fd)
	if err != nil {
		logging.E(0, "Encountered issue making meta edits: %v", err)
	}
	if edited {
		logging.D(2, "Refreshing NFO metadata after edits were made...")
		data, err := fd.NFOFileRW.RefreshMetadata()
		if err != nil {
			return nil, err
		} else {
			fd.NFOData = data
		}
	}

	// Fill to file metadata
	if ok := nfo.FillNFO(fd); !ok {
		logging.E(0, "No metadata filled from NFO file...")
	}
	return fd, nil
}
package metadata

import (
	"fmt"
	"metarr/internal/dates"
	"metarr/internal/domain/consts"
	"metarr/internal/domain/enums"
	"metarr/internal/models"
	"metarr/internal/utils/logging"
	"strings"
)

// MakeDateTag attempts to create the date tag for files using metafile data.
func MakeDateTag(metadata map[string]any, fd *models.FileData, dateFmt enums.DateFormat) (string, error) {

	if dateFmt == enums.DATEFMT_SKIP {
		logging.D(1, "Skip set, not making file date tag for %q", fd.OriginalVideoBaseName)
		return "", nil
	}

	var (
		date  string
		found bool
	)

	if fd.MDates.FormattedDate == "" {
		date, found = extractDateFromMetadata(metadata)
		if !found {
			logging.E(0, "No dates found in JSON file")
			return "", nil
		}
	} else {
		date = fd.MDates.FormattedDate
	}

	year, month, day, err := dates.ParseDateComponents(date, dateFmt)
	if err != nil {
		return "", fmt.Errorf("failed to parse date components: %w", err)
	}

	dateStr, err := dates.FormatDateString(year, month, day, dateFmt)
	if dateStr == "" || err != nil {
		logging.E(0, "Failed to create date string")
		return "", nil
	}

	dateTag := fmt.Sprintf("[%s]", dateStr)
	logging.S(0, "Made date tag %q from file '%v'", dateTag, fd.OriginalVideoBaseName)
	return dateTag, nil
}

// extractDateFromMetadata attempts to find a date in the metadata using predefined fields
func extractDateFromMetadata(metadata map[string]any) (string, bool) {
	preferredDateFields := []string{
		consts.JReleaseDate,
		"releasedate",
		"released_on",
		consts.JOriginallyAvailable,
		"originally_available",
		"originallyavailable",
		consts.JDate,
		consts.JUploadDate,
		"uploaddate",
		"uploaded_on",
		consts.JCreationTime, // Last resort, may give false positives
		"created_at",
	}

	for _, field := range preferredDateFields {
		if value, found := metadata[field]; found {
			if strVal, ok := value.(string); ok && strVal != "" && len(strVal) > 4 {
				if date, _, found := strings.Cut(strVal, "T"); found {
					return date, true
				}
				return strVal, true
			}
		}
	}
	return "", false
}
package metadata

import (
	"metarr/internal/cfg"
	keys "metarr/internal/domain/keys"
	"metarr/internal/domain/regex"
	logging "metarr/internal/utils/logging"
	"os"
	"path/filepath"
	"strings"
)

// makeFilenameTag creates the metatag string to prefix filenames with
func MakeFilenameTag(metadata map[string]any, file *os.File) string {
	logging.D(5, "Entering makeFilenameTag with data %v", metadata)

	tagFields := cfg.GetStringSlice(keys.MFilenamePfx)
	if len(tagFields) == 0 {
		return ""
	}

	var b strings.Builder
	b.Grow(len(metadata) + len("[2006-02-01]"))
	b.WriteString("[")

	written := false
	for _, field := range tagFields {

		if value, exists := metadata[field]; exists {
			if strVal, ok := value.(string); ok && strVal != "" {

				if written {
					b.WriteString("_")
				}

				b.WriteString(strVal)
				written = true

				logging.D(3, "Added metafield %v to prefix tag (Tag so far: %s)", field, b.String())
			}
		}
	}

	b.WriteString("]")

	tag := b.String()
	tag = strings.TrimSpace(tag)
	tag = strings.ToValidUTF8(tag, "")

	invalidChars := regex.InvalidCharsCompile()
	tag = invalidChars.ReplaceAllString(tag, "")

	logging.D(1, "Made metatag %q from file %q", tag, file.Name())

	if tag != "[]" {
		if strings.Contains(filepath.Base(file.Name()), tag) {
			logging.D(2, "Tag %q already detected in name, skipping...", tag)
			tag = ""
		} else {
			return tag
		}
	}
	return ""
}
package metadata

import (
	"bytes"
	"encoding/json"
	"fmt"
	"io"
	"metarr/internal/cfg"
	enums "metarr/internal/domain/enums"
	keys "metarr/internal/domain/keys"
	"metarr/internal/models"
	backup "metarr/internal/utils/fs/backup"
	logging "metarr/internal/utils/logging"
	"os"
	"sync"
)

type JSONFileRW struct {
	mu          sync.RWMutex
	muFileWrite sync.Mutex
	Meta        map[string]any
	File        *os.File
	encoder     *json.Encoder
	buffer      *bytes.Buffer
}

// NewJSONFileRW creates a new instance of the JSON file reader/writer
func NewJSONFileRW(file *os.File) *JSONFileRW {
	logging.D(3, "Retrieving new meta writer/rewriter for file %q...", file.Name())
	return &JSONFileRW{
		File: file,
		Meta: metaMapPool.Get().(map[string]any),
	}
}

// DecodeJSON parses and stores JSON metadata into a map and returns it
func (rw *JSONFileRW) DecodeJSON(file *os.File) (map[string]any, error) {
	if file == nil {
		return nil, fmt.Errorf("file passed in nil")
	}

	currentPos, err := file.Seek(0, io.SeekCurrent)
	if err != nil {
		return nil, fmt.Errorf("failed to get current position: %w", err)
	}
	success := false
	defer func() {
		if !success {
			if _, err := file.Seek(currentPos, io.SeekStart); err != nil {
				logging.E(0, err.Error())
			}
		}
	}()

	// Seek start
	if _, err := file.Seek(0, io.SeekStart); err != nil {
		return nil, fmt.Errorf("failed to seek file: %w", err)
	}

	// Decode to map
	decoder := json.NewDecoder(file)
	data := metaMapPool.Get().(map[string]any)

	if err := decoder.Decode(&data); err != nil {
		return nil, fmt.Errorf("failed to decode JSON in DecodeMetadata: %w", err)
	}

	switch {
	case len(data) == 0, data == nil:
		logging.D(3, "Metadata not stored, is blank: %v", data)
		return data, nil
	default:
		rw.updateMeta(data)
		logging.D(5, "Decoded and stored metadata: %v", data)
		success = true
		return data, nil
	}
}

// RefreshJSON reloads the metadata map from the file after updates
func (rw *JSONFileRW) RefreshJSON() (map[string]any, error) {
	if rw.File == nil {
		return nil, fmt.Errorf("file passed in nil")
	}
	return rw.DecodeJSON(rw.File)
}

// WriteJSON inserts metadata into the JSON file from a map
func (rw *JSONFileRW) WriteJSON(fieldMap map[string]*string) (map[string]any, error) {
	if fieldMap == nil {
		return nil, fmt.Errorf("field map passed in nil")
	}

	// Create a copy of the current metadata
	currentMeta := rw.copyMeta()
	logging.D(4, "Entering WriteMetadata for file %q", rw.File.Name())

	// Update metadata with new fields
	updated := false
	for k, ptr := range fieldMap {
		if ptr == nil {
			continue
		}

		if *ptr != "" {

			if currentVal, exists := currentMeta[k]; !exists {
				logging.D(3, "Adding new field %q with value %q", k, *ptr)
				currentMeta[k] = *ptr
				updated = true

			} else if currentStrVal, ok := currentVal.(string); !ok || currentStrVal != *ptr || cfg.GetBool(keys.MOverwrite) {
				logging.D(3, "Updating field %q from '%v' to %q", k, currentVal, *ptr)
				currentMeta[k] = *ptr
				updated = true

			} else {
				logging.D(3, "Skipping field %q - value unchanged and overwrite not forced", k)
			}
		}
	}

	// Return if no updates
	if !updated {
		logging.D(2, "No fields were updated")
		return currentMeta, nil
	}

	// Backup if option set
	if cfg.GetBool(keys.NoFileOverwrite) {
		if err := backup.BackupFile(rw.File); err != nil {
			return currentMeta, fmt.Errorf("failed to create backup: %w", err)
		}
	}

	// Write file
	if err := rw.writeJSONToFile(rw.File, currentMeta); err != nil {
		return currentMeta, err
	}

	rw.updateMeta(currentMeta)

	logging.D(3, "Successfully updated JSON file with new metadata")
	return currentMeta, nil
}

// MakeJSONEdits applies a series of transformations and writes the final result to the file
func (rw *JSONFileRW) MakeJSONEdits(file *os.File, fd *models.FileData) (bool, error) {
	if file == nil {
		return false, fmt.Errorf("file passed in nil")
	}

	currentMeta := rw.copyMeta()

	logging.D(5, "Entering MakeJSONEdits.\nData: %v", currentMeta)

	// SHOULD MOVE THESE INTO THE RESPECTIVE FUNCTIONS
	// THESE PRESENTLY ESCAPE TO HEAP FOR NO GOOD REASON
	var (
		edited, ok bool
		trimPfx    []models.MetaTrimPrefix
		trimSfx    []models.MetaTrimSuffix
		apnd       []models.MetaAppend
		pfx        []models.MetaPrefix
		newField   []models.MetaNewField
		replace    []models.MetaReplace
		copyTo     []models.CopyToField
		pasteFrom  []models.PasteFromField
	)

	// Initialize:
	// Replacements
	if len(fd.ModelMReplace) > 0 {
		logging.I("Model for file %q making replacements", fd.OriginalVideoBaseName)
		replace = fd.ModelMReplace
	} else if cfg.IsSet(keys.MReplaceText) {
		if replace, ok = cfg.Get(keys.MReplaceText).([]models.MetaReplace); !ok {
			logging.E(0, "Could not retrieve prefix trim, wrong type: '%T'", replace)
		}
	}

	// Field trim
	if len(fd.ModelMTrimPrefix) > 0 {
		logging.I("Model for file %q trimming prefixes", fd.OriginalVideoBaseName)
		trimPfx = fd.ModelMTrimPrefix
	} else if cfg.IsSet(keys.MTrimPrefix) {
		if trimPfx, ok = cfg.Get(keys.MTrimPrefix).([]models.MetaTrimPrefix); !ok {
			logging.E(0, "Could not retrieve prefix trim, wrong type: '%T'", trimPfx)
		}
	}

	if len(fd.ModelMTrimSuffix) > 0 {
		logging.I("Model for file %q trimming suffixes", fd.OriginalVideoBaseName)
		trimSfx = fd.ModelMTrimSuffix
	} else if cfg.IsSet(keys.MTrimSuffix) {
		if trimSfx, ok = cfg.Get(keys.MTrimSuffix).([]models.MetaTrimSuffix); !ok {
			logging.E(0, "Could not retrieve suffix trim, wrong type: '%T'", trimSfx)
		}
	}

	// Append and prefix
	if len(fd.ModelMAppend) > 0 {
		logging.I("Model for file %q adding appends", fd.OriginalVideoBaseName)
		apnd = fd.ModelMAppend
	} else if cfg.IsSet(keys.MAppend) {
		if apnd, ok = cfg.Get(keys.MAppend).([]models.MetaAppend); !ok {
			logging.E(0, "Could not retrieve appends, wrong type: '%T'", apnd)
		}
	}

	if len(fd.ModelMPrefix) > 0 {
		logging.I("Model for file %q adding prefixes", fd.OriginalVideoBaseName)
		pfx = fd.ModelMPrefix
	} else if cfg.IsSet(keys.MPrefix) {
		if pfx, ok = cfg.Get(keys.MPrefix).([]models.MetaPrefix); !ok {
			logging.E(0, "Could not retrieve prefix, wrong type: '%T'", pfx)
		}
	}

	// New fields
	if len(fd.ModelMNewField) > 0 {
		logging.I("Model for file %q applying preset new field additions", fd.OriginalVideoBaseName)
		newField = fd.ModelMNewField
	} else if cfg.IsSet(keys.MNewField) {
		if newField, ok = cfg.Get(keys.MNewField).([]models.MetaNewField); !ok {
			logging.E(0, "Could not retrieve new fields, wrong type: '%T'", newField)
		}
	}

	// Copy/paste
	if cfg.IsSet(keys.MCopyToField) {
		if copyTo, ok = cfg.Get(keys.MCopyToField).([]models.CopyToField); !ok {
			logging.E(0, "Could not retrieve copy operations, wrong type: '%T'", copyTo)
		}
	}

	if cfg.IsSet(keys.MPasteFromField) {
		if pasteFrom, ok = cfg.Get(keys.MPasteFromField).([]models.PasteFromField); !ok {
			logging.E(0, "Could not retrieve paste operations, wrong type: '%T'", pasteFrom)
		}
	}

	// Make edits:
	// Replace
	if len(replace) > 0 {
		if ok, err := replaceJSON(currentMeta, replace); err != nil {
			logging.E(0, err.Error())
		} else if ok {
			edited = true
		}
	}

	// Trim
	if len(trimPfx) > 0 {
		if ok, err := trimJSONPrefix(currentMeta, trimPfx); err != nil {
			logging.E(0, err.Error())
		} else if ok {
			edited = true
		}
	}

	if len(trimSfx) > 0 {
		if ok, err := trimJSONSuffix(currentMeta, trimSfx); err != nil {
			logging.E(0, err.Error())
		} else if ok {
			edited = true
		}
	}

	// Append and prefix
	if len(apnd) > 0 {
		if ok, err := jsonAppend(currentMeta, apnd); err != nil {
			logging.E(0, err.Error())
		} else if ok {
			edited = true
		}
	}

	if len(pfx) > 0 {
		if ok, err := jsonPrefix(currentMeta, pfx); err != nil {
			logging.E(0, err.Error())
		} else if ok {
			edited = true
		}
	}

	// Copy/paste
	if len(copyTo) > 0 {
		if ok, err := copyToField(currentMeta, copyTo); err != nil {
			logging.E(0, err.Error())
		} else if ok {
			edited = true
		}
	}

	if len(pasteFrom) > 0 {
		if ok, err := pasteFromField(currentMeta, pasteFrom); err != nil {
			logging.E(0, err.Error())
		} else if ok {
			edited = true
		}
	}

	// Add new
	if len(newField) > 0 {
		if ok, err := setJSONField(currentMeta, rw.File.Name(), fd.ModelMOverwrite, newField); err != nil {
			logging.E(0, err.Error())
		} else if ok {
			edited = true
		}
	}

	if !edited {
		logging.D(3, "No JSON metadata edits made")
		return false, nil
	}

	// Write new metadata to file
	if err := rw.writeJSONToFile(file, currentMeta); err != nil {
		return false, fmt.Errorf("failed to write updated JSON to file: %w", err)
	}

	// Save the meta back into the model
	rw.updateMeta(currentMeta)

	fmt.Println()
	logging.S(0, "Successfully applied metadata edits to: %v", file.Name())

	return edited, nil
}

// JSONDateTagEdits is a public function to add date tags into the metafile, this is useful because
// the dates may not yet be scraped when the initial MakeJSONEdits runs
func (rw *JSONFileRW) JSONDateTagEdits(file *os.File, fd *models.FileData) (edited bool, err error) {
	if file == nil {
		return false, fmt.Errorf("file passed in nil")
	}

	logging.D(4, "Entering MakeDateTagEdits for file %q", file.Name())

	currentMeta := rw.copyMeta()

	logging.D(4, "About to perform MakeDateTagEdits operations for file %q", file.Name())

	// Delete date tag first, user's may want to delete and re-build
	if cfg.IsSet(keys.MDelDateTagMap) {
		logging.D(3, "Stripping metafield date tag...")
		if delDateTagMap, ok := cfg.Get(keys.MDelDateTagMap).(map[string]models.MetaDateTag); ok {

			if len(delDateTagMap) > 0 {

				if ok, err := jsonFieldDateTag(currentMeta, delDateTagMap, fd, enums.DATE_TAG_DEL_OP); err != nil {
					logging.E(0, err.Error())
				} else if ok {
					edited = true
				}
			} else {
				logging.E(0, "delDateTagMap grabbed empty")
			}
		} else {
			logging.E(0, "Got null or wrong type for %s: %T", keys.MDelDateTagMap, delDateTagMap)
		}
	}

	// Add date tag
	if cfg.IsSet(keys.MDateTagMap) {
		logging.D(3, "Adding metafield date tag...")
		if dateTagMap, ok := cfg.Get(keys.MDateTagMap).(map[string]models.MetaDateTag); ok {

			if len(dateTagMap) > 0 {

				if ok, err := jsonFieldDateTag(currentMeta, dateTagMap, fd, enums.DATE_TAG_ADD_OP); err != nil {
					logging.E(0, err.Error())
				} else if ok {
					edited = true
				}
			} else {
				logging.E(0, "dateTagMap grabbed empty")
			}
		} else {
			logging.E(0, "Got null or wrong type for %s: %T", keys.MDateTagMap, dateTagMap)
		}
	}

	if !edited {
		logging.D(2, "No date tag edits made, returning...")
		return false, nil
	}

	// Write back to file
	if err = rw.writeJSONToFile(file, currentMeta); err != nil {
		return false, fmt.Errorf("failed to write updated JSON to file: %w", err)
	}

	rw.updateMeta(currentMeta)

	fmt.Println()
	logging.S(0, "Successfully applied date tag JSON edits to: %v", file.Name())

	return edited, nil
}
package metadata

import (
	"bytes"
	"encoding/json"
	"fmt"
	"io"
	logging "metarr/internal/utils/logging"
	"os"
	"strings"
	"sync"
)

// Map buffer
var metaMapPool = sync.Pool{
	New: func() any {
		return make(map[string]any, 81) // 81 objects in tested JSON file received from yt-dlp
	},
}

// JSON pool buffer
var jsonBufferPool = sync.Pool{
	New: func() any {
		return bytes.NewBuffer(make([]byte, 0, 4096)) // i.e. 4KiB
	},
}

// writeJSONToFile is a private metadata writing helper function
func (rw *JSONFileRW) writeJSONToFile(file *os.File, j map[string]any) error {
	if file == nil {
		return fmt.Errorf("file passed in nil")
	}

	if j == nil {
		return fmt.Errorf("JSON metadata passed in nil")
	}

	if rw.buffer == nil {
		buf := jsonBufferPool.Get().(*bytes.Buffer)
		rw.buffer = buf
	}
	rw.buffer.Reset()
	defer jsonBufferPool.Put(rw.buffer)

	if rw.encoder == nil {
		enc := json.NewEncoder(rw.buffer)
		rw.encoder = enc
	}
	rw.encoder.SetIndent("", "  ")

	// Marshal data
	if err := rw.encoder.Encode(j); err != nil {
		return fmt.Errorf("marshal error: %w", err)
	}

	// Begin file ops
	rw.muFileWrite.Lock()
	defer rw.muFileWrite.Unlock()

	currentPos, err := file.Seek(0, io.SeekCurrent)
	if err != nil {
		return fmt.Errorf("failed to get current position: %w", err)
	}
	success := false
	defer func() {
		if !success {
			if _, err := file.Seek(currentPos, io.SeekStart); err != nil {
				logging.E(0, err.Error())
			}
		}
	}()

	// Seek file start
	if _, err := file.Seek(0, io.SeekStart); err != nil {
		return fmt.Errorf("failed to seek to beginning of file: %w", err)
	}

	// File ops
	if err := file.Truncate(0); err != nil {
		return fmt.Errorf("failed to truncate file: %w", err)
	}

	if _, err := rw.buffer.WriteTo(file); err != nil {
		return fmt.Errorf("failed to write to file: %w", err)
	}

	// Ensure changes are persisted
	if err := file.Sync(); err != nil {
		return fmt.Errorf("failed to sync file: %w", err)
	}

	success = true
	return nil
}

// copyMeta creates a deep copy of the metadata map under read lock
func (rw *JSONFileRW) copyMeta() map[string]any {
	rw.mu.RLock()
	defer rw.mu.RUnlock()

	if rw.Meta == nil {
		return metaMapPool.Get().(map[string]any)
	}

	currentMeta := metaMapPool.Get().(map[string]any)
	for k, v := range rw.Meta {
		currentMeta[k] = v
	}
	return currentMeta
}

// updateMeta safely updates the metadata map under write lock
func (rw *JSONFileRW) updateMeta(newMeta map[string]any) {
	if newMeta == nil {
		newMeta = metaMapPool.Get().(map[string]any)
	}

	rw.mu.Lock()
	oldMeta := rw.Meta
	rw.Meta = newMeta
	rw.mu.Unlock()

	if oldMeta != nil {
		clear(oldMeta)
		metaMapPool.Put(oldMeta)
	}
}

// cleanFieldValue trims leading/trailing whitespaces after deletions
func cleanFieldValue(value string) string {
	cleaned := strings.TrimSpace(value)
	cleaned = strings.Join(strings.Fields(cleaned), " ")
	return cleaned
}
package metadata

import (
	"context"
	"fmt"
	"metarr/internal/cfg"
	enums "metarr/internal/domain/enums"
	keys "metarr/internal/domain/keys"
	tags "metarr/internal/metadata/tags"
	"metarr/internal/models"
	logging "metarr/internal/utils/logging"
	prompt "metarr/internal/utils/prompt"
	"strings"
)

// replaceJSON makes user defined JSON replacements
func replaceJSON(j map[string]any, rplce []models.MetaReplace) (bool, error) {

	logging.D(5, "Entering replaceJson with data: %v", j)

	if len(rplce) == 0 {
		logging.E(0, "Called replaceJson without replacements")
		return false, nil
	}

	edited := false
	for _, r := range rplce {
		if r.Field == "" || r.Value == "" {
			continue
		}

		if val, exists := j[r.Field]; exists {

			if strVal, ok := val.(string); ok {
				logging.D(3, "Identified field %q, replacing %q with %q", r.Field, r.Value, r.Replacement)
				j[r.Field] = strings.ReplaceAll(strVal, r.Value, r.Replacement)
				edited = true
			}
		}
	}
	logging.D(5, "After JSON replace: %v", j)
	return edited, nil
}

// trimJSONPrefix trims defined prefixes from specified fields
func trimJSONPrefix(j map[string]any, tPfx []models.MetaTrimPrefix) (bool, error) {

	logging.D(5, "Entering trimJsonPrefix with data: %v", j)

	if len(tPfx) == 0 {
		logging.E(0, "Called trimJsonPrefix without prefixes to trim")
		return false, nil
	}

	edited := false
	for _, p := range tPfx {
		if p.Field == "" || p.Prefix == "" {
			continue
		}

		if val, exists := j[p.Field]; exists {

			if strVal, ok := val.(string); ok {
				logging.D(3, "Identified field %q, trimming %q", p.Field, p.Prefix)
				j[p.Field] = strings.TrimPrefix(strVal, p.Prefix)
				edited = true
			}
		}
	}
	logging.D(5, "After prefix trim: %v", j)
	return edited, nil
}

// trimJSONSuffix trims defined suffixes from specified fields
func trimJSONSuffix(j map[string]any, tSfx []models.MetaTrimSuffix) (bool, error) {

	logging.D(5, "Entering trimJsonSuffix with data: %v", j)

	if len(tSfx) == 0 {
		logging.E(0, "Called trimJsonSuffix without prefixes to trim")
		return false, nil
	}

	edited := false
	for _, s := range tSfx {
		if s.Field == "" || s.Suffix == "" {
			continue
		}

		if val, exists := j[s.Field]; exists {

			if strVal, ok := val.(string); ok {
				logging.D(3, "Identified field %q, trimming %q", s.Field, s.Suffix)
				j[s.Field] = strings.TrimSuffix(strVal, s.Suffix)
				edited = true
			}
		}
	}
	logging.D(5, "After suffix trim: %v", j)
	return edited, nil
}

// jsonAppend appends to the fields in the JSON data
func jsonAppend(j map[string]any, apnd []models.MetaAppend) (bool, error) {

	logging.D(5, "Entering jsonAppend with data: %v", j)

	if len(apnd) == 0 {
		logging.E(0, "No new suffixes to append", keys.MAppend)
		return false, nil // No replacements to apply
	}

	edited := false
	for _, a := range apnd {
		if a.Field == "" || a.Suffix == "" {
			continue
		}

		if value, exists := j[a.Field]; exists {

			if strVal, ok := value.(string); ok {

				logging.D(3, "Identified input JSON field '%v', appending '%v'", a.Field, a.Suffix)
				strVal += a.Suffix
				j[a.Field] = strVal
				edited = true
			}
		}
	}
	logging.D(5, "After JSON suffix append: %v", j)

	return edited, nil
}

// metaPrefix applies prefixes to the fields in the JSON data
func jsonPrefix(j map[string]any, pfx []models.MetaPrefix) (bool, error) {

	logging.D(5, "Entering jsonPrefix with data: %v", j)

	if len(pfx) == 0 {
		logging.E(0, "No new prefix replacements found", keys.MPrefix)
		return false, nil // No replacements to apply
	}

	edited := false
	for _, p := range pfx {
		if p.Field == "" || p.Prefix == "" {
			continue
		}

		if value, found := j[p.Field]; found {

			if strVal, ok := value.(string); ok {
				logging.D(3, "Identified input JSON field '%v', adding prefix '%v'", p.Field, p.Prefix)
				strVal = p.Prefix + strVal
				j[p.Field] = strVal
				edited = true

			}
		}
	}
	logging.D(5, "After adding prefixes: %v", j)

	return edited, nil
}

// setJSONField can insert a new field which does not yet exist into the metadata file
func setJSONField(j map[string]any, file string, ow bool, newField []models.MetaNewField) (bool, error) {
	if len(newField) == 0 {
		logging.E(0, "No new field additions found", keys.MNewField)
		return false, nil
	}

	var (
		metaOW,
		metaPS bool
	)

	if !cfg.IsSet(keys.MOverwrite) && !cfg.IsSet(keys.MPreserve) {
		logging.I("Model is set to overwrite")
		metaOW = ow
	} else {
		metaOW = cfg.GetBool(keys.MOverwrite)
		metaPS = cfg.GetBool(keys.MPreserve)
		logging.I("Meta OW: %v Meta Preserve: %v", metaOW, metaPS)
	}

	logging.D(3, "Retrieved additions for new field data: %v", newField)
	processedFields := make(map[string]bool, len(newField))

	newAddition := false
	ctx := context.Background()
	for _, n := range newField {
		if n.Field == "" || n.Value == "" {
			continue
		}

		// If field doesn't exist at all, add it
		if _, exists := j[n.Field]; !exists {
			j[n.Field] = n.Value
			processedFields[n.Field] = true
			newAddition = true
			continue
		}

		// Field already exists, check with user
		if !metaOW {

			// Check for cancellation
			select {
			case <-ctx.Done():
				logging.I("Operation canceled for field: %s", n.Field)
				return false, fmt.Errorf("operation canceled")
			default:
			}

			if _, alreadyProcessed := processedFields[n.Field]; alreadyProcessed {
				continue
			}

			if existingValue, exists := j[n.Field]; exists {
				if !metaPS {
					promptMsg := fmt.Sprintf(
						"Field %q already exists with value '%v' in file '%v'. Overwrite? (y/n) to proceed, (Y/N) to apply to whole queue",
						n.Field, existingValue, file,
					)

					reply, err := prompt.PromptMetaReplace(promptMsg, metaOW, metaPS)
					if err != nil {
						logging.E(0, err.Error())
					}

					switch reply {
					case "Y":
						logging.D(2, "Received meta overwrite reply as 'Y' for %s in %s, falling through to 'y'", existingValue, file)
						cfg.Set(keys.MOverwrite, true)
						metaOW = true
						fallthrough

					case "y":
						logging.D(2, "Received meta overwrite reply as 'y' for %s in %s", existingValue, file)
						n.Field = strings.TrimSpace(n.Field)
						logging.D(3, "Adjusted field from %q to %q\n", j[n.Field], n.Field)

						j[n.Field] = n.Value
						processedFields[n.Field] = true
						newAddition = true

					case "N":
						logging.D(2, "Received meta overwrite reply as 'N' for %s in %s, falling through to 'n'", existingValue, file)
						cfg.Set(keys.MPreserve, true)
						metaPS = true
						fallthrough

					case "n":
						logging.D(2, "Received meta overwrite reply as 'n' for %s in %s", existingValue, file)
						logging.P("Skipping field %q\n", n.Field)
						processedFields[n.Field] = true
					}
				}

				switch {
				case metaOW: // EXISTS and FieldOverwrite is set
					j[n.Field] = n.Value
					processedFields[n.Field] = true
					newAddition = true

				case metaPS: // EXISTS and FieldPreserve is set
					continue
				}
			}
		} else {
			// Field does not exist or overwrite is true
			j[n.Field] = n.Value
			processedFields[n.Field] = true
			newAddition = true
		}
	}
	logging.D(3, "JSON after transformations: %v", j)

	return newAddition, nil
}

// jsonFieldDateTag sets date tags in designated meta fields
func jsonFieldDateTag(j map[string]any, dtm map[string]models.MetaDateTag, fd *models.FileData, op enums.MetaDateTaggingType) (bool, error) {

	logging.D(2, "Making metadata date tag for %q...", fd.OriginalVideoBaseName)

	if len(dtm) == 0 {
		logging.D(3, "No date tag operations to perform")
		return false, nil
	}
	if fd == nil {
		return false, fmt.Errorf("jsonFieldDateTag called with null FileData model")
	}

	edited := false
	for fld, d := range dtm {
		val, exists := j[fld]
		if !exists {
			logging.D(3, "Field %q not found in metadata", fld)
			continue
		}

		strVal, ok := val.(string)
		if !ok {
			logging.D(3, "Field %q is not a string value, type: %T", fld, val)
			continue
		}

		// Generate the date tag
		tag, err := tags.MakeDateTag(j, fd, d.Format)
		if err != nil || tag == "" {
			return false, fmt.Errorf("failed to generate date tag for field %q: %w", fld, err)
		}

		if strings.Contains(strVal, tag) {
			logging.I("Tag %q already exists in field %q", tag, strVal)
			return false, nil
		}

		// Apply the tag based on location
		switch d.Loc {
		case enums.DATE_TAG_LOC_PFX:

			switch op {
			case enums.DATE_TAG_DEL_OP:
				before := strVal
				result := strings.TrimPrefix(strVal, tag)
				result = cleanFieldValue(result)

				j[fld] = result

				if j[fld] != before {
					logging.I("Deleted date tag %q prefix from field %q", tag, fld)
					edited = true
				} else {
					logging.E(0, "Failed to strip date tag from %q", before)
				}

			case enums.DATE_TAG_ADD_OP:

				j[fld] = fmt.Sprintf("%s %s", tag, strVal)
				logging.I("Added date tag %q as prefix to field %q", tag, fld)
				edited = true
			}

		case enums.DATE_TAG_LOC_SFX:

			switch op {
			case enums.DATE_TAG_DEL_OP:

				before := strVal
				result := strings.TrimPrefix(strVal, tag)
				result = cleanFieldValue(result)
				j[fld] = result

				if j[fld] != before {
					logging.I("Deleted date tag %q suffix from field %q", tag, fld)
					edited = true
				} else {
					logging.E(0, "Failed to strip date tag from %q", before)
				}

			case enums.DATE_TAG_ADD_OP:

				j[fld] = fmt.Sprintf("%s %s", strVal, tag)
				logging.I("Added date tag %q as suffix to field %q", tag, fld)
				edited = true
			}

		default:
			return false, fmt.Errorf("invalid date tag location enum: %v", d.Loc)
		}
	}
	return edited, nil
}

// copyToField copies values from one meta field to another
func copyToField(j map[string]any, copyTo []models.CopyToField) (bool, error) {

	logging.D(5, "Entering jsonPrefix with data: %v", j)

	if len(copyTo) == 0 {
		logging.E(0, "No new copy operations found")
		return false, nil
	}

	edited := false
	for _, c := range copyTo {
		if c.Field == "" || c.Dest == "" {
			continue
		}

		if value, found := j[c.Field]; found {

			if val, ok := value.(string); ok {
				logging.I("Identified input JSON field '%v', copying to field '%v'", c.Field, c.Dest)
				j[c.Dest] = val
				edited = true

			}
		}
	}
	logging.D(5, "After making copy operation changes: %v", j)

	return edited, nil
}

// pasteFromField copies values from one meta field to another
func pasteFromField(j map[string]any, paste []models.PasteFromField) (bool, error) {

	logging.D(5, "Entering jsonPrefix with data: %v", j)

	if len(paste) == 0 {
		logging.E(0, "No new paste operations found")
		return false, nil
	}

	edited := false
	for _, p := range paste {
		if p.Field == "" || p.Origin == "" {
			continue
		}

		if value, found := j[p.Origin]; found {

			if val, ok := value.(string); ok {
				logging.I("Identified input JSON field '%v', pasting to field '%v'", p.Origin, p.Field)
				j[p.Field] = val
				edited = true
			}
		}
	}
	logging.D(5, "After making paste operation changes: %v", j)

	return edited, nil
}
package metadata

import (
	"bufio"
	"context"
	"encoding/xml"
	"fmt"
	"io"
	"metarr/internal/cfg"
	keys "metarr/internal/domain/keys"
	"metarr/internal/models"
	logging "metarr/internal/utils/logging"
	prompt "metarr/internal/utils/prompt"
	"os"
	"strings"
	"sync"
)

type NFOFileRW struct {
	mu    sync.RWMutex
	Model *models.NFOData
	Meta  string
	File  *os.File
}

// NewNFOFileRW creates a new instance of the NFO file reader/writer
func NewNFOFileRW(file *os.File) *NFOFileRW {
	logging.D(3, "Retrieving new meta writer/rewriter for file %q...", file.Name())
	return &NFOFileRW{
		File: file,
	}
}

// DecodeMetadata decodes XML from a file into a map, stores, and returns it
func (rw *NFOFileRW) DecodeMetadata(file *os.File) (*models.NFOData, error) {
	rw.mu.Lock()
	defer rw.mu.Unlock()

	// Read the entire file content first
	content, err := io.ReadAll(file)
	if err != nil {
		return nil, fmt.Errorf("failed to read file: %w", err)
	}

	rtn := rw.ensureXMLStructure(string(content))
	if rtn != "" {
		content = []byte(rtn)
	}

	// Store the raw content
	rw.Meta = string(content)

	// Reset file pointer
	if _, err := file.Seek(0, io.SeekStart); err != nil {
		return nil, fmt.Errorf("failed to seek file: %w", err)
	}

	// Single decode for the model
	decoder := xml.NewDecoder(file)
	var input *models.NFOData
	if err := decoder.Decode(&input); err != nil {
		return nil, fmt.Errorf("failed to decode XML: %w", err)
	}

	rw.Model = input
	logging.D(3, "Decoded metadata: %v", rw.Model)

	return rw.Model, nil
}

// RefreshMetadata reloads the metadata map from the file after updates
func (rw *NFOFileRW) RefreshMetadata() (*models.NFOData, error) {

	rw.mu.RLock()
	defer rw.mu.RUnlock()

	if _, err := rw.File.Seek(0, io.SeekStart); err != nil {
		return nil, fmt.Errorf("failed to seek file: %w", err)
	}

	// Decode metadata
	decoder := xml.NewDecoder(rw.File)

	if err := decoder.Decode(&rw.Model); err != nil {
		return nil, fmt.Errorf("failed to decode xml: %w", err)
	}

	logging.D(3, "Decoded metadata: %v", rw.Model)

	return rw.Model, nil
}

// MakeMetaEdits applies a series of transformations and writes the final result to the file
func (rw *NFOFileRW) MakeMetaEdits(data string, file *os.File, fd *models.FileData) (bool, error) {
	// Ensure we have valid XML
	if !strings.Contains(data, "<movie>") {
		return false, fmt.Errorf("invalid XML: missing movie tag")
	}

	var (
		edited, ok bool
		newContent string
		err        error

		trimPfx []models.MetaTrimPrefix
		trimSfx []models.MetaTrimSuffix

		apnd []models.MetaAppend
		pfx  []models.MetaPrefix

		newField []models.MetaNewField

		replace []models.MetaReplace
	)

	// Replacements
	if len(fd.ModelMReplace) > 0 {
		logging.I("Model for file %q making replacements", fd.OriginalVideoBaseName)
		replace = fd.ModelMReplace
	} else if cfg.IsSet(keys.MReplaceText) {
		if replace, ok = cfg.Get(keys.MReplaceText).([]models.MetaReplace); !ok {
			logging.E(0, "Count not retrieve prefix trim, wrong type: '%T'", replace)
		}
	}

	// Field trim
	if len(fd.ModelMTrimPrefix) > 0 {
		logging.I("Model for file %q trimming prefixes", fd.OriginalVideoBaseName)
		trimPfx = fd.ModelMTrimPrefix
	} else if cfg.IsSet(keys.MTrimPrefix) {
		if trimPfx, ok = cfg.Get(keys.MTrimPrefix).([]models.MetaTrimPrefix); !ok {
			logging.E(0, "Count not retrieve prefix trim, wrong type: '%T'", trimPfx)
		}
	}

	if len(fd.ModelMTrimSuffix) > 0 {
		logging.I("Model for file %q trimming suffixes", fd.OriginalVideoBaseName)
		trimSfx = fd.ModelMTrimSuffix
	} else if cfg.IsSet(keys.MTrimSuffix) {
		if trimSfx, ok = cfg.Get(keys.MTrimSuffix).([]models.MetaTrimSuffix); !ok {
			logging.E(0, "Count not retrieve suffix trim, wrong type: '%T'", trimSfx)
		}
	}

	// Append and prefix
	if len(fd.ModelMAppend) > 0 {
		logging.I("Model for file %q adding appends", fd.OriginalVideoBaseName)
		apnd = fd.ModelMAppend
	} else if cfg.IsSet(keys.MAppend) {
		if apnd, ok = cfg.Get(keys.MAppend).([]models.MetaAppend); !ok {
			logging.E(0, "Count not retrieve appends, wrong type: '%T'", apnd)
		}
	}

	if len(fd.ModelMPrefix) > 0 {
		logging.I("Model for file %q adding prefixes", fd.OriginalVideoBaseName)
		pfx = fd.ModelMPrefix
	} else if cfg.IsSet(keys.MPrefix) {
		if pfx, ok = cfg.Get(keys.MPrefix).([]models.MetaPrefix); !ok {
			logging.E(0, "Count not retrieve prefix, wrong type: '%T'", pfx)
		}
	}

	// New fields
	if len(fd.ModelMNewField) > 0 {
		logging.I("Model for file %q applying preset new field additions", fd.OriginalVideoBaseName)
		newField = fd.ModelMNewField
	} else if cfg.IsSet(keys.MNewField) {
		if newField, ok = cfg.Get(keys.MNewField).([]models.MetaNewField); !ok {
			logging.E(0, "Could not retrieve new fields, wrong type: '%T'", pfx)
		}
	}

	// Make edits:
	// Replace
	if len(replace) > 0 {
		if newContent, ok, err = rw.replaceXml(data, replace); err != nil {
			logging.E(0, err.Error())
		} else if ok {
			edited = true
		}
	}

	// Trim
	if len(trimPfx) > 0 {
		if newContent, ok, err = rw.trimXmlPrefix(data, trimPfx); err != nil {
			logging.E(0, err.Error())
		} else if ok {
			edited = true
		}
	}

	if len(trimSfx) > 0 {
		if newContent, ok, err = rw.trimXmlSuffix(data, trimSfx); err != nil {
			logging.E(0, err.Error())
		} else if ok {
			edited = true
		}
	}

	// Append and prefix
	if len(apnd) > 0 {
		if newContent, ok, err = rw.xmlAppend(data, apnd); err != nil {
			logging.E(0, err.Error())
		} else if ok {
			edited = true
		}
	}

	if len(pfx) > 0 {
		if newContent, ok, err = rw.xmlPrefix(data, pfx); err != nil {
			logging.E(0, err.Error())
		} else if ok {
			edited = true
		}
	}

	// Add new
	if len(newField) > 0 {
		if newContent, ok, err = rw.addNewXmlFields(data, fd.ModelMOverwrite, newField); err != nil {
			logging.E(0, err.Error())
		} else if ok {
			edited = true
		}
	}

	// Only write if changes were made
	if edited {
		if err := rw.writeMetadataToFile(file, []byte(newContent)); err != nil {
			return false, fmt.Errorf("failed to refresh metadata: %w", err)
		}
	}

	return edited, nil
}

// Helper function to ensure XML structure
func (rw *NFOFileRW) ensureXMLStructure(content string) string {
	// Ensure XML declaration
	if !strings.HasPrefix(content, "<?xml") {

		content = fmt.Sprintf(`<?xml version="1.0" encoding="UTF-8"?>
%s`, content)
	}

	// Ensure movie tag exists
	if !strings.Contains(content, "<movie>") {
		content = strings.TrimSpace(content)
		content = fmt.Sprintf("%s\n<movie>\n</movie>", content)
	}

	return content
}

// refreshMetadataInternal is a private metadata refresh function
func (rw *NFOFileRW) refreshMetadataInternal(file *os.File) error {

	if _, err := file.Seek(0, io.SeekStart); err != nil {
		return fmt.Errorf("failed to seek file: %w", err)
	}

	if rw.Model == nil {
		return fmt.Errorf("NFOFileRW's stored metadata map is empty or null, did you forget to decode?")
	}

	decoder := xml.NewDecoder(file)
	if err := decoder.Decode(&rw.Model); err != nil {
		return fmt.Errorf("failed to decode xml: %w", err)
	}

	return nil
}

// writeMetadataToFile is a private metadata writing helper function
func (rw *NFOFileRW) writeMetadataToFile(file *os.File, content []byte) error {

	if err := file.Truncate(0); err != nil {
		return fmt.Errorf("truncate file: %w", err)
	}

	if _, err := file.Seek(0, io.SeekStart); err != nil {
		return fmt.Errorf("seek file: %w", err)
	}

	// Use buffered writer for efficiency
	writer := bufio.NewWriter(file)
	if _, err := writer.Write(content); err != nil {
		return fmt.Errorf("write content: %w", err)
	}

	if err := rw.refreshMetadataInternal(file); err != nil {
		return fmt.Errorf("failed to refresh metadata: %w", err)
	}

	return writer.Flush()
}

// replaceMeta applies meta replacement to the fields in the xml data
func (rw *NFOFileRW) replaceXml(data string, replace []models.MetaReplace) (dataRtn string, edited bool, err error) {

	logging.D(5, "Entering replaceXml with data: %v", string(data))

	if len(replace) == 0 {
		return data, false, nil // No replacements to apply
	}

	for _, replacement := range replace {
		if replacement.Field == "" || replacement.Value == "" {
			continue
		}

		startTag := fmt.Sprintf("<%s>", replacement.Field)
		endTag := fmt.Sprintf("</%s>", replacement.Field)

		startIdx := strings.Index(data, startTag)
		endIdx := strings.Index(data, endTag)
		if startIdx == -1 || endIdx == -1 {
			continue // One or both tags missing
		}

		contentStart := startIdx + len(startTag)
		content := strings.TrimSpace(data[contentStart:endIdx])

		logging.D(2, "Identified input xml field %q, replacing %q with %q", replacement.Field, replacement.Value, replacement.Replacement)

		content = strings.ReplaceAll(content, replacement.Value, replacement.Replacement)
		data = data[:contentStart] + content + data[endIdx:]
		edited = true
	}
	logging.D(5, "After meta replacements: %v", data)
	return data, edited, nil
}

// trimMetaPrefix applies meta replacement to the fields in the xml data
func (rw *NFOFileRW) trimXmlPrefix(data string, trimPfx []models.MetaTrimPrefix) (dataRtn string, edited bool, err error) {

	logging.D(5, "Entering trimXmlPrefix with data: %v", string(data))

	if len(trimPfx) == 0 {
		return data, false, nil // No replacements to apply
	}

	for _, prefix := range trimPfx {
		if prefix.Field == "" || prefix.Prefix == "" {
			continue
		}

		startTag := fmt.Sprintf("<%s>", prefix.Field)
		endTag := fmt.Sprintf("</%s>", prefix.Field)

		startIdx := strings.Index(data, startTag)
		endIdx := strings.Index(data, endTag)
		if startIdx == -1 || endIdx == -1 {
			continue // One or both tags missing
		}

		contentStart := startIdx + len(startTag)
		content := strings.TrimSpace(data[contentStart:endIdx])

		logging.D(2, "Identified input xml field %q, trimming prefix %q", prefix.Field, prefix.Prefix)

		content = strings.TrimPrefix(content, prefix.Prefix)
		data = data[:contentStart] + content + data[endIdx:]
		edited = true
	}
	logging.D(5, "After trimming prefixes: %v", data)
	return data, edited, nil
}

// trimMetaSuffix trims specified
func (rw *NFOFileRW) trimXmlSuffix(data string, trimSfx []models.MetaTrimSuffix) (dataRtn string, edited bool, err error) {

	logging.D(5, "Entering trimXmlSuffix with data: %v", string(data))

	if len(trimSfx) == 0 {
		return data, false, nil // No replacements to apply
	}

	for _, suffix := range trimSfx {
		if suffix.Field == "" || suffix.Suffix == "" {
			continue
		}

		startTag := fmt.Sprintf("<%s>", suffix.Field)
		endTag := fmt.Sprintf("</%s>", suffix.Field)

		startIdx := strings.Index(data, startTag)
		endIdx := strings.Index(data, endTag)
		if startIdx == -1 || endIdx == -1 {
			continue // One or both tags missing
		}

		contentStart := startIdx + len(startTag)
		content := strings.TrimSpace(data[contentStart:endIdx])

		logging.D(2, "Identified input xml field %q, trimming suffix %q", suffix.Field, suffix.Suffix)

		content = strings.TrimSuffix(content, suffix.Suffix)
		data = data[:contentStart] + content + data[endIdx:]
		edited = true
	}
	logging.D(5, "After meta replacements: %v", data)
	return data, edited, nil
}

// trimMetaPrefix applies meta replacement to the fields in the xml data
func (rw *NFOFileRW) xmlPrefix(data string, pfx []models.MetaPrefix) (dataRtn string, edited bool, err error) {

	logging.D(5, "Entering xmlPrefix with data: %v", string(data))

	if len(pfx) == 0 {
		return data, false, nil // No replacements to apply
	}

	for _, prefix := range pfx {
		if prefix.Field == "" || prefix.Prefix == "" {
			continue
		}

		startTag := fmt.Sprintf("<%s>", prefix.Field)
		endTag := fmt.Sprintf("</%s>", prefix.Field)

		startIdx := strings.Index(data, startTag)
		endIdx := strings.Index(data, endTag)
		if startIdx == -1 || endIdx == -1 {
			continue // One or both tags missing
		}

		contentStart := startIdx + len(startTag)
		content := strings.TrimSpace(data[contentStart:endIdx])

		logging.D(2, "Identified input xml field %q, adding prefix %q", prefix.Field, prefix.Prefix)

		data = data[:contentStart] + prefix.Prefix + content + data[endIdx:]
		edited = true
	}
	logging.D(5, "After trimming prefixes: %v", data)
	return data, edited, nil
}

// trimMetaSuffix trims specified
func (rw *NFOFileRW) xmlAppend(data string, apnd []models.MetaAppend) (dataRtn string, edited bool, err error) {

	logging.D(5, "Entering xmlAppend with data: %v", string(data))

	if len(apnd) == 0 {
		return data, false, nil // No replacements to apply
	}

	for _, append := range apnd {
		if append.Field == "" || append.Suffix == "" {
			continue
		}

		startTag := fmt.Sprintf("<%s>", append.Field)
		endTag := fmt.Sprintf("</%s>", append.Field)

		startIdx := strings.Index(data, startTag)
		endIdx := strings.Index(data, endTag)
		if startIdx == -1 || endIdx == -1 {
			continue // One or both tags missing
		}

		contentStart := startIdx + len(startTag)
		content := strings.TrimSpace(data[contentStart:endIdx])

		logging.D(2, "Identified input xml field %q, appending suffix %q", append.Field, append.Suffix)

		data = data[:contentStart] + content + append.Suffix + data[endIdx:]
		edited = true
	}
	logging.D(5, "After meta replacements: %v", data)
	return data, edited, nil
}

// addNewField can insert a new field which does not yet exist into the metadata file
func (rw *NFOFileRW) addNewXmlFields(data string, ow bool, newField []models.MetaNewField) (dataRtn string, newAddition bool, err error) {

	var (
		metaOW,
		metaPS bool
	)

	logging.D(5, "Entering addNewXmlFields with data: %v", string(data))

	if len(newField) == 0 {
		return data, false, nil // No replacements to apply
	}

	if ow {
		metaOW = true
	} else {
		metaOW = cfg.GetBool(keys.MOverwrite)
		metaPS = cfg.GetBool(keys.MPreserve)
	}

	logging.D(3, "Retrieved additions for new field data: %v", newField)

	ctx := context.Background()

	for _, addition := range newField {
		if addition.Field == "" || addition.Value == "" {
			continue
		}

		// Special handling for actor fields
		if addition.Field == "actor" {
			// Check if actor already exists
			flatData := rw.flattenField(data)
			actorNameCheck := fmt.Sprintf("<name>%s</name>", rw.flattenField(addition.Value))

			if strings.Contains(flatData, actorNameCheck) {
				logging.I("Actor %q is already inserted in the metadata, no need to add...", addition.Value)
			} else {
				if modified, ok := rw.addNewActorField(data, addition.Value); ok {
					data = modified
					newAddition = true
				}
			}
			continue
		}

		// Handle non-actor fields
		tagStart := fmt.Sprintf("<%s>", addition.Field)
		tagEnd := fmt.Sprintf("</%s>", addition.Field)

		startIdx := strings.Index(data, tagStart)
		if startIdx == -1 {
			// Field doesn't exist, add it
			if modified, ok := rw.addNewField(data, fmt.Sprintf("%s%s%s", tagStart, addition.Value, tagEnd)); ok {
				data = modified
				newAddition = true
			}
			continue
		}

		// Field exists, handle overwrite
		if !metaOW {
			startContent := startIdx + len(tagStart)
			endIdx := strings.Index(data, tagEnd)
			content := strings.TrimSpace(data[startContent:endIdx])

			// Check for context cancellation
			select {
			case <-ctx.Done():
				logging.I("Operation canceled for field: %s", addition.Field)
				return data, false, fmt.Errorf("operation canceled")
			default:
				// Proceed
			}

			if !metaOW && !metaPS {
				promptMsg := fmt.Sprintf("Field %q already exists with value '%v' in file '%v'. Overwrite? (y/n) to proceed, (Y/N) to apply to whole queue",
					addition.Field, content, rw.File.Name())

				reply, err := prompt.PromptMetaReplace(promptMsg, metaOW, metaPS)
				if err != nil {
					logging.E(0, err.Error())
				}

				switch reply {
				case "Y":
					cfg.Set(keys.MOverwrite, true)
					metaOW = true
					fallthrough
				case "y":
					data = data[:startContent] + addition.Value + data[endIdx:]
					newAddition = true
				case "N":
					cfg.Set(keys.MPreserve, true)
					metaPS = true
					fallthrough
				case "n":
					logging.D(2, "Skipping field: %s", addition.Field)
				}
			} else if metaOW {
				data = data[:startContent] + addition.Value + data[endIdx:]
				newAddition = true
			}
		}
	}

	return data, newAddition, nil
}

// addNewField adds a new field into the NFO
func (rw *NFOFileRW) addNewField(data, addition string) (string, bool) {

	insertIdx := strings.Index(data, "<movie>")
	insertAfter := insertIdx + len("<movie>")

	if insertIdx != -1 {
		data = data[:insertAfter] + "\n" + addition + "\n" + data[insertAfter:]
	}
	return data, true
}

// addNewActorField adds a new actor into the file
func (rw *NFOFileRW) addNewActorField(data, name string) (string, bool) {
	castStart := strings.Index(data, "<cast>")
	castEnd := strings.Index(data, "</cast>")

	if castStart == -1 && castEnd == -1 {
		// No cast tag exists, create new structure
		movieStart := strings.Index(data, "<movie>")
		if movieStart == -1 {
			logging.E(0, "Invalid XML structure: no movie tag found")
			return data, false
		}

		movieEnd := strings.Index(data, "</movie>")
		if movieEnd == -1 {
			logging.E(0, "Invalid XML structure: no closing movie tag found")
			return data, false
		}

		// Create new cast section
		newCast := fmt.Sprintf("    <cast>\n        <actor>\n            <name>%s</name>\n        </actor>\n    </cast>", name)

		// Find the right spot to insert
		contentStart := movieStart + len("<movie>")
		if contentStart >= len(data) {
			logging.E(0, "Invalid XML structure: movie tag at end of data")
			return data, false
		}

		return data[:contentStart] + "\n" + newCast + "\n" + data[contentStart:], true
	}

	// Cast exists, validate indices
	if castStart == -1 || castEnd == -1 || castStart >= len(data) || castEnd > len(data) {
		logging.E(0, "Invalid XML structure: mismatched cast tags")
		return data, false
	}

	// Insert new actor
	newActor := fmt.Sprintf("    <actor>\n            <name>%s</name>\n        </actor>", name)

	if castEnd-castStart > 1 {
		// Cast has content, insert with proper spacing
		return data[:castEnd] + newActor + "\n    " + data[castEnd:], true
	} else {
		// Empty cast tag
		insertPoint := castStart + len("<cast>")
		return data[:insertPoint] + newActor + "\n    " + data[insertPoint:], true
	}
}

// flattenField flattens the metadata field for comparison
func (rw *NFOFileRW) flattenField(s string) string {

	rtn := strings.TrimSpace(s)
	rtn = strings.ReplaceAll(rtn, " ", "")
	rtn = strings.ReplaceAll(rtn, "\n", "")
	rtn = strings.ReplaceAll(rtn, "\r", "")
	rtn = strings.ReplaceAll(rtn, "\t", "")

	return rtn
}
package models

type Batch struct {
	Video      string
	Json       string
	IsDirs     bool
	SkipVideos bool
}
package models

import (
	"context"
	enums "metarr/internal/domain/enums"
	"os"
	"sync"
)

func NewFileData() *FileData {
	return &FileData{
		MTitleDesc: &MetadataTitlesDescs{},
		MCredits:   &MetadataCredits{},
		MDates:     &MetadataDates{},
		MShowData:  &MetadataShowData{},
		MWebData:   &MetadataWebData{},
		MOther:     &MetadataOtherData{},
	}
}

type FileData struct {
	// Files & dirs
	VideoDirectory        string   `json:"-" xml:"-"`
	OriginalVideoPath     string   `json:"-" xml:"-"`
	OriginalVideoBaseName string   `json:"-" xml:"-"`
	TempOutputFilePath    string   `json:"-" xml:"-"`
	FinalVideoPath        string   `json:"-" xml:"-"`
	FinalVideoBaseName    string   `json:"-" xml:"-"`
	VideoFile             *os.File `json:"-" xml:"-"`

	// Transformations
	FilenameMetaPrefix string `json:"-" xml:"-"`
	FilenameDateTag    string `json:"-" xml:"-"`
	RenamedVideoPath   string `json:"-" xml:"-"`
	RenamedMetaPath    string `json:"-" xml:"-"`

	// JSON paths
	JSONDirectory string `json:"-" xml:"-"`
	JSONFilePath  string `json:"-" xml:"-"`
	JSONBaseName  string `json:"-" xml:"-"`

	// NFO paths
	NFOBaseName  string `json:"-" xml:"-"`
	NFODirectory string `json:"-" xml:"-"`
	NFOFilePath  string `json:"-" xml:"-"`

	// Metadata
	MCredits   *MetadataCredits     `json:"meta_credits" xml:"credits"`
	MTitleDesc *MetadataTitlesDescs `json:"meta_title_description" xml:"titles"`
	MDates     *MetadataDates       `json:"meta_dates" xml:"dates"`
	MShowData  *MetadataShowData    `json:"meta_show_data" xml:"show"`
	MWebData   *MetadataWebData     `json:"meta_web_data" xml:"web"`
	MOther     *MetadataOtherData   `json:"meta_other_data" xml:"other"`
	NFOData    *NFOData

	// File writers
	JSONFileRW JSONFileRW
	NFOFileRW  NFOFileRW

	// Own transformations
	ModelMAppend     []MetaAppend
	ModelMNewField   []MetaNewField
	ModelMPrefix     []MetaPrefix
	ModelMReplace    []MetaReplace
	ModelMTrimPrefix []MetaTrimPrefix
	ModelMTrimSuffix []MetaTrimSuffix

	ModelFileSfxReplace []FilenameReplaceSuffix

	// Misc
	MetaFileType      enums.MetaFiletypeFound `json:"-" xml:"-"`
	MetaAlreadyExists bool                    `json:"-" xml:"-"`
	ModelMOverwrite   bool
}

type Core struct {
	Cleanup chan os.Signal
	Cancel  context.CancelFunc
	Ctx     context.Context
	Wg      *sync.WaitGroup
}
package models

import "os"

// Metadata read/write interface
type JSONFileRW interface {
	DecodeJSON(file *os.File) (map[string]any, error)
	RefreshJSON() (map[string]any, error)
	WriteJSON(fieldMap map[string]*string) (map[string]any, error)
	MakeJSONEdits(file *os.File, fd *FileData) (bool, error)
	JSONDateTagEdits(file *os.File, fd *FileData) (edited bool, err error)
}

// Metadata read/write interface
type NFOFileRW interface {
	DecodeMetadata(file *os.File) (*NFOData, error)
	RefreshMetadata() (*NFOData, error)
	MakeMetaEdits(data string, file *os.File, fd *FileData) (bool, error)
}
package models

import (
	enums "metarr/internal/domain/enums"
	"net/http"
)

var AppendOverrideMap map[enums.OverrideMetaType]string
var ReplaceOverrideMap map[enums.OverrideMetaType]MOverrideReplacePair
var SetOverrideMap map[enums.OverrideMetaType]string

type MOverrideReplacePair struct {
	Value       string
	Replacement string
}

type CopyToField struct {
	Field string
	Dest  string
}

type PasteFromField struct {
	Field  string
	Origin string
}

type MetaAppend struct {
	Field  string
	Suffix string
}

type MetaPrefix struct {
	Field  string
	Prefix string
}

type MetaTrimPrefix struct {
	Field  string
	Prefix string
}

type MetaTrimSuffix struct {
	Field  string
	Suffix string
}

type MetaNewField struct {
	Field string
	Value string
}

type MetaDateTag struct {
	Loc    enums.MetaDateTagLocation
	Format enums.DateFormat
}

type MetaReplace struct {
	Field       string
	Value       string
	Replacement string
}

type FilenameDatePrefix struct {
	YearLength  int
	MonthLength int
	DayLength   int
	Order       enums.DateFormat
}

type FilenameReplaceSuffix struct {
	Suffix      string
	Replacement string
}

type MetadataCredits struct {
	Override  string `json:"-"`
	Actor     string `json:"actor" xml:"actor"`
	Author    string `json:"author" xml:"author"`
	Artist    string `json:"artist" xml:"artist"`
	Channel   string `json:"channel" xml:"channel"`
	Creator   string `json:"creator" xml:"creator"`
	Studio    string `json:"studio" xml:"studio"`
	Publisher string `json:"publisher" xml:"publisher"`
	Producer  string `json:"producer" xml:"producer"`
	Performer string `json:"performer" xml:"performer"`
	Uploader  string `json:"uploader" xml:"uploader"`
	Composer  string `json:"composer" xml:"composer"`
	Director  string `json:"director" xml:"director"`
	Writer    string `json:"writer" xml:"writer"`

	Actors     []string
	Artists    []string
	Studios    []string
	Publishers []string
	Producers  []string
	Performers []string
	Composers  []string
	Directors  []string
	Writers    []string
}

type MetadataTitlesDescs struct {
	Fulltitle        string `json:"fulltitle" xml:"title"`
	Title            string `json:"title" xml:"originaltitle"`
	Subtitle         string `json:"subtitle" xml:"subtitle"`
	Description      string `json:"description" xml:"description"`
	LongDescription  string `json:"longdescription" xml:"plot"`
	Long_Description string `json:"long_description" xml:"long_description"`
	Synopsis         string `json:"synopsis" xml:"synopsis"`
	Summary          string `json:"summary" xml:"summary"`
	Comment          string `json:"comment" xml:"comment"`
}

type MetadataDates struct {
	FormattedDate           string `json:"-" xml:"-"`
	UploadDate              string `json:"upload_date" xml:"upload_date"`
	ReleaseDate             string `json:"release_date" xml:"release_date"`
	Date                    string `json:"date" xml:"date"`
	Year                    string `json:"year" xml:"year"`
	Originally_Available_At string `json:"originally_available_at" xml:"originally_available_at"`
	Creation_Time           string `json:"creation_time" xml:"creation_time"`
	StringDate              string `json:"-"`
}

type MetadataWebData struct {
	WebpageURL string         `json:"webpage_url" xml:"webpage_url"`
	VideoURL   string         `json:"url" xml:"url"`
	Domain     string         `json:"webpage_url_domain" xml:"domain"`
	Referer    string         `json:"referer" xml:"referer"`
	Cookies    []*http.Cookie `json:"-" xml:"-"`
	TryURLs    []string       `json:"-"`
}

type MetadataShowData struct {
	Show          string `json:"show" xml:"show"`
	Episode_ID    string `json:"episode_id" xml:"episode_id"`
	Episode_Sort  string `json:"episode_sort" xml:"episode_sort"`
	Season_Number string `json:"season_number" xml:"season_number"`
	Season_Title  string `json:"season_title" xml:"seasontitle"`
}

type MetadataOtherData struct {
	Language string `json:"language" xml:"language"`
	Genre    string `json:"genre" xml:"genre"`
	HD_Video string `json:"hd_video" xml:"hd_video"`
}
package models

import "encoding/xml"

// NFOData represents the complete NFO file structure
type NFOData struct {
	XMLName     xml.Name    `xml:"movie"`
	Title       Title       `xml:"title"`
	Plot        string      `xml:"plot"`
	Description string      `xml:"description"`
	Actors      []Person    `xml:"cast>actor"`
	Directors   []string    `xml:"director"`
	Producers   []string    `xml:"producer"`
	Publishers  []string    `xml:"publisher"`
	Writers     []string    `xml:"writer"`
	Studios     []string    `xml:"studio"`
	Year        string      `xml:"year"`
	Premiered   string      `xml:"premiered"`
	ReleaseDate string      `xml:"releasedate"`
	ShowInfo    ShowInfo    `xml:"showinfo"`
	WebpageInfo WebpageInfo `xml:"web"`
}

// Title represents nested title information
type Title struct {
	Main      string `xml:"main"`
	Original  string `xml:"original"`
	Sort      string `xml:"sort"`
	Sub       string `xml:"sub"`
	PlainText string `xml:",chardata"` // For non-nested titles
}

// Person represents a credited person with optional role
type Person struct {
	Name  string `xml:"name"`
	Role  string `xml:"role"`
	Order int    `xml:"order"`
	Thumb string `xml:"thumb"`
}

// ShowInfo represents TV show specific information
type ShowInfo struct {
	Show         string `xml:"show"`
	SeasonNumber string `xml:"season>number"`
	EpisodeID    string `xml:"episode>number"`
	EpisodeTitle string `xml:"episode>title"`
}

// ShowInfo represents TV show specific information
type WebpageInfo struct {
	URL    string `xml:"url"`
	Fanart string `xml:"fanart"`
	Thumb  string `xml:"thumb"`
}
package models

import "regexp"

type ContractionPattern struct {
	Regexp      *regexp.Regexp
	Replacement string
}
package models

// SelectorRule holds rules for specific websites for use in scrapers
type SelectorRule struct {
	Selector string
	Attr     string // empty for text content, otherwise attribute name
	Process  func(string) string
	JSONPath []string
}

type CustomCookieSource struct {
	Browser string
	Dir     string
}
package parsing

import (
	"fmt"
	"metarr/internal/domain/templates"
	"metarr/internal/models"
	"path/filepath"
	"strings"
)

const (
	open          = "{{"
	close         = "}}"
	avgReplaceLen = 32
	templateLen   = len(open) + len(close) + 4
)

type Directory struct {
	FD *models.FileData
}

func NewDirectoryParser(fd *models.FileData) *Directory {
	return &Directory{
		FD: fd,
	}
}

// ParseDirectory returns the absolute directory path with template replacements.
func (dp *Directory) ParseDirectory(dir string) (parsedDir string, err error) {
	if dir == "" {
		return "", fmt.Errorf("directory sent in empty")
	}

	parsed := dir
	if strings.Contains(dir, open) {
		var err error

		parsed, err = dp.parseTemplate(dir)
		if err != nil {
			return "", fmt.Errorf("template parsing error: %w", err)
		}
	}

	abs, err := filepath.Abs(parsed)
	if err != nil {
		return dir, err
	}

	return filepath.Clean(abs), nil
}

// parseTemplate parses template options inside the directory string.
//
// Returns error if the desired data isn't present, to prevent unexpected results for the user.
func (dp *Directory) parseTemplate(dir string) (string, error) {
	opens := strings.Count(dir, open)
	closes := strings.Count(dir, close)
	if opens != closes {
		return "", fmt.Errorf("mismatched template delimiters: %d opens, %d closes", opens, closes)
	}

	var b strings.Builder
	b.Grow(len(dir) - (opens * templateLen) + (opens * avgReplaceLen)) // Approximate size
	remaining := dir

	for i := 0; i < opens; i++ {
		startIdx := strings.Index(remaining, open)
		if startIdx == -1 {
			return "", fmt.Errorf("missing opening delimiter")
		}

		endIdx := strings.Index(remaining, close)
		if endIdx == -1 {
			return "", fmt.Errorf("missing closing delimiter")
		}

		// String up to template open
		b.WriteString(remaining[:startIdx])

		// Replacement string
		tag := remaining[startIdx+len(open) : endIdx]
		replacement, err := dp.replace(strings.TrimSpace(tag))
		if err != nil {
			return "", err
		}
		b.WriteString(replacement)

		// String after template close
		remaining = remaining[endIdx+len(close):]
	}

	// Write any remaining text after last template
	b.WriteString(remaining)

	return b.String(), nil
}

// replace makes template replacements in the directory string.
func (dp *Directory) replace(tag string) (string, error) {

	if dp.FD == nil {
		return "", fmt.Errorf("null FileData model")
	}

	d := dp.FD.MDates
	c := dp.FD.MCredits
	w := dp.FD.MWebData

	switch strings.ToLower(tag) {
	case templates.Year:
		if d.Year != "" {
			return d.Year, nil
		}
		return "", fmt.Errorf("templating: year empty")

	case templates.Author:
		if c.Author != "" {
			return c.Author, nil
		}
		return "", fmt.Errorf("templating: author empty")

	case templates.Director:
		if c.Director != "" {
			return c.Director, nil
		}
		return "", fmt.Errorf("templating: director empty")

	case templates.Domain:
		if w.Domain != "" {
			return w.Domain, nil
		}
		return "", fmt.Errorf("templating: domain empty")

	default:
		return "", fmt.Errorf("invalid template tag %q", tag)
	}
}
package processing

import (
	"fmt"
	"metarr/internal/cfg"
	keys "metarr/internal/domain/keys"
	"metarr/internal/models"
	logging "metarr/internal/utils/logging"
	"os"
	"path/filepath"
	"strings"
)

var logInit bool

// StartBatchLoop begins processing the batch
func StartBatchLoop(core *models.Core) {
	if !cfg.IsSet(keys.BatchPairs) {
		logging.I("No batches sent in?")
		return
	}

	batches, ok := cfg.Get(keys.BatchPairs).([]models.Batch)
	if !ok {
		logging.E(0, "Wrong type or null batch pair. Type: %T", batches)
		return
	}

	job := 1

	// Begin iteration...
	for _, batch := range batches {
		var (
			openVideo *os.File
			openJson  *os.File
			err       error
		)

		logging.I("Starting batch job %d. Skip videos on this run? %v", job, batch.SkipVideos)
		skipVideos := cfg.GetBool(keys.SkipVideos) || batch.SkipVideos

		// Open video file if necessary
		if !skipVideos {
			openVideo, err = os.Open(batch.Video)
			if err != nil {
				logging.E(0, "Failed to open %s", batch.Video)
				continue
			}
		}

		// Open JSON file
		openJson, err = os.Open(batch.Json)
		if err != nil {
			logging.E(0, "Failed to open %s", batch.Json)

			if openVideo != nil {
				openVideo.Close()
			}

			continue
		}

		// Start logging
		if !logInit {
			dir, err := filepath.Abs(openJson.Name())
			if err != nil {
				logging.E(0, "Failed to initialize logging on this run, could not get absolute path of %v", openJson.Name())
			}
			dir = strings.TrimSuffix(dir, openJson.Name())
			logging.I("Setting log file at %q", dir)

			if err = logging.SetupLogging(dir); err != nil {
				fmt.Printf("\n\nNotice: Log file was not created\nReason: %s\n\n", err)
			}
			logInit = true
		}

		// Process the files
		ProcessFiles(batch, core, openVideo, openJson)
		logging.I("Finished tasks for video file/dir %q and JSON file/dir %q", batch.Video, batch.Json)

		// Close files explicitly at the end of each iteration
		if openVideo != nil {
			openVideo.Close()
		}
		openJson.Close()

		job++
	}

	logging.I("All batch tasks finished!")
}
package processing

import (
	"context"
	"fmt"
	"metarr/internal/cfg"
	"metarr/internal/domain/enums"
	"metarr/internal/domain/keys"
	"metarr/internal/ffmpeg"
	metaReader "metarr/internal/metadata/reader"
	"metarr/internal/models"
	fsRead "metarr/internal/utils/fs/read"
	"metarr/internal/utils/logging"
	"os"
	"sync"
	"sync/atomic"
)

const (
	typeVideo = "video"
	typeMeta  = "metadata"
)

var (
	totalMetaFiles,
	totalVideoFiles,
	processedMetaFiles,
	processedVideoFiles int32

	failedVideos []failedVideo
)

type failedVideo struct {
	filename string
	err      string
}

type workItem struct {
	filename string
	fileData *models.FileData
}

// processFiles is the main program function to process folder entries
func ProcessFiles(batch models.Batch, core *models.Core, openVideo, openMeta *os.File) {

	// Reset counts and get skip video bool
	skipVideos := prepNewBatch(batch.SkipVideos)

	// Match and video file maps, and meta file count
	matchedFiles, videoMap, metaCount := getFiles(batch, openMeta, openVideo, skipVideos)
	atomic.StoreInt32(&totalMetaFiles, int32(metaCount))
	atomic.StoreInt32(&totalVideoFiles, int32(len(videoMap)))

	logging.I("Found %d file(s) to process", totalMetaFiles+totalVideoFiles)
	logging.D(3, "Matched metafiles: %v", matchedFiles)

	var (
		muProcessed sync.Mutex
		muFailed    sync.Mutex
	)

	cancel := core.Cancel
	cleanupChan := core.Cleanup
	ctx := core.Ctx
	wg := core.Wg

	processMetadataFiles(ctx, matchedFiles, &muFailed)

	setupCleanup(cleanupChan, cancel, wg, videoMap, &muFailed)

	if !skipVideos {
		numWorkers := cfg.GetInt(keys.Concurrency)
		if numWorkers < 1 {
			numWorkers = 1
		}

		jobs := make(chan workItem, len(matchedFiles))
		results := make(chan *models.FileData, len(matchedFiles))
		processedModels := make([]*models.FileData, 0, len(matchedFiles))

		// Start workers
		for w := 1; w <= numWorkers; w++ {
			wg.Add(1)
			go workerProcess(w, jobs, results, wg, ctx)
		}

		// Send jobs to workers
		for name, data := range matchedFiles {
			jobs <- workItem{
				filename: name,
				fileData: data,
			}
		}
		close(jobs)

		// Collect results in a separate goroutine
		go func() {
			for result := range results {
				if result != nil {
					muProcessed.Lock()
					processedModels = append(processedModels, result)
					muProcessed.Unlock()
				}
			}
		}()

		wg.Wait()
		close(results)

		// Handle temp files and cleanup
		err := cleanupTempFiles(videoMap)
		if err != nil {
			logging.ErrorArray = append(logging.ErrorArray, err)
			logging.E(0, "Failed to cleanup temp files: %v", err)
		}

		directory := renameFiles(openVideo.Name(), openMeta.Name(), processedModels, skipVideos)

		if len(logging.ErrorArray) == 0 || logging.ErrorArray == nil {
			logging.S(0, "Successfully processed all files in directory %q with no errors.", directory)
			fmt.Println()
			return
		}

		if logging.ErrorArray != nil {
			logFailedVideos()
		}
	}
}

func workerProcess(id int, jobs <-chan workItem, results chan<- *models.FileData, wg *sync.WaitGroup, ctx context.Context) {
	defer wg.Done()

	for job := range jobs {
		select {
		case <-ctx.Done():
			logging.I("Worker %d stopping due to context cancellation", id)
			return
		default:
			logging.D(1, "Worker %d processing file: %s", id, job.filename)

			rtn, err := executeFile(ctx, job.filename, job.fileData)
			if err != nil {
				logging.E(0, "Worker %d error executing file %q: %v", id, job.filename, err)
				continue
			}

			results <- rtn
		}
	}
}

// processMetadataFiles processes metafiles such as .json, .nfo, and so on.
func processMetadataFiles(ctx context.Context, matchedFiles map[string]*models.FileData, muFailed *sync.Mutex) error {
	for _, fd := range matchedFiles {
		var err error
		switch fd.MetaFileType {
		case enums.METAFILE_JSON:
			logging.D(3, "File: %s: Meta file type in model as %v", fd.JSONFilePath, fd.MetaFileType)
			_, err = metaReader.ProcessJSONFile(ctx, fd)
		case enums.METAFILE_NFO:
			logging.D(3, "File: %s: Meta file type in model as %v", fd.NFOFilePath, fd.MetaFileType)
			_, err = metaReader.ProcessNFOFiles(fd)
		}

		if err != nil {
			logging.ErrorArray = append(logging.ErrorArray, err)
			errMsg := fmt.Errorf("error processing metadata for file %q: %w", fd.OriginalVideoPath, err)
			logging.E(0, errMsg.Error())

			muFailed.Lock()
			failedVideos = append(failedVideos, failedVideo{
				filename: fd.OriginalVideoPath,
				err:      errMsg.Error(),
			})
			muFailed.Unlock()
		}
	}
	return nil
}

// getFiles returns a map of matched video/metadata files.
func getFiles(batch models.Batch, openMeta, openVideo *os.File, skipVideos bool) (matched, videos map[string]*models.FileData, metaCount int) {
	var (
		videoMap,
		metaMap map[string]*models.FileData

		err error
	)

	// Batch is a directory request...
	if batch.IsDirs {
		metaMap, err = fsRead.GetMetadataFiles(openMeta)
		if err != nil {
			logging.E(0, err.Error())
			failedVideos = append(failedVideos, failedVideo{
				filename: openMeta.Name(),
				err:      err.Error(),
			})
		}

		if !skipVideos {
			videoMap, err = fsRead.GetVideoFiles(openVideo)
			if err != nil {
				failedVideos = append(failedVideos, failedVideo{
					filename: openVideo.Name(),
					err:      err.Error(),
				})
			}
		}
	}

	// Batch is a file request...
	if !batch.IsDirs {
		metaMap, err = fsRead.GetSingleMetadataFile(openMeta)
		if err != nil {
			logging.E(0, err.Error())
			failedVideos = append(failedVideos, failedVideo{
				filename: openMeta.Name(),
				err:      err.Error(),
			})
		}

		if !skipVideos {
			videoMap, err = fsRead.GetSingleVideoFile(openVideo)
			if err != nil {
				failedVideos = append(failedVideos, failedVideo{
					filename: openVideo.Name(),
					err:      err.Error(),
				})
			}
		}
	}

	var matchedFiles map[string]*models.FileData
	// Match video and metadata files
	if !skipVideos {
		matchedFiles, err = fsRead.MatchVideoWithMetadata(videoMap, metaMap, batch.IsDirs)
		if err != nil {
			logging.E(0, "Error matching videos with metadata: %v", err)
			os.Exit(1)
		}
	} else {
		matchedFiles = metaMap
	}
	return matchedFiles, videoMap, len(metaMap)
}

// processFile handles processing for both video and metadata files
func executeFile(ctx context.Context, filename string, fd *models.FileData) (*models.FileData, error) {

	// Check for context cancellation
	select {
	case <-ctx.Done():
		return nil, fmt.Errorf("did not process %q due to program cancellation", filename)
	default:
	}

	var muPrint sync.Mutex

	// Print progress for metadata
	currentMeta := atomic.AddInt32(&processedMetaFiles, 1)
	totalMeta := atomic.LoadInt32(&totalMetaFiles)
	printProgress(typeMeta, currentMeta, totalMeta, fd.JSONDirectory, &muPrint)

	// System resource check
	sysResourceLoop(filename)

	// Process file based on type
	skipVideos := cfg.GetBool(keys.SkipVideos)
	isVideoFile := fd.OriginalVideoPath != ""

	if isVideoFile {
		logging.I("Processing file: %s", filename)
		if !skipVideos {
			if err := ffmpeg.ExecuteVideo(ctx, fd); err != nil {
				errMsg := fmt.Errorf("failed to process video '%v': %w", filename, err)
				logging.ErrorArray = append(logging.ErrorArray, errMsg)
				logging.E(0, errMsg.Error())

				failedVideos = append(failedVideos, failedVideo{
					filename: filename,
					err:      errMsg.Error(),
				})
				return nil, errMsg
			}
			logging.S(0, "Successfully processed video %s", filename)
		}
	} else {
		logging.I("Processing metadata file: %s", filename)
		logging.S(0, "Successfully processed metadata for %s", filename)
	}

	// Print progress for video
	currentVideo := atomic.AddInt32(&processedVideoFiles, 1)
	totalVideo := atomic.LoadInt32(&totalVideoFiles)
	printProgress(typeVideo, currentVideo, totalVideo, fd.JSONDirectory, &muPrint)

	return fd, nil
}

// setupCleanup creates a cleanup routine for file processing.
func setupCleanup(cleanupChan chan os.Signal, cancel context.CancelFunc, wg *sync.WaitGroup, videoMap map[string]*models.FileData, muFailed *sync.Mutex) {
	go func() {
		<-cleanupChan
		fmt.Println("\nSignal received, cleaning up temporary files...")
		cancel()
		wg.Wait()

		if err := cleanupTempFiles(videoMap); err != nil {
			logging.E(0, "Failed to cleanup temp files: %v", err)
		}

		muFailed.Lock()
		logFailedVideos()
		muFailed.Unlock()

		os.Exit(0)
	}()
}
package processing

import (
	"fmt"
	"metarr/internal/cfg"
	"metarr/internal/domain/enums"
	keys "metarr/internal/domain/keys"
	"metarr/internal/models"
	"metarr/internal/transformations"
	logging "metarr/internal/utils/logging"
	"os"
	"path/filepath"
	"sync"
	"sync/atomic"
	"time"

	"github.com/shirou/gopsutil/cpu"
	"github.com/shirou/gopsutil/mem"
)

var (
	muResource sync.Mutex
)

// renameFiles performs renaming operations.
func renameFiles(videoPath, metaPath string, processed []*models.FileData, skipVideos bool) string {

	var (
		replaceStyle                           enums.ReplaceToStyle
		ok                                     bool
		inputVideoDir, inputJsonDir, directory string
	)

	if cfg.IsSet(keys.Rename) {
		if replaceStyle, ok = cfg.Get(keys.Rename).(enums.ReplaceToStyle); !ok {
			logging.E(0, "Received wrong type for rename style. Got %T", replaceStyle)
		} else {
			logging.D(2, "Got rename style as %T index %v", replaceStyle, replaceStyle)
		}
	}

	inputJsonDir = filepath.Dir(metaPath)
	if !skipVideos {
		inputVideoDir = filepath.Dir(videoPath)
	}

	switch {
	case inputJsonDir != "":
		directory = inputJsonDir
	case inputVideoDir != "":
		directory = inputVideoDir
	default:
		logging.E(0, "Not renaming file, no directory detected for this batch.")
		logging.ErrorArray = append(logging.ErrorArray, fmt.Errorf("not renaming files in batch, both input JSON and input video directories could not be discerned"))
		return ""
	}

	err := transformations.FileRename(processed, replaceStyle, skipVideos)
	if err != nil {
		logging.ErrorArray = append(logging.ErrorArray, err)
		logging.E(0, "Failed to rename files: %v", err)
	} else {
		logging.S(0, "Successfully formatted file names in directory: %s", directory)
	}
	return directory
}

// sysResourceLoop checks the system resources, controlling whether a new routine should be spawned
func sysResourceLoop(fileStr string) {
	var (
		resourceMsg bool
		backoff     = time.Second
		maxBackoff  = 10 * time.Second
	)

	memoryThreshold := cfg.GetUint64(keys.MinMemMB)

	for {
		// Fetch system resources and determine if processing can proceed
		muResource.Lock()
		proceed, availableMemory, CPUUsage, err := checkSysResources(memoryThreshold)
		muResource.Unlock()

		if err != nil {
			logging.ErrorArray = append(logging.ErrorArray, err)
			logging.E(0, "Error checking system resources: %v", err)

			time.Sleep(backoff)
			backoff *= 2
			if backoff > maxBackoff {
				backoff = maxBackoff
			}
			continue
		}

		if proceed {
			resourceMsg = false
			break
		}

		// Log resource info only once when insufficient resources are detected
		if !resourceMsg {
			logging.I("Not enough system resources to process %s, waiting...", fileStr)
			logging.D(1, "Memory available: %.2f MB\tCPU usage: %.2f%%\n", float64(availableMemory)/(1024*1024), CPUUsage)
			resourceMsg = true
		}

		time.Sleep(backoff)
		backoff *= 2
		if backoff > maxBackoff {
			backoff = maxBackoff
		}
	}
}

// checkAvailableMemory checks if enough memory is available (at least the threshold).
func checkSysResources(requiredMemory uint64) (proceed bool, availMem uint64, cpuUsagePct float64, err error) {
	vMem, err := mem.VirtualMemory()
	if err != nil {
		return false, 0, 0, err
	}

	cpuPct, err := cpu.Percent(0, false)
	if err != nil {
		return false, 0, 0, err
	}

	maxCpuUsage := cfg.GetFloat64(keys.MaxCPU)
	return (vMem.Available >= requiredMemory && cpuPct[0] <= maxCpuUsage), vMem.Available, cpuPct[0], nil
}

// cleanupTempFiles removes temporary files
func cleanupTempFiles(files map[string]*models.FileData) error {

	var (
		errReturn error
		path      string
	)

	for _, data := range files {
		path = data.TempOutputFilePath
		if _, err := os.Stat(path); err == nil {
			fmt.Printf("Removing temp file: %s\n", path)
			err = os.Remove(path)
			if err != nil {
				errReturn = fmt.Errorf("error removing temp file: %w", err)
			}
		}
	}
	return errReturn
}

// printProgress creates a printout of the current process completion status.
func printProgress(fileType string, current, total int32, directory string, muPrint *sync.Mutex) {
	muPrint.Lock()
	defer muPrint.Unlock()

	fmt.Printf("\n==============================================================\n")
	fmt.Printf("    Processed %s file %d of %d\n", fileType, current, total)
	fmt.Printf("    Remaining in %q: %d\n", directory, total-current)
	fmt.Printf("==============================================================\n\n")
}

// resetCounters resets the file counter per batch operation.
func prepNewBatch(modelSkipVideos bool) (skipVideos bool) {

	atomic.StoreInt32(&totalMetaFiles, 0)
	atomic.StoreInt32(&totalVideoFiles, 0)
	atomic.StoreInt32(&processedMetaFiles, 0)
	atomic.StoreInt32(&processedVideoFiles, 0)

	if cfg.IsSet(keys.SkipVideos) {
		skipVideos = cfg.GetBool(keys.SkipVideos)
	} else {
		skipVideos = modelSkipVideos
	}
	return skipVideos
}

// logFailedVideos logs videos which failed during this batch.
func logFailedVideos() {
	for i, failed := range failedVideos {
		if i == 0 {
			logging.E(0, "Program finished, but some errors were encountered:")
		}
		fmt.Println()
		logging.P("Filename: %v", failed.filename)
		logging.P("Error: %v", failed.err)
	}
	fmt.Println()
}
package transformations

import (
	"metarr/internal/cfg"
	keys "metarr/internal/domain/keys"
	"metarr/internal/models"
	logging "metarr/internal/utils/logging"
)

// CensoredTvTransformations adds preset transformations to
// files for censored.tv videos
func CensoredTvTransformations(fd *models.FileData) {

	logging.I("Making preset censored.tv meta replacements")

	censoredTvTrimSuffixes(fd)
	censoredTvFSuffixes(fd)
}

// censoredTvMSuffixes adds meta suffix replacements
func censoredTvTrimSuffixes(fd *models.FileData) {

	var (
		trimSfx []models.MetaTrimSuffix
		ok      bool
	)

	if cfg.IsSet(keys.MTrimSuffix) {
		trimSfx, ok = cfg.Get(keys.MTrimSuffix).([]models.MetaTrimSuffix)
		if !ok {
			logging.E(2, "Got type %T, may be null", trimSfx)
		}
	}

	var newSfx = make([]models.MetaTrimSuffix, 0, 4)

	newSfx = append(newSfx, models.MetaTrimSuffix{
		Field:  "title",
		Suffix: " (1)",
	}, models.MetaTrimSuffix{
		Field:  "fulltitle",
		Suffix: " (1)",
	}, models.MetaTrimSuffix{
		Field:  "id",
		Suffix: "-1",
	}, models.MetaTrimSuffix{
		Field:  "display_id",
		Suffix: "-1",
	})

	for _, newSuffix := range newSfx {
		exists := false
		for _, existingSuffix := range trimSfx {
			if existingSuffix.Field == newSuffix.Field {
				exists = true
				break
			}
		}
		if !exists {
			logging.I("Adding new censored.tv meta suffix replacement: %v", newSuffix)
			trimSfx = append(trimSfx, newSuffix)
		}
	}

	if logging.Level >= 2 {
		var entries []string
		for _, entry := range trimSfx {
			entries = append(entries, "("+entry.Field+":", entry.Suffix+")")
		}
		logging.I("After adding preset suffixes, suffixes to be trimmed for %q: %v", fd.OriginalVideoBaseName, entries)
	}

	fd.ModelMTrimSuffix = trimSfx
}

// censoredTvFSuffixes adds filename suffix replacements
func censoredTvFSuffixes(fd *models.FileData) {

	var sfx []models.FilenameReplaceSuffix

	v := fd.OriginalVideoBaseName

	if cfg.IsSet(keys.FilenameReplaceSfx) {
		existingSfx, ok := cfg.Get(keys.FilenameReplaceSfx).([]models.FilenameReplaceSuffix)
		if !ok {
			logging.E(2, "Unexpected type %T, initializing new suffix list.", existingSfx)
		} else {
			sfx = existingSfx
		}
	}

	logging.D(3, "Retrieved file name: %s", v)
	vExt := ""
	if len(v) > 1 {
		check := v[len(v)-2:]
		logging.D(3, "Got last element of file name: %s", check)
		switch check {
		case " 1", "_1":
			vExt = check
			logging.D(2, "Found file name suffix: %s", vExt)
		}
	}

	// Check if suffix is already present
	alreadyExists := false
	for _, existingSuffix := range sfx {
		if existingSuffix.Suffix == vExt && existingSuffix.Replacement == "" {
			alreadyExists = true
			break
		}
	}

	// Add suffix if it does not already exist
	if !alreadyExists {
		sfx = append(sfx, models.FilenameReplaceSuffix{
			Suffix:      "_1",
			Replacement: "",
		})
		logging.I("Added filename suffix replacement %q", vExt)
	}

	fd.ModelFileSfxReplace = sfx
	logging.I("Total filename suffix replacements: %d", len(sfx))
}
package transformations

import (
	"fmt"
	"metarr/internal/cfg"
	enums "metarr/internal/domain/enums"
	keys "metarr/internal/domain/keys"
	"metarr/internal/models"
	writefs "metarr/internal/utils/fs/write"
	logging "metarr/internal/utils/logging"
	validate "metarr/internal/utils/validation"
	"path/filepath"
	"strings"
	"sync"
)

// fileProcessor handles the renaming and moving of files
type fileProcessor struct {
	fd         *models.FileData
	style      enums.ReplaceToStyle
	skipVideos bool
}

// FileRename formats the file names
func FileRename(dataArray []*models.FileData, style enums.ReplaceToStyle, skipVideos bool) error {
	var wg sync.WaitGroup
	conc := cfg.GetInt(keys.Concurrency)

	sem := make(chan struct{}, conc)
	errChan := make(chan error, len(dataArray))

	for i := range dataArray {
		wg.Add(1)
		go func(fileData *models.FileData) {
			defer wg.Done()
			sem <- struct{}{}
			defer func() { <-sem }()

			fp := &fileProcessor{
				fd:         fileData,
				style:      style,
				skipVideos: skipVideos,
			}

			if err := fp.process(); err != nil {
				errChan <- fmt.Errorf("error processing %s: %w", fileData.OriginalVideoBaseName, err)
			}
		}(dataArray[i])
	}

	wg.Wait()
	close(errChan)

	var errors []error
	for err := range errChan {
		errors = append(errors, err)
	}

	if len(errors) > 0 {
		return fmt.Errorf("encountered %d errors during rename: %v", len(errors), errors)
	}

	return nil
}

// process handles the main file transformation processing logic.
func (fp *fileProcessor) process() error {

	rename, move := shouldRenameOrMove(fp.fd)

	if !rename && !move {
		logging.D(1, "Do not need to rename or move %q", fp.fd.FinalVideoPath)
		return nil
	}

	if !rename {
		logging.D(1, "Do not need to rename %q, just moving...", fp.fd.FinalVideoPath)
		if err := fp.writeResult(); err != nil {
			return err
		}

		return nil
	}

	// Handle renaming
	if err := fp.handleRenaming(); err != nil {
		return err
	}

	// Write changes and handle final operations
	logging.I("Writing final file transformations to filesystem...")
	if err := fp.writeResult(); err != nil {
		return err
	}

	return nil
}

// writeResult handles the purge and move operations.
func (fp *fileProcessor) writeResult() error {

	var (
		err         error
		deletedMeta bool
	)

	fsWriter, err := writefs.NewFSFileWriter(fp.fd, fp.skipVideos)
	if err != nil {
		return err
	}

	if err := fsWriter.WriteResults(); err != nil {
		return err
	}

	if cfg.IsSet(keys.MetaPurge) {
		if err, deletedMeta = fsWriter.DeleteMetafile(fp.fd.JSONFilePath); err != nil {
			return fmt.Errorf("failed to purge metafile: %v", err)
		}
	}

	if cfg.IsSet(keys.MoveOnComplete) {
		if err := fsWriter.MoveFile(deletedMeta); err != nil {
			return fmt.Errorf("failed to move to destination folder: %v", err)
		}
	}
	return nil
}

// handleRenaming processes the renaming operations.
func (fp *fileProcessor) handleRenaming() error {
	metaBase, metaDir, originalMPath := getMetafileData(fp.fd)
	videoBase := fp.fd.FinalVideoBaseName
	originalVPath := fp.fd.FinalVideoPath

	// Get ext
	vidExt := fp.determineVideoExtension(originalVPath)

	// Rename
	renamedVideo, renamedMeta := fp.processRenames(videoBase, metaBase)

	// Fix contractions
	var err error
	if renamedVideo, renamedMeta, err = fixContractions(renamedVideo, renamedMeta, fp.style); err != nil {
		return fmt.Errorf("failed to fix contractions for %s. error: %v", renamedVideo, err)
	}

	// Add tags and trim
	renamedVideo, renamedMeta = addTags(renamedVideo, renamedMeta, fp.fd, fp.style)
	renamedVideo = strings.TrimSpace(renamedVideo)
	renamedMeta = strings.TrimSpace(renamedMeta)

	logging.D(2, "Rename replacements:\nVideo: %v\nMetafile: %v", renamedVideo, renamedMeta)

	// Construct and validate final paths
	if err := fp.constructFinalPaths(renamedVideo, renamedMeta, vidExt, metaDir, filepath.Ext(originalMPath)); err != nil {
		return err
	}

	return nil
}

// determineVideoExtension gets the appropriate video extension.
func (fp *fileProcessor) determineVideoExtension(originalPath string) string {
	if !cfg.IsSet(keys.OutputFiletype) {
		return filepath.Ext(originalPath)
	}

	vidExt := validate.ValidateExtension(cfg.GetString(keys.OutputFiletype))
	if vidExt == "" {
		vidExt = filepath.Ext(originalPath)
	}
	return vidExt
}

// processRenames handles the renaming logic for both video and meta files.
func (fp *fileProcessor) processRenames(videoBase, metaBase string) (string, string) {
	var renamedVideo, renamedMeta string

	if !fp.skipVideos {
		renamedVideo = constructNewNames(videoBase, fp.style, fp.fd)
		renamedMeta = renamedVideo // Video name as meta base (if possible) for better consistency
		logging.D(2, "Renamed video to %q", renamedVideo)
	} else {
		renamedMeta = constructNewNames(metaBase, fp.style, fp.fd)
		logging.D(3, "Renamed meta now %q", renamedMeta)
	}

	return renamedVideo, renamedMeta
}

// constructFinalPaths creates and validates the final file paths.
func (fp *fileProcessor) constructFinalPaths(renamedVideo, renamedMeta, vidExt, metaDir, metaExt string) error {

	renamedVPath := filepath.Join(fp.fd.VideoDirectory, renamedVideo+vidExt)
	renamedMPath := filepath.Join(metaDir, renamedMeta+metaExt)

	logging.D(1, "Final paths with extensions:\nVideo: %s\nMeta: %s", renamedVPath, renamedMPath)

	var err error

	if filepath.IsAbs(renamedVPath) {
		fp.fd.RenamedVideoPath = renamedVPath
	} else {
		fp.fd.RenamedVideoPath, err = filepath.Abs(renamedVPath)
		if err != nil {
			return fmt.Errorf("failed to get absolute path for renamed video: %w", err)
		}
	}

	if filepath.IsAbs(renamedMPath) {
		fp.fd.RenamedMetaPath = renamedMPath
	} else {
		fp.fd.RenamedMetaPath, err = filepath.Abs(renamedMPath)
		if err != nil {
			return fmt.Errorf("failed to get absolute path for renamed meta: %w", err)
		}
	}

	// Handle final paths if they're set
	if fp.fd.FinalVideoPath != "" && !filepath.IsAbs(fp.fd.FinalVideoPath) {
		fp.fd.FinalVideoPath, err = filepath.Abs(fp.fd.FinalVideoPath)
		if err != nil {
			return fmt.Errorf("failed to get absolute path for final video: %w", err)
		}
	}

	if fp.fd.JSONFilePath != "" && !filepath.IsAbs(fp.fd.JSONFilePath) {
		fp.fd.JSONFilePath, err = filepath.Abs(fp.fd.JSONFilePath)
		if err != nil {
			return fmt.Errorf("failed to get absolute path for JSON file: %w", err)
		}
	}

	logging.D(1, "Saved into struct:\nVideo: %s\nMeta: %s", fp.fd.RenamedVideoPath, fp.fd.RenamedMetaPath)
	return nil
}

// constructNewNames constructs the new file names.
func constructNewNames(fileBase string, style enums.ReplaceToStyle, fd *models.FileData) string {
	logging.D(2, "Processing metafile base name: %q", fileBase)

	var (
		suffixes []models.FilenameReplaceSuffix
		ok       bool
	)

	if len(fd.ModelFileSfxReplace) > 0 {
		suffixes = fd.ModelFileSfxReplace
	} else if cfg.IsSet(keys.FilenameReplaceSfx) {
		suffixes, ok = cfg.Get(keys.FilenameReplaceSfx).([]models.FilenameReplaceSuffix)
		if !ok && len(fd.ModelFileSfxReplace) == 0 {
			logging.E(0, "Got wrong type %T for filename replace suffixes", suffixes)
			return fileBase
		}
	}

	if len(suffixes) == 0 && style == enums.RENAMING_SKIP {
		return fileBase
	} else if len(suffixes) > 0 {
		fileBase = replaceSuffix(fileBase, suffixes)
	}

	if style != enums.RENAMING_SKIP {
		fileBase = applyNamingStyle(style, fileBase)
	} else {
		logging.D(1, "No naming style selected, skipping rename style")
	}
	return fileBase
}
package transformations

import (
	"fmt"
	"metarr/internal/cfg"
	enums "metarr/internal/domain/enums"
	"metarr/internal/domain/keys"
	"metarr/internal/domain/regex"
	"metarr/internal/models"
	presets "metarr/internal/transformations/presets"
	utils "metarr/internal/utils/browser"
	logging "metarr/internal/utils/logging"
	"strings"
	"unicode"
)

// shouldRename determines if file rename operations are needed for this file
func shouldRenameOrMove(fd *models.FileData) (rename, move bool) {
	dateFmt := cfg.GetString(keys.FileDateFmt)
	rName := enums.RENAMING_SKIP

	var ok bool
	if cfg.IsSet(keys.Rename) {
		rName, ok = cfg.Get(keys.Rename).(enums.ReplaceToStyle)
		if !ok {
			logging.E(0, "Got wrong type or null rename. Got %T, want %q", rName, "enums.ReplaceToStyle")
		}
	}

	switch {
	case fd.FilenameMetaPrefix != "",
		len(fd.ModelFileSfxReplace) != 0,
		dateFmt != "",
		rName != enums.RENAMING_SKIP:

		logging.I("Flag detected that %q should be renamed\n\nFilename prefix: %q\nFile suffix replacements: %v\nFile date format: %q\nFile rename: %v",
			fd.OriginalVideoPath,
			fd.FilenameMetaPrefix,
			fd.ModelFileSfxReplace,
			dateFmt,
			rName != enums.RENAMING_SKIP)

		rename = true
	}

	if cfg.IsSet(keys.MoveOnComplete) {
		move = true
	}

	return rename, move
}

// TryTransPresets checks if any URLs in the video metadata have a known match.
// Applies preset transformations for those which match.
func TryTransPresets(urls []string, fd *models.FileData) (matches string) {

	for _, url := range urls {

		_, domain, _, _ := utils.ExtractDomainName(url)

		switch {
		case strings.Contains(domain, "censored.tv"):
			presets.CensoredTvTransformations(fd)
			logging.I("Found transformation preset for URL %q", url)
			return url
		default:
			// Not yet implemented
		}
	}
	return ""

}

// getMetafileData retrieves meta type specific data.
func getMetafileData(m *models.FileData) (metaBase, metaDir, metaPath string) {

	switch m.MetaFileType {
	case enums.METAFILE_JSON:
		return m.JSONBaseName, m.JSONDirectory, m.JSONFilePath
	case enums.METAFILE_NFO:
		return m.NFOBaseName, m.NFODirectory, m.NFOFilePath
	default:
		logging.E(0, "No metafile type set in model %v", m)
		return "", "", ""
	}
}

// applyNamingStyle applies renaming conventions.
func applyNamingStyle(style enums.ReplaceToStyle, input string) (output string) {

	switch style {
	case enums.RENAMING_SPACES:
		output = strings.ReplaceAll(input, "_", " ")
	case enums.RENAMING_UNDERSCORES:
		output = strings.ReplaceAll(input, " ", "_")
	default:
		logging.I("Skipping space or underscore renaming conventions...")
		output = input
	}
	return output
}

// addTags handles the tagging of the video files where necessary.
func addTags(renamedVideo, renamedMeta string, m *models.FileData, style enums.ReplaceToStyle) (renamedV, renamedM string) {

	if len(m.FilenameMetaPrefix) > 2 {
		renamedVideo = fmt.Sprintf("%s %s", m.FilenameMetaPrefix, renamedVideo)
		renamedMeta = fmt.Sprintf("%s %s", m.FilenameMetaPrefix, renamedMeta)
	}

	if len(m.FilenameDateTag) > 2 {
		renamedVideo = fmt.Sprintf("%s %s", m.FilenameDateTag, renamedVideo)
		renamedMeta = fmt.Sprintf("%s %s", m.FilenameDateTag, renamedMeta)
	}

	if style == enums.RENAMING_UNDERSCORES {
		renamedVideo = strings.ReplaceAll(renamedVideo, " ", "_")
		renamedMeta = strings.ReplaceAll(renamedMeta, " ", "_")
	}

	return renamedVideo, renamedMeta
}

// fixContractions fixes the contractions created by FFmpeg's restrict-filenames flag.
func fixContractions(videoBase, metaBase string, style enums.ReplaceToStyle) (renamedV, renamedM string, err error) {

	if videoBase == "" || metaBase == "" {
		return videoBase, metaBase, fmt.Errorf("empty input strings")
	}

	var contractionsMap map[string]models.ContractionPattern

	switch style {

	case enums.RENAMING_SPACES:
		contractionsMap = regex.ContractionMapSpacesCompile()

	case enums.RENAMING_UNDERSCORES:
		contractionsMap = regex.ContractionMapUnderscoresCompile()

	case enums.RENAMING_FIXES_ONLY:
		contractionsMap = regex.ContractionMapAllCompile()

	default:
		return videoBase, metaBase, nil
	}

	videoBase = replaceLoneS(videoBase, style)
	metaBase = replaceLoneS(metaBase, style)

	fmt.Printf("After replacement - Video: %s, Meta: %s\n", videoBase, metaBase)

	// Function to replace contractions in a filename
	replaceContractions := func(filename string) string {
		for _, replacement := range contractionsMap {
			repIdx := replacement.Regexp.FindStringIndex(strings.ToLower(filename))
			if repIdx == nil {
				continue
			}

			var b strings.Builder
			b.Grow(len(replacement.Replacement))
			originalContraction := filename[repIdx[0]:repIdx[1]]

			// Match original case for each character in the replacement
			for i, char := range replacement.Replacement {
				if i < len(originalContraction) && unicode.IsUpper(rune(originalContraction[i])) {
					b.WriteString(strings.ToUpper(string(char)))
				} else {
					b.WriteString(string(char))
				}
			}

			// Replace in filename with adjusted case
			filename = filename[:repIdx[0]] + b.String() + filename[repIdx[1]:]
			b.Reset()
		}

		logging.D(2, "Made contraction replacements for file %q", filename)
		return filename
	}

	// Replace contractions in both filenames
	videoBase = strings.TrimSpace(videoBase)
	metaBase = strings.TrimSpace(metaBase)
	return replaceContractions(videoBase),
		replaceContractions(metaBase),
		nil
}

// replaceSuffix applies configured suffix replacements to a filename.
func replaceSuffix(filename string, suffixes []models.FilenameReplaceSuffix) string {

	logging.D(2, "Received filename %s", filename)

	if len(suffixes) == 0 {
		logging.D(1, "No suffix replacements configured, keeping original filename: %q", filename)
		return filename
	}

	logging.D(2, "Processing filename %s with suffixes: %v", filename, suffixes)

	var result string
	for _, suffix := range suffixes {
		logging.D(2, "Checking suffix %q against filename %q", suffix.Suffix, filename)

		if strings.HasSuffix(filename, suffix.Suffix) {
			result = strings.TrimSuffix(filename, suffix.Suffix) + suffix.Replacement
			logging.D(2, "Applied suffix replacement: %q -> %q", suffix.Suffix, suffix.Replacement)
		}
	}

	if result != "" {
		logging.D(2, "Suffix replacement complete: %s -> %s", filename, result)
		return result
	}

	return filename
}

// replaceLoneS performs replacements without regex
func replaceLoneS(f string, style enums.ReplaceToStyle) string {
	if style == enums.RENAMING_SKIP {
		return f
	}

	prevString := ""

	// Keep replacing until no more changes occur
	// fixes accidental double spaces or double underscores
	// in the "s" contractions
	for f != prevString {
		prevString = f

		if style == enums.RENAMING_SPACES || style == enums.RENAMING_FIXES_ONLY {
			if strings.HasSuffix(f, " s") {
				f = fmt.Sprintf("%ss", f[:len(f)-2])
			}

			f = strings.ReplaceAll(f, " s ", "s ")
			f = strings.ReplaceAll(f, " s.", "s.")
			f = strings.ReplaceAll(f, " s[", "s[")
			f = strings.ReplaceAll(f, " s(", "s(")
			f = strings.ReplaceAll(f, " s)", "s)")
			f = strings.ReplaceAll(f, " s]", "s]")
			f = strings.ReplaceAll(f, " s-", "s-")
			f = strings.ReplaceAll(f, " s_", "s_")
			f = strings.ReplaceAll(f, " s,", "s,")
			f = strings.ReplaceAll(f, " s!", "s!")
			f = strings.ReplaceAll(f, " s'", "s'")
			f = strings.ReplaceAll(f, " s&", "s&")
			f = strings.ReplaceAll(f, " s=", "s=")
			f = strings.ReplaceAll(f, " s;", "s;")
			f = strings.ReplaceAll(f, " s#", "s#")
			f = strings.ReplaceAll(f, " s@", "s@")
			f = strings.ReplaceAll(f, " s$", "s$")
			f = strings.ReplaceAll(f, " s%", "s%")
			f = strings.ReplaceAll(f, " s+", "s+")
			f = strings.ReplaceAll(f, " s{", "s{")
			f = strings.ReplaceAll(f, " s}", "s}")
		}

		if style == enums.RENAMING_UNDERSCORES || style == enums.RENAMING_FIXES_ONLY {
			if strings.HasSuffix(f, "_s") {
				f = fmt.Sprintf("%ss", f[:len(f)-2])
			}

			f = strings.ReplaceAll(f, "_s_", "s_")
			f = strings.ReplaceAll(f, "_s.", "s.")
			f = strings.ReplaceAll(f, "_s[", "s[")
			f = strings.ReplaceAll(f, "_s(", "s(")
			f = strings.ReplaceAll(f, "_s)", "s)")
			f = strings.ReplaceAll(f, "_s]", "s]")
			f = strings.ReplaceAll(f, "_s-", "s-")
			f = strings.ReplaceAll(f, "_s ", "s ")
			f = strings.ReplaceAll(f, "_s,", "s,")
			f = strings.ReplaceAll(f, "_s!", "s!")
			f = strings.ReplaceAll(f, "_s'", "s'")
			f = strings.ReplaceAll(f, "_s&", "s&")
			f = strings.ReplaceAll(f, "_s=", "s=")
			f = strings.ReplaceAll(f, "_s;", "s;")
			f = strings.ReplaceAll(f, "_s#", "s#")
			f = strings.ReplaceAll(f, "_s@", "s@")
			f = strings.ReplaceAll(f, "_s$", "s$")
			f = strings.ReplaceAll(f, "_s%", "s%")
			f = strings.ReplaceAll(f, "_s+", "s+")
			f = strings.ReplaceAll(f, "_s{", "s{")
			f = strings.ReplaceAll(f, "_s}", "s}")
		}
	}
	return f
}
package utils

import (
	logging "metarr/internal/utils/logging"
	"net/url"
	"strings"

	"golang.org/x/net/publicsuffix"
)

// ExtractDomainName extracts various forms of a domain from a URL
func ExtractDomainName(u string) (withProtocol, noProtocol, withProtocolAndPort, noProtocolWithPort string) {
	const (
		https = "https://"
		http  = "http://"
	)

	var (
		proto string
		port  string
	)

	// Detect and remove protocol if present
	switch {
	case strings.HasPrefix(u, https):
		u = strings.TrimPrefix(u, https)
		proto = https
	case strings.HasPrefix(u, http):
		u = strings.TrimPrefix(u, http)
		proto = http
	}

	// Extract port if present and remove from main URL
	if colIdx := strings.Index(u, ":"); colIdx != -1 {
		portPart := u[colIdx:]
		port = strings.SplitN(portPart, "/", 2)[0]
		u = u[:colIdx]
	}

	// Prepare URL for parsing
	parseProto := proto
	if parseProto == "" {
		parseProto = https
	}

	// Parse the URL
	parsedURL, err := url.Parse(parseProto + u)
	if err != nil {
		return makeURLStrings(proto, u, port)
	}

	// Get the host and extract domain
	host := parsedURL.Hostname()
	domain, err := publicsuffix.EffectiveTLDPlusOne(host)
	if err != nil {
		return makeURLStrings(proto, u, port)
	}

	return makeURLStrings(proto, domain, port)
}

// Private /////////////////////////////////////////////

// makeURLStrings builds the URL strings using strings.Builder
func makeURLStrings(proto, domain, port string) (withProtocol, noProtocol, withProtocolAndPort, noProtocolWithPort string) {
	var b strings.Builder

	// Calculate maximum capacity needed
	maxLen := len(proto) + len(domain) + len(port)

	// Build withProtocol
	b.Grow(maxLen)
	b.WriteString(proto)
	b.WriteString(domain)
	withProtocol = b.String()

	// Build withProtocolAndPort
	b.WriteString(port)
	withProtocolAndPort = b.String()

	// Build noProtocol
	b.Reset()
	b.Grow(maxLen)
	b.WriteString(domain)
	noProtocol = b.String()

	// Build noProtocolWithPort
	b.WriteString(port)
	noProtocolWithPort = b.String()

	logging.D(1, "Made URL strings:\n\nWith protocol: %q\nNo protocol: %q\nProtocol + port: %q\nNo protocol + port: %q\n",
		withProtocol,
		noProtocol,
		withProtocolAndPort,
		noProtocolWithPort)

	return
}
package utils

// import (
// 	"metarr/internal/domain/consts"
// )

// // cookieFilePatterns holds the various filenames for cookie/auth related files by browser
// var cookieFilePatterns = map[string][]string{

// 	consts.BrowserFirefox: {
// 		// Core cookie and storage files
// 		"cookies.sqlite",
// 		"cookies.sqlite-shm",
// 		"cookies.sqlite-wal",
// 		"storage.sqlite",
// 		"storage.sqlite-shm",
// 		"storage.sqlite-wal",
// 		"webappsstore.sqlite",

// 		// Encryption and security
// 		"key*.db",
// 		"key*.db-journal",
// 		"cert9.db",
// 		"cert8.db",
// 		"pkcs11.txt",
// 		"secmod.db",
// 		"logins.json",
// 		"passwords.json",
// 		"signons.sqlite",

// 		// History and places
// 		"places.sqlite",
// 		"places.sqlite-shm",
// 		"places.sqlite-wal",
// 		"favicons.sqlite",
// 		"favicons.sqlite-shm",
// 		"favicons.sqlite-wal",

// 		// Session and preferences
// 		"prefs.js",
// 		"user.js",
// 		"sessionstore.js",
// 		"sessionstore.jsonlz4",
// 		"sessionCheckpoints.json",
// 		"permissions.sqlite",
// 		"extensions.json",
// 		"containers.json",
// 	},

// 	consts.BrowserChrome: {
// 		// Core files
// 		"Cookies",
// 		"Cookies-journal",
// 		"Login Data",
// 		"Login Data-journal",
// 		"Web Data",
// 		"Web Data-journal",

// 		// Local storage
// 		"Local Storage/*",
// 		"IndexedDB/*",

// 		// Preferences and sync
// 		"Preferences",
// 		"Secure Preferences",
// 		"Session Storage/*",

// 		// History
// 		"History",
// 		"History-journal",
// 		"Visited Links",

// 		// Authentication
// 		"Origin Bound Certs",
// 		"*.jwt", // Auth token
// 		"Network/*",
// 		"Extension Cookies",
// 		"Extension Cookies-journal",
// 	},

// 	consts.BrowserEdge: { // Similar to Chrome
// 		"Cookies",
// 		"Cookies-journal",
// 		"Login Data",
// 		"Login Data-journal",
// 		"Web Data",
// 		"Web Data-journal",
// 		"Local Storage/*",
// 		"IndexedDB/*",
// 		"Preferences",
// 		"Secure Preferences",
// 		"Session Storage/*",
// 		"History",
// 		"History-journal",
// 		"Network/*",
// 		"Extension Cookies",
// 		"Extension Cookies-journal",
// 		"*.jwt",
// 		"Microsoft Edge*", // Edge-specific files
// 	},

// 	consts.BrowserSafari: {
// 		"Cookies.binarycookies",
// 		"Cookies.plist",
// 		"LastSession.plist",
// 		"History.db",
// 		"History.db-shm",
// 		"History.db-wal",
// 		"Downloads.plist",
// 		"Extensions.plist",
// 		"Databases/*",
// 		"LocalStorage/*",
// 		"WebKit/WebsiteData/*",
// 		"Preferences.plist",
// 		"TopSites.plist",
// 		"ApplicationCache.db",
// 		"Cache.db",
// 		"PerSite/*",
// 		"SafeBrowsing/*",
// 	},
// }
package utils

import (
	"fmt"
	"metarr/internal/cfg"
	keys "metarr/internal/domain/keys"
	logging "metarr/internal/utils/logging"
	"net/http"
	"net/url"
	"strings"

	"github.com/browserutils/kooky"
	_ "github.com/browserutils/kooky/browser/all"
	"github.com/browserutils/kooky/browser/chrome"
	"github.com/browserutils/kooky/browser/firefox"
	"github.com/browserutils/kooky/browser/safari"
)

var (
	allStores  []kooky.CookieStore
	allCookies []*http.Cookie
)

// initializeCookies initializes all browser cookie stores
func initializeCookies() {
	allStores = kooky.FindAllCookieStores()
	allCookies = []*http.Cookie{}
}

// GetBrowserCookies retrieves cookies for a given URL, using a specified cookie file if provided.
func getBrowserCookies(u string) ([]*http.Cookie, error) {
	baseURL, err := extractBaseDomain(u)
	if err != nil {
		return nil, fmt.Errorf("failed to extract base domain: %v", err)
	}

	cookieFilePath := cfg.GetString(keys.CookiePath)

	// If a cookie file path is provided, use it
	if cookieFilePath != "" {
		logging.D(2, "Reading cookies from specified file: %s", cookieFilePath)
		kookyCookies, err := readCookieFile(cookieFilePath)
		if err != nil {
			return nil, fmt.Errorf("failed to read cookies from file: %v", err)
		}
		return convertToHTTPCookies(kookyCookies), nil
	}

	// Otherwise, proceed to use browser cookie stores
	if allStores == nil || allCookies == nil || len(allCookies) == 0 {
		initializeCookies()
	}

	attemptedBrowsers := make(map[string]bool, len(allStores))

	for _, store := range allStores {
		browserName := store.Browser()
		logging.D(2, "Attempting to read cookies from %s", browserName)
		attemptedBrowsers[browserName] = true

		cookies, err := store.ReadCookies(kooky.Valid, kooky.Domain(baseURL))
		if err != nil {
			logging.D(2, "Failed to read cookies from %s: %v", browserName, err)
			continue
		}

		if len(cookies) > 0 {
			logging.I("Successfully read %d cookies from %s for domain %s", len(cookies), browserName, baseURL)
			allCookies = append(allCookies, convertToHTTPCookies(cookies)...)
		} else {
			logging.D(2, "No cookies found for %s", browserName)
		}
	}

	// Log summary of attempted browsers
	logging.I("Attempted to read cookies from the following browsers: %v", keysFromMap(attemptedBrowsers))

	if len(allCookies) == 0 {
		logging.I("No cookies found for %q, proceeding without cookies", u)
	} else {
		logging.I("Found a total of %d cookies for %q", len(allCookies), u)
	}

	return allCookies, nil
}

// convertToHTTPCookies converts kooky cookies to http.Cookie format
func convertToHTTPCookies(kookyCookies []*kooky.Cookie) []*http.Cookie {
	httpCookies := make([]*http.Cookie, len(kookyCookies))
	for i, c := range kookyCookies {
		httpCookies[i] = &http.Cookie{
			Name:   c.Name,
			Value:  c.Value,
			Path:   c.Path,
			Domain: c.Domain,
			Secure: c.Secure,
		}
	}
	return httpCookies
}

// extractBaseDomain parses a URL and extracts its base domain
func extractBaseDomain(urlString string) (string, error) {
	parsedURL, err := url.Parse(urlString)
	if err != nil {
		return "", err
	}

	parts := strings.Split(parsedURL.Hostname(), ".")
	if len(parts) > 2 {
		return strings.Join(parts[len(parts)-2:], "."), nil
	}
	return parsedURL.Hostname(), nil
}

// keysForMap helper function to get keys from a map
func keysFromMap(m map[string]bool) []string {
	mapKeys := make([]string, 0, len(m))
	for k := range m {
		mapKeys = append(mapKeys, k)
	}
	return mapKeys
}

// readCookieFile reads cookies from the specified cookie file
func readCookieFile(cookieFilePath string) ([]*kooky.Cookie, error) {
	var store kooky.CookieStore
	var err error

	// Attempt to identify and read cookies based on known browser stores
	switch {
	case strings.Contains(cookieFilePath, "firefox") || strings.Contains(cookieFilePath, "cookies.sqlite"):
		store, err = firefox.CookieStore(cookieFilePath)

	case strings.Contains(cookieFilePath, "safari") || strings.Contains(cookieFilePath, "Cookies.binarycookies"):
		store, err = safari.CookieStore(cookieFilePath)

	case strings.Contains(cookieFilePath, "chrome") || strings.Contains(cookieFilePath, "Cookies"):
		store, err = chrome.CookieStore(cookieFilePath)

	default:
		return nil, fmt.Errorf("unsupported cookie file format")
	}

	if err != nil {
		return nil, fmt.Errorf("failed to create cookie store: %w", err)
	}

	// Read cookies from the store
	cookies, err := store.ReadCookies()
	if err != nil {
		return nil, fmt.Errorf("failed to read cookies: %w", err)
	}

	return cookies, nil
}

// newCustomCookieSource validates and retrieves a custom cookie source profile
// func newCustomCookieSource() (*models.CustomCookieSource, error) {

// 	if !cfg.IsSet(keys.CookiePath) {
// 		logging.D(2, "No custom cookie directory sent in")
// 		return nil, nil
// 	}

// 	cDir := cfg.GetString(keys.CookiePath)
// 	cDir = filepath.Clean(cDir)

// 	info, err := os.Stat(cDir)
// 	if err != nil {
// 		if os.IsNotExist(err) {
// 			return nil, fmt.Errorf("cookie directory does not exist: %w", err)
// 		}
// 		return nil, err
// 	}

// 	if !info.IsDir() {
// 		return nil, fmt.Errorf("cookie directory sent in as file, should be directory")
// 	}

// 	dirContents, err := os.ReadDir(cDir)
// 	if err != nil {
// 		return nil, err
// 	}

// 	var cSource models.CustomCookieSource
// 	foundFiles := make(map[string][]string, len(cookieFilePatterns))

// 	for browser, patterns := range cookieFilePatterns {
// 		for _, pattern := range patterns {
// 			// For each file in directory
// 			for _, dirFile := range dirContents {

// 				fileName := dirFile.Name()
// 				match, err := filepath.Match(pattern, fileName)
// 				if err != nil {
// 					logging.D(2, "Pattern matching error: %v", err)
// 					continue
// 				}

// 				if match {
// 					foundFiles[browser] = append(foundFiles[browser], fileName)
// 				}
// 			}
// 			if len(foundFiles[browser]) == len(cookieFilePatterns[browser]) {
// 				logging.S(0, "Got all required cookie and auth files for browser %s", browser)
// 				cSource.Browser = browser
// 				cSource.Dir = cDir
// 				break
// 			}
// 		}
// 	}

// 	if cSource.Browser == "" {
// 		for browser, files := range foundFiles {
// 			if len(files) > 0 {
// 				logging.D(2, "Found %d files for %s: %v", len(files), browser, files)

// 				cSource.Browser = browser
// 				cSource.Dir = cDir
// 			}
// 		}
// 	}

// 	return &cSource, nil
// }
package utils

import (
	enums "metarr/internal/domain/enums"
	"metarr/internal/models"
	logging "metarr/internal/utils/logging"
	"strconv"
	"strings"
	"time"
)

// BitchuteComRules holds rules for scraping bitchute.com
var BitchuteComRules = map[enums.WebClassTags][]models.SelectorRule{
	enums.WEBCLASS_CREDITS: {

		{Selector: "q-item__label ellipsis text-subtitle1 ellipsis", Process: strings.TrimSpace},
	},
	enums.WEBCLASS_DATE: {
		{
			Selector: "span[data-v-3c3cf957]",
			Attr:     "data-v-3c3cf957",
			Process:  BitchuteComParseDate,
		},
	},
	enums.WEBCLASS_DESCRIPTION: {

		{Selector: `meta[name="description"]`, Attr: "content", Process: strings.TrimSpace},
		{Selector: `meta[property="og:description"]`, Attr: "content", Process: strings.TrimSpace},
		{
			Selector: `meta[itemprop="name"]`,
			Attr:     "content",
			Process:  strings.TrimSpace,
		},
	},
	enums.WEBCLASS_TITLE: {
		{
			Selector: `meta[itemprop="name"]`,
			Attr:     "content",
			Process:  strings.TrimSpace,
		},
	},
}

// BitchuteComParseDate attempts to parse dates like "9 hours ago" (etc.)
func BitchuteComParseDate(date string) string {
	date = strings.TrimSpace(date)

	dateSplit := strings.Split(date, " ")

	var (
		unit  string
		digit int
		err   error
	)

	if len(dateSplit) >= 3 {
		digit, err = strconv.Atoi(dateSplit[0])
		if err != nil {
			logging.E(0, "Failed to convert string to digits: %v", err)
		}
		unit = strings.TrimSuffix(strings.ToLower(dateSplit[1]), "s") // handles both "hour" and "hours"

		var duration time.Duration
		now := time.Now()

		switch unit {
		case "second":
			duration = time.Duration(digit) * time.Second
			return now.Add(-duration).Format(time.RFC3339)
		case "minute":
			duration = time.Duration(digit) * time.Minute
			return now.Add(-duration).Format(time.RFC3339)
		case "hour":
			duration = time.Duration(digit) * time.Hour
			return now.Add(-duration).Format(time.RFC3339)
		case "day":
			duration = time.Duration(digit) * time.Hour * 24
			return now.Add(-duration).Format(time.RFC3339)
		case "week":
			duration = time.Duration(digit) * time.Hour * 24 * 7
			return now.Add(-duration).Format(time.RFC3339)
		case "month":
			return now.AddDate(0, -digit, 0).Format(time.RFC3339)
		case "year":
			return now.AddDate(-digit, 0, 0).Format(time.RFC3339)
		default:
			logging.E(0, "Unknown time unit: %s", unit)
			return ""
		}
	}
	logging.E(0, "Wrong date length passed in")
	return ""
}
package utils

import (
	enums "metarr/internal/domain/enums"
	"metarr/internal/models"
	logging "metarr/internal/utils/logging"
	"strings"

	"golang.org/x/text/cases"
	"golang.org/x/text/language"
)

var CensoredTvRules = map[enums.WebClassTags][]models.SelectorRule{
	enums.WEBCLASS_DATE: {
		{Selector: ".main-episode-player-container p.text-muted.text-right.text-date.mb-0", Process: strings.TrimSpace},
		{Selector: ".text-date", Process: strings.TrimSpace},
	},
	enums.WEBCLASS_DESCRIPTION: {
		{Selector: ".p-3 check-for-urls", Process: strings.TrimSpace},
		{Selector: `meta[name="description"]`, Attr: "content", Process: strings.TrimSpace},
	},
	enums.WEBCLASS_TITLE: {
		{Selector: ".p-3 h4", Process: strings.TrimSpace},
		{Selector: "[title]", Attr: "title", Process: strings.TrimSpace},
	},
}

// CensoredTvChannelName gets the channel name from the URL string
func CensoredTvChannelName(url string) string {
	if url == "" {
		logging.E(0, "url passed in empty")
		return ""
	}
	urlSplit := strings.Split(url, "/")

	var channel string
	for i, seg := range urlSplit {
		if strings.HasSuffix(seg, "shows") && len(urlSplit) > i+1 {
			channel = urlSplit[i+1]
		}
	}

	if channel == "" {
		logging.E(0, "failed to fill channel name from url, out of bounds?")
	}
	channel = strings.ReplaceAll(channel, "-", " ")

	caser := cases.Title(language.English)
	channel = caser.String(channel)

	if strings.EqualFold(channel, "atheism is unstoppable") {
		channel = "Atheism-is-Unstoppable"
	}
	return channel
}
package utils

import (
	enums "metarr/internal/domain/enums"
	"metarr/internal/models"
	"strings"
)

// OdyseeComRules holds rules for scraping odysee.com
var OdyseeComRules = map[enums.WebClassTags][]models.SelectorRule{
	enums.WEBCLASS_CREDITS: {
		{
			Selector: "script[type='application/ld+json']",
			JSONPath: []string{"author", "name"},
			Process:  strings.TrimSpace,
		},
	},
	enums.WEBCLASS_DATE: {
		{
			Selector: "script[type='application/ld+json']",
			JSONPath: []string{"uploadDate"},
			Process:  strings.TrimSpace,
		},
		{Selector: `meta[property="og:video:release_date"]`, Attr: "content", Process: strings.TrimSpace},
	},
	enums.WEBCLASS_DESCRIPTION: {
		{
			Selector: "script[type='application/ld+json']",
			JSONPath: []string{"description"},
			Process:  strings.TrimSpace,
		},
		{Selector: `meta[name="description"]`, Attr: "content", Process: strings.TrimSpace},
		{Selector: `meta[property="og:description"]`, Attr: "content", Process: strings.TrimSpace},
	},
	enums.WEBCLASS_TITLE: {

		{Selector: "title", Process: strings.TrimSpace},
		{
			Selector: "script[type='application/ld+json']",
			JSONPath: []string{"name"},
			Process:  strings.TrimSpace,
		},
	},
}
package utils

import (
	enums "metarr/internal/domain/enums"
	"metarr/internal/models"
	"strings"
)

// RumbleComRules holds rules for scraping rumble.com
var RumbleComRules = map[enums.WebClassTags][]models.SelectorRule{
	enums.WEBCLASS_CREDITS: {

		{Selector: ".media-subscribe-and-notify", Attr: "data-title", Process: strings.TrimSpace},
		{Selector: ".media-by--a .media-heading-name", Process: strings.TrimSpace},
	},
	enums.WEBCLASS_DATE: {
		{Selector: "time", Attr: "datetime", Process: strings.TrimSpace},
		{
			Selector: "script[type='application/ld+json']",
			JSONPath: []string{"uploadDate"},
			Process:  strings.TrimSpace,
		},
	},
	enums.WEBCLASS_DESCRIPTION: {
		{
			Selector: "script[type='application/ld+json']",
			JSONPath: []string{"description"},
			Process:  strings.TrimSpace,
		},
		{Selector: `meta[name="description"]`, Attr: "content", Process: strings.TrimSpace},
		{Selector: `meta[property="og:description"]`, Attr: "content", Process: strings.TrimSpace},
	},
	enums.WEBCLASS_TITLE: {

		{Selector: "title", Process: strings.TrimSpace},
		{
			Selector: "script[type='application/ld+json']",
			JSONPath: []string{"name"},
			Process:  strings.TrimSpace,
		},
	},
}
package utils

import (
	"encoding/json"
	"fmt"
	"metarr/internal/domain/consts"
	"metarr/internal/domain/enums"
	"metarr/internal/models"
	presets "metarr/internal/utils/browser/presets"
	"metarr/internal/utils/logging"
	"net/http"
	"regexp"
	"strings"
	"time"

	"github.com/gocolly/colly"
)

const (
	maxRetries = 3
	retryDelay = 5 * time.Second
)

// scrapeMeta gets cookies for a given URL and returns a grabbed string
func ScrapeMeta(w *models.MetadataWebData, find enums.WebClassTags) string {

	var (
		err  error
		data string
	)

	w.Cookies, err = getBrowserCookies(w.WebpageURL)
	if err != nil {
		logging.E(2, "Was unable to grab browser cookies: %v", err)
	}
	for _, try := range w.TryURLs {
		data, err = scrape(try, w.Cookies, find, false)
		if err != nil {
			logging.E(0, "Failed to scrape %q for requested metadata: %v", try, err)
		} else {
			break
		}
	}
	return data
}

// scrape handles scrape attempts.
func scrape(url string, cookies []*http.Cookie, tag enums.WebClassTags, skipPresets bool) (string, error) {
	var lastErr error

	for attempt := 1; attempt <= maxRetries; attempt++ {
		result, err := attemptScrape(url, cookies, tag, skipPresets)
		if err == nil {
			return result, nil
		}

		lastErr = err
		logging.E(0, "Scrape attempt %d/%d failed for %s: %v",
			attempt, maxRetries, url, err)

		if attempt < maxRetries {
			logging.I("Waiting %v before retry...", retryDelay)
			time.Sleep(retryDelay)
		}
	}

	return "", fmt.Errorf("all %d scrape attempts failed for %s: %w",
		maxRetries, url, lastErr)
}

// attemptScrape searches relevant URLs to try and fill missing metadata.
func attemptScrape(url string, cookies []*http.Cookie, tag enums.WebClassTags, skipPresets bool) (string, error) {

	var (
		result      string
		scrapeError error
		custom      bool
	)

	// Initialize the collector
	c := colly.NewCollector(
		colly.AllowURLRevisit(),
		colly.MaxDepth(1),
		colly.Async(true),
	)
	c.SetRequestTimeout(15 * time.Second)

	if len(cookies) > 0 {
		c.SetCookies(url, cookies)
	}

	// Define preset scraping rules if the URL matches a known pattern
	switch {
	case strings.Contains(url, "bitchute.com") && !skipPresets:

		custom = true
		logging.I("Using bitchute.com preset scraper")
		setupPresetScraping(c, tag, presets.BitchuteComRules, &result, url)

	case strings.Contains(url, "censored.tv") && !skipPresets:

		custom = true
		logging.I("Using censored.tv preset scraper")
		if tag == enums.WEBCLASS_CREDITS {
			return presets.CensoredTvChannelName(url), nil
		}
		setupPresetScraping(c, tag, presets.CensoredTvRules, &result, url)

	case strings.Contains(url, "rumble.com") && !skipPresets:

		custom = true
		logging.I("Using rumble.com preset scraper")
		setupPresetScraping(c, tag, presets.RumbleComRules, &result, url)

	case strings.Contains(url, "odysee.com") && !skipPresets:

		custom = true
		logging.I("Using odysee.com preset scraper")
		setupPresetScraping(c, tag, presets.OdyseeComRules, &result, url)

	default:
		logging.I("Generic scrape attempt...")
		setupGenericScraping(c, tag, &result, url)
	}

	// Error handler
	c.OnError(func(r *colly.Response, err error) {
		scrapeError = fmt.Errorf("failed to scrape %s: %v", r.Request.URL, err)
	})

	// Attempt visit and wait for async scraping
	if err := c.Visit(url); err != nil {
		return "", fmt.Errorf("unable to visit given web page")
	}
	c.Wait()

	if scrapeError != nil {
		switch result {
		case "":
			return "", scrapeError
		default:
			logging.E(0, "Error during scrape (%v) but got result anyway. Returning result %q...", scrapeError, result)
			return result, nil
		}
	}

	// If custom preset was used and failed, try again with default
	if result == "" && custom {
		return scrape(url, cookies, tag, true)
	}

	return result, nil
}

// setupPresetScraping applies specific scraping rules for known sites
func setupPresetScraping(c *colly.Collector, tag enums.WebClassTags, rules map[enums.WebClassTags][]models.SelectorRule, result *string, url string) {
	if result == nil {
		return
	}
	if ruleSet, exists := rules[tag]; exists {
		for _, rule := range ruleSet {
			c.OnHTML(rule.Selector, func(h *colly.HTMLElement) {
				if *result != "" {
					return
				}

				var value string
				switch {
				case len(rule.JSONPath) > 0:
					if jsonVal, err := jsonExtractor([]byte(h.Text), rule.JSONPath); err == nil {
						value = jsonVal
					}

				case rule.Attr != "":
					value = h.Attr(rule.Attr)

				default:
					value = h.Text
				}

				if value != "" {
					logging.S(0, "Grabbed value %q for URL %q using preset scraper", value, url)
					*result = rule.Process(value)
				}
			})
		}
	}
}

// setupGenericScraping defines a generic scraping approach for non-preset sites
func setupGenericScraping(c *colly.Collector, tag enums.WebClassTags, result *string, url string) {
	if result == nil {
		return
	}

	var tags []string

	// Determine the appropriate tags based on the metadata being fetched
	switch tag {
	case enums.WEBCLASS_DATE:
		tags = consts.WebDateTags[:]
	case enums.WEBCLASS_DESCRIPTION:
		tags = consts.WebDescriptionTags[:]
	case enums.WEBCLASS_CREDITS:
		tags = consts.WebCreditsTags[:]
	case enums.WEBCLASS_TITLE:
		tags = consts.WebTitleTags[:]
	default:
		return
	}

	// Set up the HTML scraper for each tag
	c.OnHTML("*", func(e *colly.HTMLElement) {
		if *result != "" {
			return
		}

		classAttr := strings.ToLower(e.Attr("class"))
		idAttr := strings.ToLower(e.Attr("id"))
		text := strings.TrimSpace(e.Text)

		if classAttr != "" {
			logging.D(2, "Checking element with class: %q", classAttr)
		}

		for _, t := range tags {
			if (e.Name == "p" && strings.Contains(idAttr, t)) ||
				strings.Contains(classAttr, t) ||
				strings.Contains(idAttr, t) {

				if tag == enums.WEBCLASS_DATE && !looksLikeDate(text) {
					continue
				}

				*result = text
				logging.I("Found %q in element with class %q and id %q for URL %q",
					*result, classAttr, idAttr, url)
				return
			}
		}
	})
}

// jsonExtractor helps extract values from nested JSON structures
func jsonExtractor(data []byte, path []string) (string, error) {
	var result map[string]any
	if err := json.Unmarshal(data, &result); err != nil {
		return "", err
	}
	current := result
	for _, key := range path[:len(path)-1] {
		if next, ok := current[key].(map[string]any); ok {
			current = next
		} else {
			return "", fmt.Errorf("invalid JSON path at %s", key)
		}
	}
	if val, ok := current[path[len(path)-1]].(string); ok {
		return val, nil
	}
	return "", fmt.Errorf("value at path is not a string")
}

// looksLikeDate validates if the text appears to be a date
func looksLikeDate(text string) bool {
	text = strings.TrimSpace(strings.ToLower(text))

	// Common date patterns
	datePatterns := []string{
		`\d{4}-\d{2}-\d{2}`,       // YYYY-MM-DD
		`\d{1,2}/\d{1,2}/\d{2,4}`, // M/D/YY or MM/DD/YYYY
		`(?i)(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\s+\d{1,2},?\s+\d{4}`, // Month DD, YYYY
	}

	for _, pattern := range datePatterns {
		matched, err := regexp.MatchString(pattern, text)
		if err == nil && matched {
			return true
		}
	}

	// Additional date indicators
	dateIndicators := []string{"uploaded", "published", "created", "date:", "on"}
	for _, indicator := range dateIndicators {
		if strings.Contains(text, indicator) {
			return true
		}
	}

	return false
}
package utils

import (
	"fmt"
	"io"
	"metarr/internal/domain/consts"
	"metarr/internal/utils/logging"
	"os"
	"path/filepath"
	"strings"
	"sync"
)

var (
	muBackup sync.Mutex
)

// createBackup creates a backup copy of the original file before modifying it.
func BackupFile(file *os.File) error {

	originalFilePath := file.Name()

	backupFilePath := generateBackupFilename(originalFilePath)
	logging.D(3, "Creating backup of file %q as %q", originalFilePath, backupFilePath)

	// Current position
	currentPos, err := file.Seek(0, io.SeekCurrent)
	if err != nil {
		return fmt.Errorf("failed to get current file position: %w", err)
	}
	defer func() {
		file.Seek(currentPos, io.SeekStart)
	}()

	muBackup.Lock()
	defer muBackup.Unlock()

	// Seek to start for backup
	if _, err := file.Seek(0, io.SeekStart); err != nil {
		return fmt.Errorf("failed to seek to beginning of original file: %w", err)
	}

	// Open the backup file for writing
	backupFile, err := os.Create(backupFilePath)
	if err != nil {
		return fmt.Errorf("failed to create backup file: %w", err)
	}
	defer backupFile.Close()

	// Copy the content of the original file to the backup file
	buf := make([]byte, 4*1024*1024)
	_, err = io.CopyBuffer(backupFile, file, buf)
	if err != nil {
		return fmt.Errorf("failed to copy content to backup file: %w", err)
	}

	logging.D(3, "Backup successfully created at %q", backupFilePath)
	return nil
}

// generateBackupFilename creates a backup filename by appending "_backup" to the original filename
func generateBackupFilename(originalFilePath string) string {
	ext := filepath.Ext(originalFilePath)
	base := strings.TrimSuffix(originalFilePath, ext)
	return fmt.Sprintf(base + consts.BackupTag + ext)
}

// RenameToBackup renames the passed in file
func RenameToBackup(filename string) (backupName string, err error) {

	if filename == "" {
		logging.E(0, "filename was passed in to backup empty")
	}

	backupName = generateBackupFilename(filename)

	if err := os.Rename(filename, backupName); err != nil {
		return "", fmt.Errorf("failed to backup filename %q to %q", filename, backupName)
	}
	return backupName, nil
}
package utils

import (
	"fmt"
	"metarr/internal/cfg"
	"metarr/internal/domain/consts"
	"metarr/internal/domain/enums"
	"metarr/internal/domain/keys"
	"metarr/internal/domain/regex"
	"metarr/internal/models"
	"metarr/internal/utils/logging"
	"os"
	"path/filepath"
	"strings"
)

// Variable cache
var (
	inputPrefixes []string

	videoExtensions,
	metaExtensions map[string]bool
)

// InitFetchFilesVars sets up the cached variables to be used in file fetching ops.
func InitFetchFilesVars() (err error) {

	// Handle video extension input
	inVExts, ok := cfg.Get(keys.InputVExtsEnum).([]enums.ConvertFromFiletype)
	if !ok {
		return fmt.Errorf("wrong type sent in. Received type %T", inVExts)
	}

	if videoExtensions, err = setVideoExtensions(inVExts); err != nil {
		return err
	}

	// Handle meta extension input
	inMExts, ok := cfg.Get(keys.InputMExtsEnum).([]enums.MetaFiletypeFilter)
	if !ok {
		return fmt.Errorf("wrong type sent in. Received type %T", inMExts)
	}

	if metaExtensions, err = setMetaExtensions(inMExts); err != nil {
		return err
	}

	// Set prefix filter
	inputPrefixes = SetPrefixFilter(cfg.GetStringSlice(keys.FilePrefixes))
	logging.D(2, "Setting prefix filter: %v", inputPrefixes)

	return nil
}

// GetVideoFiles fetches video files from a directory.
func GetVideoFiles(videoDir *os.File) (map[string]*models.FileData, error) {
	files, err := videoDir.ReadDir(-1)
	if err != nil {
		return nil, fmt.Errorf("error reading video directory: %w", err)
	}

	logging.P("\n\nFiltering directory %q:\n\nFile extensions: %v\nFile prefixes: %v\n\n", videoDir.Name(), videoExtensions, inputPrefixes)

	videoFiles := make(map[string]*models.FileData, len(files))

	for _, file := range files {

		if cfg.IsSet(keys.FilePrefixes) {
			if !HasPrefix(file.Name(), inputPrefixes) {
				continue
			}
		}

		if !file.IsDir() && HasFileExtension(file.Name(), videoExtensions) {

			filenameBase := filepath.Base(file.Name())

			m := models.NewFileData()
			m.OriginalVideoPath = filepath.Join(videoDir.Name(), file.Name())
			m.OriginalVideoBaseName = strings.TrimSuffix(filenameBase, filepath.Ext(file.Name()))
			m.VideoDirectory = videoDir.Name()

			if !strings.HasSuffix(m.OriginalVideoBaseName, consts.BackupTag) {
				videoFiles[file.Name()] = m
				logging.I("Added video to queue: %v", filenameBase)
			} else {
				logging.I("Skipping file %q containing backup tag (%q)", m.OriginalVideoBaseName, consts.BackupTag)
			}
		}
	}

	if len(videoFiles) == 0 {
		return nil, fmt.Errorf("no video files with extensions: %v and prefixes: %v found in directory: %s", videoExtensions, inputPrefixes, videoDir.Name())
	}
	return videoFiles, nil
}

// GetMetadataFiles fetches metadata files from a directory.
func GetMetadataFiles(metaDir *os.File) (map[string]*models.FileData, error) {
	files, err := metaDir.ReadDir(-1)
	if err != nil {
		return nil, fmt.Errorf("error reading metadata directory: %w", err)
	}

	metaFiles := make(map[string]*models.FileData, len(files))

	for _, file := range files {
		if file.IsDir() {
			continue
		}

		ext := filepath.Ext(file.Name())
		logging.D(3, "Checking file %q with extension %q", file.Name(), ext)

		if cfg.IsSet(keys.FilePrefixes) {
			if !HasPrefix(file.Name(), inputPrefixes) {
				continue
			}
		}

		var match bool
		for range metaExtensions {
			if !metaExtensions[ext] {
				continue
			}
			match = true
			break
		}
		if !match {
			continue
		}

		filenameBase := filepath.Base(file.Name())
		baseName := strings.TrimSuffix(filenameBase, ext)

		m := models.NewFileData()
		filePath := filepath.Join(metaDir.Name(), file.Name())

		switch ext {
		case consts.MExtJSON:

			logging.D(1, "Detected JSON file %q", file.Name())
			m.JSONFilePath = filePath
			m.JSONBaseName = baseName
			m.JSONDirectory = metaDir.Name()
			m.MetaFileType = enums.METAFILE_JSON

		case consts.MExtNFO:

			logging.D(1, "Detected NFO file %q", file.Name())
			m.NFOFilePath = filePath
			m.NFOBaseName = baseName
			m.NFODirectory = metaDir.Name()
			m.MetaFileType = enums.METAFILE_NFO
		}

		if !strings.Contains(baseName, consts.BackupTag) {
			metaFiles[file.Name()] = m
		} else {
			logging.I("Skipping file %q containing backup tag (%q)", baseName, consts.BackupTag)
		}
	}

	if len(metaFiles) == 0 {
		return nil, fmt.Errorf("no meta files with extensions: %v and prefixes: %v found in directory: %s", metaExtensions, inputPrefixes, metaDir.Name())
	}

	logging.D(3, "Returning meta files %v", metaFiles)
	return metaFiles, nil
}

// GetSingleVideoFile handles a single video file.
func GetSingleVideoFile(videoFile *os.File) (map[string]*models.FileData, error) {
	videoMap := make(map[string]*models.FileData, 1)
	filename := filepath.Base(videoFile.Name())

	videoData := models.NewFileData()
	videoData.OriginalVideoPath = videoFile.Name()
	videoData.OriginalVideoBaseName = strings.TrimSuffix(filename, filepath.Ext(filename))
	videoData.VideoDirectory = filepath.Dir(videoFile.Name())
	videoData.VideoFile = videoFile

	logging.D(3, "Created video file data for single file: %s", filename)

	videoMap[filename] = videoData
	return videoMap, nil
}

// GetSingleMetadataFile handles a single metadata file.
func GetSingleMetadataFile(metaFile *os.File) (map[string]*models.FileData, error) {
	metaMap := make(map[string]*models.FileData, 1)
	filename := filepath.Base(metaFile.Name())

	fileData := models.NewFileData()
	ext := filepath.Ext(metaFile.Name())

	switch ext {
	case consts.MExtJSON:

		fileData.MetaFileType = enums.METAFILE_JSON
		fileData.JSONFilePath = metaFile.Name()
		fileData.JSONBaseName = strings.TrimSuffix(filename, ext)
		fileData.JSONDirectory = filepath.Dir(metaFile.Name())
		logging.D(3, "Created JSON metadata file data for single file: %s", filename)

	case consts.MExtNFO:

		fileData.MetaFileType = enums.METAFILE_NFO
		fileData.NFOFilePath = metaFile.Name()
		fileData.NFOBaseName = strings.TrimSuffix(filename, ext)
		fileData.NFODirectory = filepath.Dir(metaFile.Name())
		logging.D(3, "Created NFO metadata file data for single file: %s", filename)

	default:
		return nil, fmt.Errorf("unsupported metadata file type: %s", ext)
	}

	metaMap[filename] = fileData
	return metaMap, nil
}

// MatchVideoWithMetadata matches video files with their corresponding metadata files
func MatchVideoWithMetadata(videoFiles, metaFiles map[string]*models.FileData, isDirs bool) (map[string]*models.FileData, error) {
	logging.D(3, "Entering metadata and video file matching loop...")

	matchedFiles := make(map[string]*models.FileData, len(videoFiles))

	specialChars := regex.SpecialCharsCompile()
	extraSpaces := regex.ExtraSpacesCompile()

	// Pre-process metaFiles into a lookup map
	metaLookup := make(map[string]*models.FileData, len(metaFiles))
	for metaName, metaData := range metaFiles {
		baseKey := NormalizeFilename(TrimMetafileSuffixes(metaName, ""), specialChars, extraSpaces)
		metaLookup[baseKey] = metaData
	}

	for filename := range videoFiles {
		videoBase := strings.TrimSuffix(filename, filepath.Ext(filename))
		normalizedVideoBase := NormalizeFilename(videoBase, specialChars, extraSpaces)

		if metaData, exists := metaLookup[normalizedVideoBase]; exists { // This checks if the key exists in the metaLookup map
			matchedFiles[filename] = videoFiles[filename]
			matchedFiles[filename].MetaFileType = metaData.MetaFileType

			switch metaData.MetaFileType {
			case enums.METAFILE_JSON:
				matchedFiles[filename].JSONFilePath = metaData.JSONFilePath
				matchedFiles[filename].JSONBaseName = metaData.JSONBaseName
				matchedFiles[filename].JSONDirectory = metaData.JSONDirectory

			case enums.METAFILE_NFO:
				matchedFiles[filename].NFOFilePath = metaData.NFOFilePath
				matchedFiles[filename].NFOBaseName = metaData.NFOBaseName
				matchedFiles[filename].NFODirectory = metaData.NFODirectory
			}
		}
	}

	if len(matchedFiles) == 0 {
		return nil, fmt.Errorf("no matching metadata files found for any videos")
	}

	return matchedFiles, nil
}
package utils

import (
	"fmt"
	"metarr/internal/domain/consts"
	"metarr/internal/domain/enums"
	"metarr/internal/utils/logging"
	"os"
	"path/filepath"
	"regexp"
	"strings"
)

// hasVideoExtension checks if the file has a valid video extension
func HasFileExtension(filename string, extensions map[string]bool) bool {
	if extensions == nil {
		logging.E(0, "Extensions sent in nil.")
		return false
	}

	ext := strings.ToLower(filepath.Ext(filename))
	if ext == "" {
		return false
	}

	if _, exists := extensions[ext]; exists {
		logging.I("File %q has valid extension %q, processing...", filename, ext)
		return true
	}
	logging.D(3, "File %q does not appear to have an extension contained in the extensions map", filename)
	return false
}

// hasPrefix determines if the input file has the desired prefix
func HasPrefix(fileName string, prefixes []string) bool {

	if prefixes == nil {
		prefixes = append(prefixes, "")
	}

	for _, data := range prefixes {
		if strings.HasPrefix(strings.ToLower(fileName), strings.ToLower(data)) {
			return true
		}
	}
	return false
}

// setVideoExtensions creates a list of extensions to filter
func setVideoExtensions(exts []enums.ConvertFromFiletype) (map[string]bool, error) {

	videoExtensions := make(map[string]bool, len(consts.AllVidExtensions))

	for _, arg := range exts {
		switch arg {
		case enums.VID_EXTS_MKV:
			videoExtensions[consts.ExtMKV] = true

		case enums.VID_EXTS_MP4:
			videoExtensions[consts.ExtMP4] = true

		case enums.VID_EXTS_WEBM:
			videoExtensions[consts.ExtWEBM] = true

		case enums.VID_EXTS_ALL:
			for key := range consts.AllVidExtensions {
				videoExtensions[key] = true
			}
		}
	}

	if len(videoExtensions) == 0 {
		return nil, fmt.Errorf("failed to set video extensions")
	}

	return videoExtensions, nil
}

// setMetaExtensions creates a lists of meta extensions to filter
func setMetaExtensions(exts []enums.MetaFiletypeFilter) (map[string]bool, error) {

	metaExtensions := make(map[string]bool, len(consts.AllMetaExtensions))

	for _, arg := range exts {
		switch arg {
		case enums.META_EXTS_JSON:
			metaExtensions[consts.MExtJSON] = true

		case enums.META_EXTS_NFO:
			metaExtensions[consts.MExtNFO] = true

		case enums.META_EXTS_ALL:
			for key := range consts.AllMetaExtensions {
				metaExtensions[key] = true
			}
		}
	}

	if len(metaExtensions) == 0 {
		return nil, fmt.Errorf("failed to set meta extensions")
	}

	return metaExtensions, nil
}

// setPrefixFilter sets a list of prefixes to filter
func SetPrefixFilter(inputPrefixFilters []string) []string {

	prefixFilters := make([]string, 0, len(inputPrefixFilters))
	prefixFilters = append(prefixFilters, inputPrefixFilters...)

	return prefixFilters
}

// GetDirStats returns the number of video or metadata files in a directory, so maps/slices can be suitable sized
func GetDirStats(dir string) (vidCount, metaCount int) {

	// Quick initial scan just counting files, not storing anything
	entries, err := os.ReadDir(dir)
	if err != nil {
		return 0, 0
	}
	for _, entry := range entries {
		if !entry.IsDir() {
			ext := strings.ToLower(filepath.Ext(entry.Name()))

			for key := range consts.AllVidExtensions {
				if ext == key {
					vidCount++
					continue
				}
				switch ext {
				case consts.MExtJSON, consts.MExtNFO:
					metaCount++
					continue
				}
			}
		}
	}
	return vidCount, metaCount
}

// normalizeFilename removes special characters and normalizes spacing
func NormalizeFilename(filename string, specialChars, extraSpaces *regexp.Regexp) string {

	normalized := strings.ToLower(filename)
	normalized = specialChars.ReplaceAllString(normalized, "")
	normalized = extraSpaces.ReplaceAllString(normalized, " ")
	normalized = strings.TrimSpace(normalized)

	return normalized
}

// trimJsonSuffixes normalizes away common json string suffixes
// e.g. ".info" for yt-dlp outputted JSON files
func TrimMetafileSuffixes(metaBase, videoBase string) string {

	switch {

	case strings.HasSuffix(metaBase, ".info.json"): // FFmpeg
		if !strings.HasSuffix(videoBase, ".info") {
			metaBase = strings.TrimSuffix(metaBase, ".info.json")
		} else {
			metaBase = strings.TrimSuffix(metaBase, consts.MExtJSON)
		}

	case strings.HasSuffix(metaBase, ".metadata.json"): // Angular
		if !strings.HasSuffix(videoBase, ".metadata") {
			metaBase = strings.TrimSuffix(metaBase, ".metadata.json")
		} else {
			metaBase = strings.TrimSuffix(metaBase, consts.MExtJSON)
		}

	case strings.HasSuffix(metaBase, ".model.json"):
		if !strings.HasSuffix(videoBase, ".model") {
			metaBase = strings.TrimSuffix(metaBase, ".model.json")
		} else {
			metaBase = strings.TrimSuffix(metaBase, consts.MExtJSON)
		}

	case strings.HasSuffix(metaBase, ".manifest.cdfd.json"):
		if !strings.HasSuffix(videoBase, ".manifest.cdm") {
			metaBase = strings.TrimSuffix(metaBase, ".manifest.cdfd.json")
		} else {
			metaBase = strings.TrimSuffix(metaBase, consts.MExtJSON)
		}

	default:
		switch {
		case !strings.HasSuffix(videoBase, consts.MExtJSON): // Edge cases where metafile extension is in the suffix of the video file
			metaBase = strings.TrimSuffix(metaBase, consts.MExtJSON)

		case !strings.HasSuffix(videoBase, consts.MExtNFO):
			metaBase = strings.TrimSuffix(metaBase, consts.MExtNFO)

		default:
			logging.D(1, "Common suffix not found for metafile (%s)", metaBase)
		}
	}
	return metaBase
}
package utils

import (
	"fmt"
	"metarr/internal/cfg"
	"metarr/internal/domain/consts"
	"metarr/internal/domain/enums"
	"metarr/internal/domain/keys"
	"metarr/internal/models"
	"metarr/internal/parsing"
	"metarr/internal/utils/logging"
	"os"
	"path/filepath"
	"strings"
	"sync"
)

type FSFileWriter struct {
	Fd           *models.FileData
	SkipVids     bool
	RenamedVideo string
	InputVideo   string
	RenamedMeta  string
	InputMeta    string
	muFs         sync.RWMutex
}

func NewFSFileWriter(fd *models.FileData, skipVids bool) (*FSFileWriter, error) {

	inputVid := fd.FinalVideoPath
	renamedVid := fd.RenamedVideoPath
	inputMeta := fd.JSONFilePath
	renamedMeta := fd.RenamedMetaPath

	if !skipVids {
		if inputVid == "" && renamedVid == "" {
			return nil, fmt.Errorf("some required video paths are empty:\n\nVid src: %q\nVid dest: %q", inputVid, renamedVid)
		}
	}

	if inputMeta == "" && renamedMeta == "" {
		return nil, fmt.Errorf("some required meta paths are empty:\n\nMeta src: %q\nMeta dest: %q", inputMeta, renamedMeta)
	}

	if logging.Level > 1 {
		differ := 0
		if !strings.EqualFold(renamedVid, inputVid) {
			differ++
		}
		if !strings.EqualFold(renamedMeta, inputMeta) {
			differ++
		}

		logging.D(2, "Made FSFileWriter with parameters:\n\nSkip videos? %v\n\nOriginal Video: %s\nRenamed Video:  %s\n\nOriginal Metafile: %s\nRenamed Metafile:  %s\n\n%d file names will be changed...\n\n",
			skipVids, inputVid, renamedVid, inputMeta, renamedMeta, differ)
	}

	return &FSFileWriter{
		Fd:           fd,
		SkipVids:     skipVids,
		RenamedVideo: renamedVid,
		InputVideo:   inputVid,
		RenamedMeta:  renamedMeta,
		InputMeta:    inputMeta,
	}, nil
}

// WriteResults executes the final commands to write the transformed files
func (fs *FSFileWriter) WriteResults() error {
	fs.muFs.Lock()
	defer fs.muFs.Unlock()

	// Rename video file
	if shouldProcess(fs.InputVideo, fs.RenamedVideo, true, fs.SkipVids) {
		if err := os.Rename(fs.InputVideo, fs.RenamedVideo); err != nil {
			return fmt.Errorf("failed to rename %s to %s. error: %v", fs.InputVideo, fs.RenamedVideo, err)
		}
		logging.S(0, "Successfully renamed %q to %q", fs.InputVideo, fs.RenamedVideo)
	}

	// Rename meta file
	if shouldProcess(fs.InputMeta, fs.RenamedMeta, false, fs.SkipVids) {
		if err := os.Rename(fs.InputMeta, fs.RenamedMeta); err != nil {
			return fmt.Errorf("failed to rename %s to %s. error: %v", fs.InputMeta, fs.RenamedMeta, err)
		}
		logging.S(0, "Successfully renamed %q to %q", fs.InputMeta, fs.RenamedMeta)
	}

	return nil
}

// MoveFile moves files to specified location
func (fs *FSFileWriter) MoveFile(noMeta bool) error {
	fs.muFs.Lock()
	defer fs.muFs.Unlock()

	if !cfg.IsSet(keys.MoveOnComplete) {
		return nil
	}

	if fs.RenamedVideo == "" && fs.RenamedMeta == "" {
		return fmt.Errorf("video and metafile source strings both empty")
	}

	dstIn := cfg.GetString(keys.MoveOnComplete)

	prs := parsing.NewDirectoryParser(fs.Fd)
	dst, err := prs.ParseDirectory(dstIn)
	if err != nil {
		return err
	}

	if _, err := os.Stat(dst); os.IsNotExist(err) {
		if err := os.MkdirAll(dst, 0o755); err != nil {
			return fmt.Errorf("failed to create or find destination directory: %w", err)
		}
	}

	// Move/copy video and metadata file
	if !fs.SkipVids {
		if fs.RenamedVideo != "" {
			videoDestPath := filepath.Join(dst, filepath.Base(fs.RenamedVideo))
			if err := moveOrCopyFile(fs.RenamedVideo, videoDestPath); err != nil {
				return fmt.Errorf("failed to move video file from %q to %q: %w", fs.RenamedVideo, videoDestPath, err)
			}
			logging.S(0, "Moved %q to %q", fs.RenamedVideo, videoDestPath)
		}
	}

	if !noMeta {
		if fs.RenamedMeta != "" {
			metaDestPath := filepath.Join(dst, filepath.Base(fs.RenamedMeta))
			if err := moveOrCopyFile(fs.RenamedMeta, metaDestPath); err != nil {
				return fmt.Errorf("failed to move metadata file from %q to %q: %w", fs.RenamedMeta, metaDestPath, err)
			}
			logging.S(0, "Moved %q to %q", fs.RenamedMeta, metaDestPath)
		}
	}
	return nil
}

// DeleteJSON safely removes JSON metadata files once file operations are complete
func (fs *FSFileWriter) DeleteMetafile(file string) (error, bool) {

	if !cfg.IsSet(keys.MetaPurgeEnum) {
		return fmt.Errorf("meta purge enum not set"), false
	}

	e, ok := cfg.Get(keys.MetaPurgeEnum).(enums.PurgeMetafiles)
	if !ok {
		return fmt.Errorf("wrong type for purge metafile enum. Got %T", e), false
	}

	ext := filepath.Ext(file)
	ext = strings.ToLower(ext)

	switch e {
	case enums.PURGEMETA_ALL:
		break

	case enums.PURGEMETA_JSON:
		if ext != consts.MExtJSON {
			logging.D(3, "Skipping deletion of metafile %q as extension does not match user selection", file)
			return nil, false
		}

	case enums.PURGEMETA_NFO:
		if ext != consts.MExtNFO {
			logging.D(3, "Skipping deletion of metafile %q as extension does not match user selection", file)
			return nil, false
		}

	case enums.PURGEMETA_NONE:
		return fmt.Errorf("user selected to skip purging metadata, this should be inaccessible. Exiting function"), false

	default:
		return fmt.Errorf("support not added for this metafile purge enum yet, exiting function"), false
	}

	fileInfo, err := os.Stat(file)
	if err != nil {
		return err, false
	}

	if fileInfo.IsDir() {
		return fmt.Errorf("metafile %q is a directory, not a file", file), false
	}

	if !fileInfo.Mode().IsRegular() {
		return fmt.Errorf("metafile %q is not a regular file", file), false
	}

	if err := os.Remove(file); err != nil {
		return fmt.Errorf("unable to delete meta file: %w", err), false
	}

	logging.S(0, "Successfully deleted metafile. Bye bye %q!", file)

	return nil, true
}
package utils

import (
	"bufio"
	"bytes"
	"crypto/sha256"
	"fmt"
	"io"
	logging "metarr/internal/utils/logging"
	"os"
	"path/filepath"
	"strings"
)

// moveOrCopyFile attempts rename first, falls back to copy+delete for cross-device moves
func moveOrCopyFile(src, dst string) error {
	src = filepath.Clean(src)
	dst = filepath.Clean(dst)

	if src == dst {
		return nil // Same file, nothing to do
	}

	srcHash, err := calculateFileHash(src)
	if err != nil {
		return fmt.Errorf("failed to calculate initial source hash: %w", err)
	}

	// Try rename (pure move) first
	err = os.Rename(src, dst)
	if err == nil {
		dstHash, verifyErr := calculateFileHash(dst)
		if verifyErr != nil {
			return fmt.Errorf("move verification failed: %w", verifyErr)
		}
		if !bytes.Equal(srcHash, dstHash) {
			return fmt.Errorf("hash mismatch after move")
		}
		return nil
	}

	logging.S(0, "Moved file from %q to %q", src, dst)

	// If cross-device error, fall back to copy+delete
	if strings.Contains(err.Error(), "invalid cross-device link") {
		logging.D(1, "Falling back to copy for moving %q to %q", src, dst)

		// Copy the file
		if err := copyFile(src, dst); err != nil {
			os.Remove(dst)
			return fmt.Errorf("failed to copy file: %w", err)
		}

		// Verify copy with hash comparison
		dstHash, verifyErr := calculateFileHash(dst)
		if verifyErr != nil {
			os.Remove(dst)
			return fmt.Errorf("copy verification failed: %w", verifyErr)
		}
		if !bytes.Equal(srcHash, dstHash) {
			os.Remove(dst)
			return fmt.Errorf("hash mismatch after copy")
		}

		// Remove source after successful copy and verification
		if err := os.Remove(src); err != nil {
			logging.E(0, "Failed to remove source file after verified copy: %v", err)
			// Operation successful, do not return error, just log the error
		}
		return nil
	}
	return fmt.Errorf("failed to move file: %w", err)
}

// copyFile copies a file to a target destination
func copyFile(src, dst string) error {
	src = filepath.Clean(src)
	dst = filepath.Clean(dst)

	if src == dst {
		return fmt.Errorf("entered source file %q and destination %q file as the same name and same path", src, dst)
	}

	logging.I("Copying:\n%q\nto\n%q...", src, dst)

	// Validate source file
	sourceInfo, err := os.Stat(src)
	if err != nil {
		return fmt.Errorf("failed to stat source file: %w", err)
	}
	if !sourceInfo.Mode().IsRegular() {
		return fmt.Errorf("source is not a regular file: %s", src)
	}
	if sourceInfo.Size() == 0 {
		return fmt.Errorf("source file is empty: %s", src)
	}

	// Check destination
	if destInfo, err := os.Stat(dst); err == nil {
		if os.SameFile(sourceInfo, destInfo) {
			return nil // Same file
		}
		return fmt.Errorf("aborting move, destination file %q is equal to source file %q", dst, src)
	} else if !os.IsNotExist(err) {
		return fmt.Errorf("error checking destination file: %w", err)
	}

	// Ensure destination directory exists
	if _, err := os.Stat(dst); os.IsNotExist(err) {
		if err := os.MkdirAll(dst, 0o755); err != nil {
			return fmt.Errorf("failed to create or find destination directory: %w", err)
		}
	}

	// Open source file
	sourceFile, err := os.Open(src)
	if err != nil {
		return fmt.Errorf("failed to open source file: %w", err)
	}
	defer sourceFile.Close()

	// Create destination file
	destFile, err := os.Create(dst)
	if err != nil {
		return fmt.Errorf("failed to create destination file, do you have adequate permissions on the destination folder?: %w", err)
	}
	defer func() {
		destFile.Close()
		if err != nil {
			os.Remove(dst) // Clean up on error
		}
	}()

	// Copy contents with buffer
	bufferedSource := bufio.NewReaderSize(sourceFile, 4*1024*1024) // 4MB: 1024 * 1024 is 1 MB
	bufferedDest := bufio.NewWriterSize(destFile, 4*1024*1024)
	defer bufferedDest.Flush()

	buf := make([]byte, 4*1024*1024)

	if _, err = io.CopyBuffer(bufferedDest, bufferedSource, buf); err != nil {
		return fmt.Errorf("failed to copy file contents: %w", err)
	}

	// Sync to ensure write is complete
	if err = destFile.Sync(); err != nil {
		return fmt.Errorf("failed to sync destination file: %w", err)
	}

	// Set same permissions as source
	if err = os.Chmod(dst, sourceInfo.Mode()); err != nil {
		logging.I("Failed to set file permissions, is destination folder remote? (%v)", err)
	}

	// Verify destination file
	check, err := destFile.Stat()
	if err != nil {
		return fmt.Errorf("error statting destination file: %w", err)
	}
	if check.Size() != sourceInfo.Size() {
		return fmt.Errorf("destination file size (%d) does not match source size (%d)",
			check.Size(), sourceInfo.Size())
	}
	return nil
}

// shouldProcess determines if the file move/rename should be processed
func shouldProcess(src, dst string, isVid, skipVids bool) bool {
	switch {
	case skipVids && isVid:
		logging.I("Not processing video files. Skip vids is %v", skipVids)
		return false

	case strings.EqualFold(src, dst):
		logging.I("Not processing files. Source and destination match: Src: %v, Dest %v", src, dst)
		return false

	case src == "", dst == "":
		logging.I("Not processing files. Source or destination path empty: Src: %v, Dest %v", src, dst)
		return false

	default:
		logging.I("Processing file operations for %q", src)
		return true
	}
}

// calculateFileHash computes SHA-256 hash of a file
func calculateFileHash(fpath string) ([]byte, error) {
	file, err := os.Open(fpath)
	if err != nil {
		return nil, fmt.Errorf("failed to open file for hashing: %w", err)
	}
	defer file.Close()

	hash := sha256.New()
	buf := make([]byte, 4*1024*1024) // 4MB buffer
	reader := bufio.NewReaderSize(file, 4*1024*1024)

	for {
		n, err := reader.Read(buf)
		if n > 0 {
			if _, err := hash.Write(buf[:n]); err != nil {
				return nil, fmt.Errorf("error writing to hash: %w", err)
			}
		}
		if err == io.EOF {
			break
		}
		if err != nil {
			return nil, fmt.Errorf("error reading file for hash: %w", err)
		}
	}

	return hash.Sum(nil), nil
}
package logging

import (
	"fmt"
	"metarr/internal/domain/consts"
	"path/filepath"
	"runtime"
	"strconv"
	"strings"
	"time"
)

const (
	tagBaseLen = 15 + // 01/02 15:04:05(space)
		1 + // "["
		len(consts.ColorBlue) +
		10 + // "Function: "
		len(consts.ColorReset) +
		3 + // " - "
		len(consts.ColorBlue) +
		6 + // "File: "
		len(consts.ColorReset) +
		3 + // " : "
		len(consts.ColorBlue) +
		6 + // "Line: "
		len(consts.ColorReset) +
		2 // "]\n"
)

var (
	Level int = -1 // Pre initialization
)

// Log Error:
//
// Print and log a message of the error type.
func E(l int, format string, args ...any) string {
	if Level < l {
		return ""
	}

	pc, file, line, _ := runtime.Caller(1)
	file = filepath.Base(file)
	funcName := filepath.Base(runtime.FuncForPC(pc).Name())

	var b strings.Builder
	b.Grow(len(consts.RedError) + tagBaseLen + len(format) + (len(args) * 32) + len(funcName) + len(file) + line)

	b.WriteString(time.Now().Format("01/02 15:04:05"))
	b.WriteRune(' ')
	b.WriteString(consts.RedError)

	// Write formatted message
	if len(args) != 0 && args != nil {
		fmt.Fprintf(&b, format, args...)
	} else {
		b.WriteString(format)
	}

	b.WriteRune('[')
	b.WriteString(consts.ColorBlue)
	b.WriteString("Function: ")
	b.WriteString(consts.ColorReset)
	b.WriteString(funcName)
	b.WriteString(" - ")
	b.WriteString(consts.ColorBlue)
	b.WriteString("File: ")
	b.WriteString(consts.ColorReset)
	b.WriteString(file)
	b.WriteString(" : ")
	b.WriteString(consts.ColorBlue)
	b.WriteString("Line: ")
	b.WriteString(consts.ColorReset)
	b.WriteString(strconv.Itoa(line))
	b.WriteString("]\n")

	msg := b.String()

	fmt.Print(msg)
	writeLog(msg, l)

	return msg
}

// Log Success:
//
// Print and log a message of the success type.
func S(l int, format string, args ...any) string {
	if Level < l {
		return ""
	}

	var b strings.Builder
	b.Grow(len(consts.GreenSuccess) + len(format) + len(consts.ColorReset) + 1 + (len(args) * 32))

	b.WriteString(time.Now().Format("01/02 15:04:05"))
	b.WriteRune(' ')
	b.WriteString(consts.GreenSuccess)

	// Write formatted message
	if len(args) != 0 && args != nil {
		fmt.Fprintf(&b, format, args...)
	} else {
		b.WriteString(format)
	}

	b.WriteRune('\n')
	msg := b.String()
	fmt.Print(msg)
	writeLog(msg, l)

	return msg
}

// Log Debug:
//
// Print and log a message of the debug type.
func D(l int, format string, args ...any) string {
	if Level < l {
		return ""
	}

	pc, file, line, _ := runtime.Caller(1)
	file = filepath.Base(file)
	funcName := filepath.Base(runtime.FuncForPC(pc).Name())

	var b strings.Builder
	b.Grow(len(consts.YellowDebug) + tagBaseLen + len(format) + (len(args) * 32) + len(funcName) + len(file) + line)

	b.WriteString(time.Now().Format("01/02 15:04:05"))
	b.WriteRune(' ')
	b.WriteString(consts.YellowDebug)

	// Write formatted message
	if len(args) != 0 && args != nil {
		fmt.Fprintf(&b, format, args...)
	} else {
		b.WriteString(format)
	}

	b.WriteRune('[')
	b.WriteString(consts.ColorBlue)
	b.WriteString("Function: ")
	b.WriteString(consts.ColorReset)
	b.WriteString(funcName)
	b.WriteString(" - ")
	b.WriteString(consts.ColorBlue)
	b.WriteString("File: ")
	b.WriteString(consts.ColorReset)
	b.WriteString(file)
	b.WriteString(" : ")
	b.WriteString(consts.ColorBlue)
	b.WriteString("Line: ")
	b.WriteString(consts.ColorReset)
	b.WriteString(strconv.Itoa(line))
	b.WriteString("]\n")

	msg := b.String()

	fmt.Print(msg)
	writeLog(msg, l)

	return msg
}

// Log Info:
//
// Print and log a message of the info type.
func I(format string, args ...any) string {

	var b strings.Builder
	b.Grow(len(consts.BlueInfo) + len(format) + len(consts.ColorReset) + 1 + (len(args) * 32))

	b.WriteString(time.Now().Format("01/02 15:04:05"))
	b.WriteRune(' ')
	b.WriteString(consts.BlueInfo)

	// Write formatted message
	if len(args) != 0 && args != nil {
		fmt.Fprintf(&b, format, args...)
	} else {
		b.WriteString(format)
	}

	b.WriteRune('\n')
	msg := b.String()
	fmt.Print(msg)
	writeLog(msg, 0)

	return msg
}

// Log:
//
// Print and log a plain message.
func P(format string, args ...any) string {

	var b strings.Builder
	b.Grow(len(format) + 1 + (len(args) * 32))

	b.WriteString(time.Now().Format("01/02 15:04:05"))
	b.WriteRune(' ')

	// Write formatted message
	if len(args) != 0 && args != nil {
		fmt.Fprintf(&b, format, args...)
	} else {
		b.WriteString(format)
	}

	b.WriteRune('\n')
	msg := b.String()
	fmt.Print(msg)
	writeLog(msg, 0)

	return msg
}
package logging

import (
	"log"
	"metarr/internal/domain/regex"
	"path/filepath"
	"strings"
	"sync"
	"time"

	"gopkg.in/natefinch/lumberjack.v2"
)

var (
	ErrorArray []error
	Loggable   bool = false
	Logger     *log.Logger
	mu         sync.Mutex

	// Matches ANSI escape codes
	ansiEscape = regex.AnsiEscapeCompile()
)

// SetupLogging creates and/or opens the log file
func SetupLogging(targetDir string) error {

	logFile := &lumberjack.Logger{
		Filename:   filepath.Join(targetDir, "metarr.log"), // Log file path
		MaxSize:    1,                                      // Max size in MB before rotation
		MaxBackups: 3,                                      // Number of backups to retain
		Compress:   true,                                   // Gzip compression
	}

	// Assign lumberjack logger to standard log output
	Logger = log.New(logFile, "", 0)
	Loggable = true

	Logger.Printf(":\n=========== %v ===========\n\n", time.Now().Format(time.RFC1123Z))
	return nil
}

// Write writes error information to the log file
func writeLog(msg string, level int) {
	mu.Lock()
	defer mu.Unlock()

	if Loggable && level < 2 {
		if !strings.HasPrefix(msg, "\n") {
			msg += "\n"
		}

		if ansiEscape == nil {
			ansiEscape = regex.AnsiEscapeCompile()
		}

		Logger.Print(ansiEscape.ReplaceAllString(msg, ""))
	}
}
package printout

import (
	"fmt"
	"metarr/internal/domain/consts"
	"metarr/internal/models"
	"metarr/internal/utils/logging"
	"reflect"
	"strings"
	"sync"
)

var muPrint sync.Mutex

// CreateModelPrintout prints out the values stored in a struct.
// taskName allows you to enter your own identifier for this task.
func CreateModelPrintout(model any, filename, taskName string, args ...any) {
	if model == nil {
		logging.E(0, "Model entered nil for taskname %q", taskName)
		return
	}

	muPrint.Lock()
	defer muPrint.Unlock()

	var b strings.Builder
	b.Grow(20000)

	// Helper function to add sections
	addSection := func(title string, content string) {
		b.WriteString(consts.ColorYellow + "\n" + title + ":\n" + consts.ColorReset)
		b.WriteString(content)
	}

	// Header
	b.WriteString("\n\n================= ")
	b.WriteString(consts.ColorCyan + "Printing metadata fields for: " + consts.ColorReset)
	b.WriteString("'" + consts.ColorReset + filename + "'")
	b.WriteString(" =================\n")

	if taskName != "" {
		str := fmt.Sprintf("'"+taskName+"'", args...)
		b.WriteString("\n" + consts.ColorGreen + "Printing model at point of task " + consts.ColorReset + str + "\n")
	}

	// Add fields from the struct
	addSection("File Information", printStructFields(model))

	switch m := model.(type) {
	case *models.FileData:

		addSection("Credits", printStructFields(m.MCredits))
		addSection("Titles and descriptions", printStructFields(m.MTitleDesc))
		addSection("Dates and timestamps", printStructFields(m.MDates))
		addSection("Webpage data", printStructFields(m.MWebData))
		addSection("Show data", printStructFields(m.MShowData))
		addSection("Other data", printStructFields(m.MOther))

	case *models.NFOData:
		// Credits section
		b.WriteString(consts.ColorYellow + "\nCredits:\n" + consts.ColorReset)

		// Handle each slice type separately
		for _, actor := range m.Actors {
			b.WriteString(printStructFields(actor.Name))
		}
		for _, director := range m.Directors {
			b.WriteString(printStructFields(director))
		}
		for _, producer := range m.Producers {
			b.WriteString(printStructFields(producer))
		}
		for _, publisher := range m.Publishers {
			b.WriteString(printStructFields(publisher))
		}
		for _, studio := range m.Studios {
			b.WriteString(printStructFields(studio))
		}
		for _, writer := range m.Writers {
			b.WriteString(printStructFields(writer))
		}

		addSection("Titles and descriptions", printStructFields(m.Title)+
			printStructFields(m.Description)+
			printStructFields(m.Plot))

		addSection("Webpage data", printStructFields(m.WebpageInfo))

		addSection("Show data", printStructFields(m.ShowInfo.Show)+
			printStructFields(m.ShowInfo.EpisodeID)+
			printStructFields(m.ShowInfo.EpisodeTitle)+
			printStructFields(m.ShowInfo.SeasonNumber))
	}

	// Footer
	b.WriteString("\n\n================= ")
	b.WriteString(consts.ColorYellow + "End metadata fields for: " + consts.ColorReset)
	b.WriteString("'" + filename + "'")
	b.WriteString(" =================\n\n")

	logging.P(b.String())
}

// Function to print the fields of a struct using reflection
func printStructFields(s any) string {
	val := reflect.ValueOf(s)

	// Dereference pointer
	if val.Kind() == reflect.Ptr {
		val = val.Elem()
	}

	if val.Kind() != reflect.Struct {
		return fmt.Sprintf("Expected a struct, got %s\n", val.Kind())
	}

	typ := val.Type()

	var b strings.Builder
	b.Grow(val.NumField() * 1024)

	for i := 0; i < val.NumField(); i++ {
		field := typ.Field(i)      // Get field metadata
		fieldValue := val.Field(i) // Get field value

		// Skip zero or empty fields
		if fieldValue.IsZero() {
			b.WriteString(field.Name + consts.ColorRed + " [empty]\n" + consts.ColorReset)
			continue
		}

		fieldName := field.Name
		fieldValueStr := fmt.Sprintf("%v", fieldValue.Interface()) // Convert the value to a string

		// Append the field name and value in key-value format
		b.WriteString(fmt.Sprintf("%s: %s\n", fieldName, fieldValueStr))
	}

	return b.String()
}

// Print out the fetched fields
func PrintGrabbedFields(fieldType string, p map[string]string) {
	muPrint.Lock()
	defer muPrint.Unlock()

	fmt.Println()
	logging.I("Found and stored %s metadata fields from metafile:", fieldType)
	fmt.Println()

	for k, v := range p {
		if k != "" && v != "" {
			logging.P(fmt.Sprintf(consts.ColorGreen + "Key: " + consts.ColorReset + k + consts.ColorYellow + "\nValue: " + consts.ColorReset + v + "\n"))
		}
	}
	fmt.Println()
}
package utils

import (
	"bufio"
	"context"
	"fmt"
	logging "metarr/internal/utils/logging"
	"os"
	"strings"
)

var (
	userInputChan = make(chan string) // Channel for user input
	decisionMade  bool
)

// InitUserInputReader initializes a user input reading function in a goroutine
func InitUserInputReader() {
	go func() {
		reader := bufio.NewReader(os.Stdin)
		for {
			input, _ := reader.ReadString('\n')
			userInputChan <- strings.TrimSpace(input)
		}
	}()
}

// PromptMetaReplace displays a prompt message and waits for valid user input.
// The option can be used to tell the program to overwrite all in the queue,
// preserve all in the queue, or move through value by value
func PromptMetaReplace(promptMsg string, ow, ps bool) (string, error) {

	logging.D(3, "Entering PromptUser dialogue...")
	ctx := context.Background()

	if decisionMade {
		// If overwriteAll, return "Y" without waiting
		if ow {
			logging.D(3, "Overwrite all is set...")
			return "Y", nil
		} else if ps {
			logging.D(3, "Preserve all is set...")
			return "N", nil
		}
	}

	fmt.Println()
	logging.I(promptMsg)

	// Wait for user input
	select {
	case response := <-userInputChan:
		if response == "Y" {
			ow = true
		}
		decisionMade = true
		return response, nil

	case <-ctx.Done():
		logging.I("Operation canceled during input.")
		return "", fmt.Errorf("operation canceled")
	}
}
package utils

import "strings"

// validateExtension checks if the output extension is valid
func ValidateExtension(ext string) string {
	ext = strings.TrimSpace(ext)

	// Handle empty or invalid cases
	if ext == "" || ext == "." {
		return ""
	}

	// Ensure proper dot prefix
	if !strings.HasPrefix(ext, ".") {
		ext = "." + ext
	}

	// Verify the extension is not just a lone dot
	if len(ext) <= 1 {
		return ""
	}

	return ext
}
package main

import (
	"context"
	"fmt"
	"metarr/internal/cfg"
	keys "metarr/internal/domain/keys"
	"metarr/internal/models"
	"metarr/internal/processing"
	fsRead "metarr/internal/utils/fs/read"
	logging "metarr/internal/utils/logging"
	prompt "metarr/internal/utils/prompt"
	"os"
	"os/signal"
	"runtime/pprof"
	"runtime/trace"
	"sync"
	"syscall"
	"time"
)

// String constants
const (
	timeFormat     = "2006-01-02 15:04:05.00 MST"
	startLogFormat = "metarr started at: %s"
	endLogFormat   = "metarr finished at: %s"
	elapsedFormat  = "Time elapsed: %.2f seconds"
)

// Sigs here prevents heap escape
var (
	startTime time.Time
	sigInt    = syscall.SIGINT
	sigTerm   = syscall.SIGTERM
)

func init() {
	startTime = time.Now()
	logging.I(startLogFormat, startTime.Format(timeFormat))

	// Benchmarking
	if cfg.GetBool(keys.Benchmarking) {
		setupBenchmarking()
	}
}

func main() {
	if err := cfg.Execute(); err != nil {
		fmt.Fprintln(os.Stderr, err)
		fmt.Println()
		os.Exit(1)
	}

	if !cfg.GetBool("execute") {
		fmt.Println()
		logging.I(`(Separate fields supporting multiple entries by commas with no spaces e.g. "title:example,date:20240101")`)
		fmt.Println()
		return // Exit early if not meant to execute
	}

	// Program elements
	ctx, cancel := context.WithCancel(context.Background())
	cleanupChan := make(chan os.Signal, 1)
	signal.Notify(cleanupChan, sigInt, sigTerm)
	wg := new(sync.WaitGroup)

	core := &models.Core{
		Cleanup: cleanupChan,
		Cancel:  cancel,
		Ctx:     ctx,
		Wg:      wg,
	}

	if err := fsRead.InitFetchFilesVars(); err != nil {
		logging.E(0, "Failed to initialize variables to fetch files. Exiting...")
		cancel() // Do not remove call before exit
		os.Exit(1)
	}

	prompt.InitUserInputReader()

	if cfg.IsSet(keys.BatchPairs) {
		processing.StartBatchLoop(core)
	} else {
		logging.I("No files or directories to process. Exiting.")
	}

	endTime := time.Now()
	logging.I(endLogFormat, endTime.Format(timeFormat))
	logging.I(elapsedFormat, endTime.Sub(startTime).Seconds())
	fmt.Println()
}

// Benchmarking ////////////////////////////////////////////////////////////////////////////////////////////

type benchFiles struct {
	cpuFile   *os.File
	memFile   *os.File
	traceFile *os.File
}

func setupBenchmarking() {
	var (
		b   benchFiles
		err error
	)

	// CPU profile
	b.cpuFile, err = os.Create("cpu.prof")
	if err != nil {
		closeBenchFiles(&b, fmt.Sprintf("could not create CPU profile: %v", err))
	}

	if err := pprof.StartCPUProfile(b.cpuFile); err != nil {
		closeBenchFiles(&b, fmt.Sprintf("could not start CPU profile: %v", err))
	}

	defer pprof.StopCPUProfile()

	// Memory profile
	b.memFile, err = os.Create("mem.prof")
	if err != nil {
		closeBenchFiles(&b, fmt.Sprintf("could not create memory profile: %v", err))
	}
	defer func() {
		if cfg.GetBool(keys.Benchmarking) {
			if err := pprof.WriteHeapProfile(b.memFile); err != nil {
				closeBenchFiles(&b, fmt.Sprintf("could not write memory profile: %v", err))
			}
		}
	}()

	// Trace
	b.traceFile, err = os.Create("trace.out")
	if err != nil {
		closeBenchFiles(&b, fmt.Sprintf("could not create trace file: %v", err))
	}
	if err := trace.Start(b.traceFile); err != nil {
		closeBenchFiles(&b, fmt.Sprintf("could not start trace: %v", err))
	}
}

// closeBenchFiles closes bench files on program termination
func closeBenchFiles(b *benchFiles, exitMsg string) {

	if b.cpuFile != nil {
		b.cpuFile.Close()
	}

	if b.memFile != nil {
		b.memFile.Close()
	}

	if b.traceFile != nil {
		b.traceFile.Close()
	}

	logging.E(0, exitMsg)
	os.Exit(1)
}
