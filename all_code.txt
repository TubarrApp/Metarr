package config

import (
	keys "metarr/internal/domain/keys"

	"github.com/spf13/viper"
)

// initFilesDirs initializes user flag settings for input files and directories
func initFilesDirs() {

	// Videos
	rootCmd.PersistentFlags().StringP(keys.VideoDir, "v", ".", "Video directory")
	viper.BindPFlag(keys.VideoDir, rootCmd.PersistentFlags().Lookup(keys.VideoDir))

	rootCmd.PersistentFlags().StringP(keys.VideoFile, "V", ".", "Video file")
	viper.BindPFlag(keys.VideoFile, rootCmd.PersistentFlags().Lookup(keys.VideoFile))

	// JSON
	rootCmd.PersistentFlags().StringP(keys.JsonDir, "j", ".", "JSON directory")
	viper.BindPFlag(keys.JsonDir, rootCmd.PersistentFlags().Lookup(keys.JsonDir))

	rootCmd.PersistentFlags().StringP(keys.JsonFile, "J", ".", "JSON file")
	viper.BindPFlag(keys.JsonFile, rootCmd.PersistentFlags().Lookup(keys.JsonFile))

	// Cookies
	rootCmd.PersistentFlags().String(keys.CookiePath, "", "Specify cookie location")
	viper.BindPFlag(keys.CookiePath, rootCmd.PersistentFlags().Lookup(keys.CookiePath))
}

// initResourceRelated initializes user flag settings for parameters related to system hardware
func initResourceRelated() {

	// Concurrency limit
	rootCmd.PersistentFlags().IntP(keys.Concurrency, "l", 5, "Max concurrency limit")
	viper.BindPFlag(keys.Concurrency, rootCmd.PersistentFlags().Lookup(keys.Concurrency))

	// CPU usage
	rootCmd.PersistentFlags().Float64P(keys.MaxCPU, "c", 100.0, "Max CPU usage")
	viper.BindPFlag(keys.MaxCPU, rootCmd.PersistentFlags().Lookup(keys.MaxCPU))

	// Min memory
	rootCmd.PersistentFlags().Uint64P(keys.MinMem, "m", 0, "Minimum RAM to start process")
	viper.BindPFlag(keys.MinMem, rootCmd.PersistentFlags().Lookup(keys.MinMem))

	// Hardware accelerated transcoding
	rootCmd.PersistentFlags().StringP(keys.GPU, "g", "none", "GPU acceleration type (nvidia, amd, intel, none)")
	viper.BindPFlag(keys.GPU, rootCmd.PersistentFlags().Lookup(keys.GPU))
}

// initAllFileTransformers initializes user flag settings for transformations applying to all files
func initAllFileTransformers() {

	// Prefix file with metafield
	rootCmd.PersistentFlags().StringSlice(keys.MFilenamePfx, nil, "Adds a specified metatag's value onto the start of the filename")
	viper.BindPFlag(keys.MFilenamePfx, rootCmd.PersistentFlags().Lookup(keys.MFilenamePfx))

	// Prefix files with date tag
	rootCmd.PersistentFlags().String(keys.InputFileDatePfx, "", "Looks for dates in metadata to prefix the video with. (date:format [e.g. Ymd for yyyy-mm-dd])")
	viper.BindPFlag(keys.InputFileDatePfx, rootCmd.PersistentFlags().Lookup(keys.InputFileDatePfx))

	// Rename convention
	rootCmd.PersistentFlags().StringP(keys.RenameStyle, "r", "fixes-only", "Rename flag (spaces, underscores, fixes-only, or skip)")
	viper.BindPFlag(keys.RenameStyle, rootCmd.PersistentFlags().Lookup(keys.RenameStyle))

	// Replace filename suffix
	rootCmd.PersistentFlags().StringSliceVar(&filenameReplaceSuffixInput, keys.InputFilenameReplaceSfx, nil, "Replaces a specified suffix on filenames. (suffix:replacement)")
	viper.BindPFlag(keys.InputFilenameReplaceSfx, rootCmd.PersistentFlags().Lookup(keys.InputFilenameReplaceSfx))

	// Backup files by renaming original files
	rootCmd.PersistentFlags().BoolP(keys.NoFileOverwrite, "n", false, "Renames the original files to avoid overwriting")
	viper.BindPFlag(keys.NoFileOverwrite, rootCmd.PersistentFlags().Lookup(keys.NoFileOverwrite))

	// Output directory (can be external)
	rootCmd.PersistentFlags().StringP(keys.MoveOnComplete, "o", "", "Move files to given directory on program completion")
	viper.BindPFlag(keys.MoveOnComplete, rootCmd.PersistentFlags().Lookup(keys.MoveOnComplete))
}

// initMetaTransformers initializes user flag settings for manipulation of metadata
func initMetaTransformers() {

	// Metadata transformations
	rootCmd.PersistentFlags().StringSlice(keys.MetaOps, nil, "Metadata operations (field:operation:value) - e.g. title:set:New Title, description:prefix:Draft-, tags:append:newtag")
	viper.BindPFlag(keys.MetaOps, rootCmd.PersistentFlags().Lookup(keys.MetaOps))

	// Prefix or append description fields with dates
	rootCmd.PersistentFlags().Bool(keys.MDescDatePfx, false, "Adds the date to the start of the description field.")
	viper.BindPFlag(keys.MDescDatePfx, rootCmd.PersistentFlags().Lookup(keys.MDescDatePfx))

	rootCmd.PersistentFlags().Bool(keys.MDescDateSfx, false, "Adds the date to the end of the description field.")
	viper.BindPFlag(keys.MDescDateSfx, rootCmd.PersistentFlags().Lookup(keys.MDescDateSfx))

	// Overwrite or preserve metafields
	rootCmd.PersistentFlags().Bool(keys.MOverwrite, false, "When adding new metadata fields, automatically overwrite existing fields with your new values")
	viper.BindPFlag(keys.MOverwrite, rootCmd.PersistentFlags().Lookup(keys.MOverwrite))

	rootCmd.PersistentFlags().Bool(keys.MPreserve, false, "When adding new metadata fields, skip already existent fields")
	viper.BindPFlag(keys.MPreserve, rootCmd.PersistentFlags().Lookup(keys.MPreserve))

	rootCmd.PersistentFlags().String(keys.MetaPurge, "", "Delete metadata files (e.g. .json, .nfo) after the video is successfully processed")
	viper.BindPFlag(keys.MetaPurge, rootCmd.PersistentFlags().Lookup(keys.MetaPurge))
}

// initVideoTransformers initializes user flag settings for transformation of video files
func initVideoTransformers() {

	// Output extension type
	rootCmd.PersistentFlags().String(keys.OutputFiletypeInput, "", "File extension to output files as (mp4 works best for most media servers)")
	viper.BindPFlag(keys.OutputFiletypeInput, rootCmd.PersistentFlags().Lookup(keys.OutputFiletypeInput))
}

// initFiltering initializes user flag settings for filtering files to work with
func initFiltering() {

	// Video file extensions to convert
	rootCmd.PersistentFlags().StringSliceP(keys.InputVideoExts, "e", []string{"all"}, "File extensions to convert (all, mkv, mp4, webm)")
	viper.BindPFlag(keys.InputVideoExts, rootCmd.PersistentFlags().Lookup(keys.InputVideoExts))

	// Meta file extensions to convert
	rootCmd.PersistentFlags().StringSlice(keys.InputMetaExts, []string{"all"}, "File extensions to convert (all, json, nfo)")
	viper.BindPFlag(keys.InputMetaExts, rootCmd.PersistentFlags().Lookup(keys.InputMetaExts))

	// Only convert files with prefix
	rootCmd.PersistentFlags().StringSliceP(keys.FilePrefixes, "p", []string{""}, "Filters files by prefixes")
	viper.BindPFlag(keys.FilePrefixes, rootCmd.PersistentFlags().Lookup(keys.FilePrefixes))
}

// initProgramFunctions initializes user flag settings for miscellaneous program features such as debug level
func initProgramFunctions() {

	// Debugging level
	rootCmd.PersistentFlags().Uint16P(keys.DebugLevel, "d", 0, "Level of debugging (0 - 3)")
	viper.BindPFlag(keys.DebugLevel, rootCmd.PersistentFlags().Lookup(keys.DebugLevel))

	// Skip videos, only alter metafiles
	rootCmd.PersistentFlags().Bool(keys.SkipVideos, false, "Skips compiling/transcoding the videos and just edits the file names/JSON file fields")
	viper.BindPFlag(keys.SkipVideos, rootCmd.PersistentFlags().Lookup(keys.SkipVideos))

	// Preset configurations for sites
	rootCmd.PersistentFlags().String(keys.InputPreset, "", "Use a preset configuration (e.g. censoredtv)")
	viper.BindPFlag(keys.InputPreset, rootCmd.PersistentFlags().Lookup(keys.InputPreset))

	// Output benchmarking files
	rootCmd.PersistentFlags().Bool(keys.Benchmarking, false, "Benchmarks the program")
	viper.BindPFlag(keys.Benchmarking, rootCmd.PersistentFlags().Lookup(keys.Benchmarking))
}
package config

import (
	"fmt"
	consts "metarr/internal/domain/constants"
	enums "metarr/internal/domain/enums"
	keys "metarr/internal/domain/keys"
	logging "metarr/internal/utils/logging"
	"os"
	"strings"

	"github.com/shirou/gopsutil/mem"
	"github.com/spf13/cobra"
	"github.com/spf13/viper"
)

var rootCmd = &cobra.Command{
	Use:   "metarr",
	Short: "metarr is a video and metatagging tool",
	RunE: func(cmd *cobra.Command, args []string) error {
		if cmd.Flags().Lookup("help").Changed {
			return nil // Stop further execution if help is invoked
		}
		viper.Set("execute", true)
		return execute()
	},
}

// init sets the initial Viper settings
func init() {

	// Files and directories
	initFilesDirs()

	// System resource related
	initResourceRelated()

	// Filtering
	initFiltering()

	// All file transformations
	initAllFileTransformers()

	// Filename transformations
	initVideoTransformers()

	// Metadata and metafile manipulation
	initMetaTransformers()

	// Special functions
	initProgramFunctions()
}

// Execute is the primary initializer of Viper
func Execute() error {

	fmt.Println()

	err := rootCmd.Execute()
	if err != nil {
		logging.E(0, "Failed to execute cobra")
		return err

	}
	return nil
}

// execute more thoroughly handles settings created in the Viper init
func execute() error {

	// Parse GPU settings and set commands
	verifyHWAcceleration()

	// Concurrency
	verifyConcurrencyLimit()

	// Resource usage limits (CPU and memory)
	verifyResourceLimits()

	// File extension settings
	verifyInputFiletypes()

	// File prefix filter settings
	verifyFilePrefixes()

	// Debugging level
	verifyDebugLevel()

	// Filetype to output as
	verifyOutputFiletype()

	// Meta overwrite and preserve flags
	verifyMetaOverwritePreserve()

	// Verify user metafile purge settings
	verifyPurgeMetafiles()

	// Ensure no video and metadata location conflicts
	if err := checkFileDirs(); err != nil {
		return err
	}

	if err := initTextReplace(); err != nil {
		return err
	}

	if err := initDateReplaceFormat(); err != nil {
		return err
	}

	return nil
}

// checkFileDirConflicts ensures no conflicts in the file and directories entered by the user
func checkFileDirs() error {

	videoFile := strings.TrimSpace(viper.GetString(keys.VideoFile))
	videoFileSet := viper.IsSet(keys.VideoFile)

	videoDir := strings.TrimSpace(viper.GetString(keys.VideoDir))
	videoDirSet := viper.IsSet(keys.VideoDir)

	jsonFile := strings.TrimSpace(viper.GetString(keys.JsonFile))
	jsonFileSet := viper.IsSet(keys.JsonFile)

	jsonDir := strings.TrimSpace(viper.GetString(keys.JsonDir))
	jsonDirSet := viper.IsSet(keys.JsonDir)

	// Validate configuration
	if jsonFileSet {
		if jsonDirSet {
			return fmt.Errorf("cannot set both the JSON file and the JSON directory")
		}
		if videoDirSet {
			return fmt.Errorf("cannot set singular metadata file for whole video directory")
		}
	}
	if videoFileSet {
		if videoDirSet {
			return fmt.Errorf("cannot set singular video file AND video directory")
		}
		viper.Set(keys.SingleFile, true)
	}

	// Check files and dirs exist
	if viper.IsSet(keys.JsonFile) {
		if _, err := os.Stat(jsonFile); err != nil {
			return fmt.Errorf("file '%s' does not exist", jsonFile)
		}
		if fileInfo, _ := os.Stat(jsonFile); fileInfo.IsDir() {
			return fmt.Errorf("entered directory '%s' as a file", jsonFile)
		}
	}

	if viper.IsSet(keys.JsonDir) {
		if _, err := os.Stat(jsonDir); err != nil {
			return fmt.Errorf("directory '%s' does not exist", jsonDir)
		}
		if fileInfo, _ := os.Stat(jsonDir); !fileInfo.IsDir() {
			return fmt.Errorf("entered file '%s' as a directory", jsonDir)
		}
	}

	if viper.IsSet(keys.VideoFile) {
		if _, err := os.Stat(videoFile); err != nil {
			return fmt.Errorf("file '%s' does not exist", videoFile)
		}
		if fileInfo, _ := os.Stat(videoFile); fileInfo.IsDir() {
			return fmt.Errorf("entered directory '%s' as a file", videoFile)
		}
	}

	if viper.IsSet(keys.VideoDir) {
		if _, err := os.Stat(videoDir); err != nil {
			return fmt.Errorf("directory '%s' does not exist", videoDir)
		}
		if fileInfo, _ := os.Stat(videoDir); !fileInfo.IsDir() {
			return fmt.Errorf("entered file '%s' as a directory", videoDir)
		}
	}

	return nil
}

// verifyFilePrefixes checks and sets the file prefix filters
func verifyFilePrefixes() {
	if !viper.IsSet(keys.FilePrefixes) {
		return
	}

	argsInputPrefixes := viper.GetStringSlice(keys.FilePrefixes)
	filePrefixes := make([]string, 0, len(argsInputPrefixes))

	for _, arg := range argsInputPrefixes {
		if arg != "" {
			filePrefixes = append(filePrefixes, arg)
		}
	}
	if len(filePrefixes) > 0 {
		viper.Set(keys.FilePrefixes, filePrefixes)
	}
}

// verifyMetaOverwritePreserve checks if the entered meta overwrite and preserve flags are valid
func verifyMetaOverwritePreserve() {
	if GetBool(keys.MOverwrite) && GetBool(keys.MPreserve) {
		logging.E(0, "Cannot enter both meta preserve AND meta overwrite, exiting...")
		os.Exit(1)
	}
}

// verifyDebugLevel checks and sets the debugging level to use
func verifyDebugLevel() {
	debugLevel := viper.GetUint16(keys.DebugLevel)
	if debugLevel > 5 {
		debugLevel = 5
	} else if debugLevel == 0 {
		logging.I("Debugging level: %v", debugLevel)
	}
	viper.Set(keys.DebugLevel, debugLevel)
	logging.Level = int(debugLevel)
}

// verifyInputFiletypes checks that the inputted filetypes are accepted
func verifyInputFiletypes() {
	argsVInputExts := viper.GetStringSlice(keys.InputVideoExts)
	inputVExts := make([]enums.ConvertFromFiletype, 0, len(argsVInputExts))

	for _, data := range argsVInputExts {
		switch data {
		case "mkv":
			inputVExts = append(inputVExts, enums.VID_EXTS_MKV)
		case "mp4":
			inputVExts = append(inputVExts, enums.VID_EXTS_MP4)
		case "webm":
			inputVExts = append(inputVExts, enums.VID_EXTS_WEBM)
		default:
			inputVExts = append(inputVExts, enums.VID_EXTS_ALL)
		}
	}
	if len(inputVExts) == 0 {
		inputVExts = append(inputVExts, enums.VID_EXTS_ALL)
	}
	logging.D(2, "Received video input extension filter: %v", inputVExts)
	viper.Set(keys.InputVExtsEnum, inputVExts)

	argsMInputExts := viper.GetStringSlice(keys.InputMetaExts)
	inputMExts := make([]enums.MetaFiletypeFilter, 0, len(argsMInputExts))

	for _, data := range argsMInputExts {
		switch data {
		case "json":
			inputMExts = append(inputMExts, enums.META_EXTS_JSON)
		case "nfo":
			inputMExts = append(inputMExts, enums.META_EXTS_NFO)
		default:
			inputMExts = append(inputMExts, enums.META_EXTS_ALL)
		}
	}
	if len(inputMExts) == 0 {
		inputMExts = append(inputMExts, enums.META_EXTS_ALL)
	}
	logging.D(2, "Received meta input extension filter: %v", inputMExts)
	viper.Set(keys.InputMExtsEnum, inputMExts)
}

// verifyHWAcceleration checks and sets HW acceleration to use
func verifyHWAcceleration() {
	switch viper.GetString(keys.GPU) {
	case "nvidia":
		viper.Set(keys.GPUEnum, enums.GPU_NVIDIA)
		logging.P("GPU acceleration selected by user: %v", keys.GPU)
	case "amd":
		viper.Set(keys.GPUEnum, enums.GPU_AMD)
		logging.P("GPU acceleration selected by user: %v", keys.GPU)
	case "intel":
		viper.Set(keys.GPUEnum, enums.GPU_INTEL)
		logging.P("GPU acceleration selected by user: %v", keys.GPU)
	default:
		viper.Set(keys.GPUEnum, enums.GPU_NO_HW_ACCEL)
	}
}

// verifyConcurrencyLimit checks and ensures correct concurrency limit input
func verifyConcurrencyLimit() {
	maxConcurrentProcesses := viper.GetInt(keys.Concurrency)

	switch {
	case maxConcurrentProcesses < 1:
		maxConcurrentProcesses = 1
		logging.E(2, "Max concurrency set too low, set to minimum value: %d", maxConcurrentProcesses)
	default:
		logging.I("Max concurrency: %d", maxConcurrentProcesses)
	}
	viper.Set(keys.Concurrency, maxConcurrentProcesses)
}

// verifyCPUUsage verifies the value used to limit the CPU needed to spawn a new routine
func verifyResourceLimits() {
	MinMemUsage := viper.GetUint64(keys.MinMem)
	MinMemUsage *= 1024 * 1024 // Convert input to MB

	currentAvailableMem, err := mem.VirtualMemory()
	if err != nil {
		logging.E(0, "Could not get system memory, using default max RAM requirements", err)
		currentAvailableMem.Available = 1024
	}
	if MinMemUsage > currentAvailableMem.Available {
		MinMemUsage = currentAvailableMem.Available
	}

	if MinMemUsage > 0 {
		logging.I("Min RAM to spawn process: %v", MinMemUsage)
	}
	viper.Set(keys.MinMemMB, MinMemUsage)

	maxCPUUsage := viper.GetFloat64(keys.MaxCPU)
	switch {
	case maxCPUUsage > 100.0:
		maxCPUUsage = 100.0
		logging.E(2, "Max CPU usage entered too high, setting to default max: %.2f%%", maxCPUUsage)

	case maxCPUUsage < 1.0:
		maxCPUUsage = 10.0
		logging.E(0, "Max CPU usage entered too low, setting to default low: %.2f%%", maxCPUUsage)
	}
	if maxCPUUsage != 100.0 {
		logging.I("Max CPU usage: %.2f%%", maxCPUUsage)
	}
	viper.Set(keys.MaxCPU, maxCPUUsage)
}

// Verify the output filetype is valid for FFmpeg
func verifyOutputFiletype() {
	if !viper.IsSet(keys.OutputFiletype) {
		return
	}

	o := GetString(keys.OutputFiletypeInput)
	o = strings.TrimSpace(o)

	if !strings.HasPrefix(o, ".") {
		o = "." + o
		viper.Set(keys.OutputFiletype, o)
	}

	valid := false
	for _, ext := range consts.AllVidExtensions {
		if o != ext {
			continue
		} else {
			valid = true
			break
		}
	}

	if valid {
		logging.I("Outputting files as %s", o)
	}
}

// verifyPurgeMetafiles checks and sets the type of metafile purge to perform
func verifyPurgeMetafiles() {
	if !viper.IsSet(keys.MetaPurge) {
		return
	}

	var e enums.PurgeMetafiles
	purgeType := viper.GetString(keys.MetaPurge)

	purgeType = strings.TrimSpace(purgeType)
	purgeType = strings.ToLower(purgeType)
	purgeType = strings.ReplaceAll(purgeType, ".", "")

	switch purgeType {
	case "all":
		e = enums.PURGEMETA_ALL
	case "json":
		e = enums.PURGEMETA_JSON
	case "nfo":
		e = enums.PURGEMETA_NFO
	default:
		e = enums.PURGEMETA_NONE
	}

	viper.Set(keys.MetaPurgeEnum, e)
}
package config

import (
	"fmt"
	enums "metarr/internal/domain/enums"
	keys "metarr/internal/domain/keys"
	"metarr/internal/models"
	logging "metarr/internal/utils/logging"
	"strings"

	"github.com/spf13/viper"
)

var (
	filenameReplaceSuffixInput []string
)

type metaOpsLen struct {
	newLen,
	apndLen,
	pfxLen,
	trimSfxLen,
	trimPfxLen,
	replaceLen,
	dTagLen,
	delDTagLen int
}

// initTextReplace initializes text replacement functions
func initTextReplace() error {

	// Parse rename flag
	setRenameFlag()

	// Meta operations
	if err := validateMetaOps(); err != nil {
		return err
	}

	// Replace filename suffixes
	if err := validateFilenameSuffixReplace(); err != nil {
		return err
	}

	return nil
}

// validateMetaOps parses the meta transformation operations
func validateMetaOps() error {

	metaOpsInput := GetStringSlice(keys.MetaOps)
	if len(metaOpsInput) == 0 {
		logging.D(2, "No metadata operations passed in")
		return nil
	}

	m := metaOpsLen{}
	m = metaOpsMapLength(metaOpsInput, m)

	// Add and replace
	newField := make([]*models.MetaNewField, 0, m.newLen)
	replace := make([]*models.MetaReplace, 0, m.replaceLen)

	// Prefixes and suffixes
	apnd := make([]*models.MetaAppend, 0, m.apndLen)
	pfx := make([]*models.MetaPrefix, 0, m.pfxLen)
	trimSfx := make([]*models.MetaTrimSuffix, 0, m.trimSfxLen)
	trimPfx := make([]*models.MetaTrimPrefix, 0, m.trimPfxLen)

	// Date tagging
	dateTag := make(map[string]*models.MetaDateTag, m.dTagLen)
	delDateTag := make(map[string]*models.MetaDateTag, m.delDTagLen)

	for _, op := range metaOpsInput {

		// Check validity
		parts := strings.Split(op, ":")

		if len(parts) < 3 || len(parts) > 4 {
			return fmt.Errorf("malformed input meta-ops, each entry must be at least 3 parts, split by : (e.g. 'title:add:Video Title')")
		}

		field := parts[0]
		operation := parts[1]
		value := parts[2]

		switch strings.ToLower(operation) {
		case "set":
			newFieldModel := &models.MetaNewField{
				Field: field,
				Value: value,
			}
			newField = append(newField, newFieldModel)
			fmt.Println()
			logging.D(3, "Added new field op:\nField: %s\nValue: %s", newFieldModel.Field, newFieldModel.Value)
			fmt.Println()

		case "append":
			apndModel := &models.MetaAppend{
				Field:  field,
				Suffix: value,
			}
			apnd = append(apnd, apndModel)
			fmt.Println()
			logging.D(3, "Added new append op:\nField: %s\nAppend: %s", apndModel.Field, apndModel.Suffix)
			fmt.Println()

		case "prefix":
			pfxModel := &models.MetaPrefix{
				Field:  field,
				Prefix: value,
			}
			pfx = append(pfx, pfxModel)
			fmt.Println()
			logging.D(3, "Added new prefix op:\nField: %s\nPrefix: %s", pfxModel.Field, pfxModel.Prefix)
			fmt.Println()

		case "trim-suffix":
			tSfxModel := &models.MetaTrimSuffix{
				Field:  field,
				Suffix: value,
			}
			trimSfx = append(trimSfx, tSfxModel)
			fmt.Println()
			logging.D(3, "Added new suffix trim op:\nField: %s\nSuffix: %s", tSfxModel.Field, tSfxModel.Suffix)
			fmt.Println()

		case "trim-prefix":
			tPfxModel := &models.MetaTrimPrefix{
				Field:  field,
				Prefix: value,
			}
			trimPfx = append(trimPfx, tPfxModel)
			fmt.Println()
			logging.D(3, "Added new prefix trim op:\nField: %s\nPrefix: %s", tPfxModel.Field, tPfxModel.Prefix)
			fmt.Println()

		case "replace":
			if len(parts) != 4 {
				return fmt.Errorf("replacement should be in format 'field:replace:text:replacement'")
			}
			rModel := &models.MetaReplace{
				Field:       field,
				Value:       value,
				Replacement: parts[3],
			}
			replace = append(replace, rModel)
			fmt.Println()
			logging.D(3, "Added new replace operation:\nField: %s\nValue: %s\nReplacement: %s\n", rModel.Field, rModel.Value, rModel.Replacement)
			fmt.Println()

		case "date-tag":
			if len(parts) != 4 {
				return fmt.Errorf("date-tag should be in format 'field:date-tag:location:format' (Ymd is yyyy-mm-dd, ymd is yy-mm-dd)")
			}
			var loc enums.MetaDateTagLocation

			switch strings.ToLower(value) {
			case "prefix":
				loc = enums.DATE_TAG_LOC_PFX
			case "suffix":
				loc = enums.DATE_TAG_LOC_SFX
			default:
				return fmt.Errorf("date tag location must be prefix, or suffix")
			}
			if e, err := dateEnum(parts[3]); err != nil {
				return err
			} else {
				dateTag[field] = &models.MetaDateTag{
					Loc:    loc,
					Format: e,
				}
				fmt.Println()
				logging.D(3, "Added new date tag operation:\nField: %s\nLocation: %s\nReplacement: %s\n", field, value, parts[3])
				fmt.Println()
			}

		case "delete-date-tag":
			if len(parts) != 4 {
				return fmt.Errorf("date-tag should be in format 'field:date-tag:location:format' (Ymd is yyyy-mm-dd, ymd is yy-mm-dd)")
			}
			var loc enums.MetaDateTagLocation

			switch strings.ToLower(value) {
			case "prefix":
				loc = enums.DATE_TAG_LOC_PFX
			case "suffix":
				loc = enums.DATE_TAG_LOC_SFX
			default:
				return fmt.Errorf("date tag location must be prefix, or suffix")
			}
			if e, err := dateEnum(parts[3]); err != nil {
				return err
			} else {
				delDateTag[field] = &models.MetaDateTag{
					Loc:    loc,
					Format: e,
				}
				fmt.Println()
				logging.D(3, "Added delete date tag operation:\nField: %s\nLocation: %s\nReplacement: %s\n", field, value, parts[3])
				fmt.Println()
			}

		default:
			return fmt.Errorf("unrecognized meta operation '%s' (valid operations: add, append, prefix, trim-suffix, trim-prefix)", parts[1])
		}
	}

	if len(apnd) > 0 {
		logging.I("Appending: %v", apnd)
		viper.Set(keys.MAppend, apnd)
	}

	if len(newField) > 0 {
		logging.I("New meta fields: %v", newField)
		viper.Set(keys.MNewField, newField)
	}

	if len(pfx) > 0 {
		logging.I("Prefixing: %v", apnd)
		viper.Set(keys.MPrefix, pfx)
	}

	if len(trimPfx) > 0 {
		logging.I("Trimming prefix: %v", trimPfx)
		viper.Set(keys.MTrimPrefix, trimPfx)
	}

	if len(trimSfx) > 0 {
		logging.I("Trimming suffix: %v", trimSfx)
		viper.Set(keys.MTrimSuffix, trimSfx)
	}

	if len(replace) > 0 {
		logging.I("Replacing text: %v", replace)
		viper.Set(keys.MReplaceText, replace)
	}

	if len(dateTag) > 0 {
		logging.I("Adding date tags: %v", dateTag)
		viper.Set(keys.MDateTagMap, dateTag)
	}

	if len(delDateTag) > 0 {
		logging.I("Deleting date tags: %v", delDateTag)
		viper.Set(keys.MDelDateTagMap, delDateTag)
	}

	return nil
}

// metaOpsMapLength quickly grabs the lengths needed for each map
func metaOpsMapLength(metaOpsInput []string, m metaOpsLen) metaOpsLen {

	for _, op := range metaOpsInput {
		if i := strings.IndexByte(op, ':'); i >= 0 {
			if j := strings.IndexByte(op[i+1:], ':'); j >= 0 {
				op = op[i+1 : i+1+j]

				switch op {
				case "set":
					m.newLen++
				case "append":
					m.apndLen++
				case "prefix":
					m.pfxLen++
				case "trim-suffix":
					m.trimSfxLen++
				case "trim-prefix":
					m.trimPfxLen++
				case "replace":
					m.replaceLen++
				case "date-tag":
					m.dTagLen++
				case "delete-date-tag":
					m.delDTagLen++
				}
			}
		}

	}
	fmt.Println()
	logging.D(2, "Meta additions: %d\nMeta appends: %d\nMeta prefix: %d\nMeta suffix trim: %d\nMeta prefix trim: %d\nMeta replacements: %d\nDate tags: %d\nDelete date tags: %d", m.newLen, m.apndLen, m.pfxLen, m.trimSfxLen, m.trimPfxLen, m.replaceLen, m.dTagLen, m.delDTagLen)
	fmt.Println()
	return m
}

// validateFilenameSuffixReplace checks if the input format for filename suffix replacement is valid
func validateFilenameSuffixReplace() error {
	filenameReplaceSuffix := make([]*models.FilenameReplaceSuffix, 0, len(filenameReplaceSuffixInput))

	for _, pair := range filenameReplaceSuffixInput {
		parts := strings.SplitN(pair, ":", 2)
		if len(parts) < 3 {
			return fmt.Errorf("invalid use of filename-replace-suffix, values must be written as (suffix:replacement)")
		}
		filenameReplaceSuffix = append(filenameReplaceSuffix, &models.FilenameReplaceSuffix{
			Suffix:      parts[0],
			Replacement: parts[1],
		})
	}
	if len(filenameReplaceSuffix) > 0 {
		logging.I("Meta replace suffixes: %v", filenameReplaceSuffix)
		viper.Set(keys.FilenameReplaceSfx, filenameReplaceSuffix)
	}
	return nil
}

// setRenameFlag sets the rename style to apply
func setRenameFlag() {

	var renameFlag enums.ReplaceToStyle
	argRenameFlag := GetString(keys.RenameStyle)

	// Trim whitespace for more robust validation
	argRenameFlag = strings.TrimSpace(argRenameFlag)
	argRenameFlag = strings.ToLower(argRenameFlag)

	switch argRenameFlag {
	case "spaces", "space":
		renameFlag = enums.RENAMING_SPACES
		logging.P("Rename style selected: %v", argRenameFlag)

	case "underscores", "underscore":
		renameFlag = enums.RENAMING_UNDERSCORES
		logging.P("Rename style selected: %v", argRenameFlag)

	case "fixes-only", "fixesonly":
		renameFlag = enums.RENAMING_FIXES_ONLY
		logging.P("Rename style selected: %v", argRenameFlag)

	default:
		logging.D(1, "'Spaces' or 'underscores' not selected for renaming style, skipping these modifications.")
		renameFlag = enums.RENAMING_SKIP
	}
	viper.Set(keys.Rename, renameFlag)
}

// initDateReplaceFormat initializes the user's preferred format for dates
func initDateReplaceFormat() error {

	if IsSet(keys.InputFileDatePfx) {
		dateFmt := GetString(keys.InputFileDatePfx)

		// Trim whitespace for more robust validation
		dateFmt = strings.TrimSpace(dateFmt)

		formatEnum, err := dateEnum(dateFmt)
		if err != nil {
			return err
		}

		viper.Set(keys.FileDateFmt, formatEnum)
		logging.D(1, "Set file date format to %v", formatEnum)
	}
	return nil
}

// dateEnum returns the date format enum type
func dateEnum(dateFmt string) (formatEnum enums.DateFormat, err error) {

	if len(dateFmt) < 2 {
		return enums.DATEFMT_SKIP, fmt.Errorf("invalid date format entered as '%s', please enter up to three characters (where 'Y' is yyyy and 'y' is yy)", dateFmt)
	} else {
		switch dateFmt {
		case "Ymd":
			return enums.DATEFMT_YYYY_MM_DD, nil
		case "ymd":
			return enums.DATEFMT_YY_MM_DD, nil
		case "Ydm":
			return enums.DATEFMT_YYYY_DD_MM, nil
		case "ydm":
			return enums.DATEFMT_YY_DD_MM, nil
		case "dmY":
			return enums.DATEFMT_DD_MM_YYYY, nil
		case "dmy":
			return enums.DATEFMT_DD_MM_YY, nil
		case "mdY":
			return enums.DATEFMT_MM_DD_YYYY, nil
		case "mdy":
			return enums.DATEFMT_MM_DD_YY, nil
		case "md":
			return enums.DATEFMT_MM_DD, nil
		case "dm":
			return enums.DATEFMT_DD_MM, nil
		}
	}
	return enums.DATEFMT_SKIP, fmt.Errorf("invalid date format entered as '%s', please enter up to three ymd characters (where capital Y is yyyy and y is yy)", dateFmt)
}
package config

import (
	"github.com/spf13/viper"
)

// Set sets the value for the key in the override register. Set is case-insensitive for a key. Will be used instead of values obtained via flags, config file, ENV, default, or key/value store.
func Set(key string, value any) {

	viper.Set(key, value)
}

// Get can retrieve any value given the key to use. Get is case-insensitive for a key. Get has the behavior of returning the value associated with the first place from where it is set. Viper will check in the following order: override, flag, env, config file, key/value store, default
// Get returns an interface. For a specific value use one of the Get____ methods.
func Get(key string) any {
	return viper.Get(key)
}

// GetBool returns the value associated with the key as a boolean.
func GetBool(key string) bool {
	return viper.GetBool(key)
}

// GetInt returns the value associated with the key as an integer.
func GetInt(key string) int {
	return viper.GetInt(key)
}

// GetUint64 returns the value associated with the key as an unsigned integer.
func GetUint64(key string) uint64 {
	return viper.GetUint64(key)
}

// GetFloat64 returns the value associated with the key as a float64.
func GetFloat64(key string) float64 {
	return viper.GetFloat64(key)
}

// GetString returns the value associated with the key as a string.
func GetString(key string) string {
	return viper.GetString(key)
}

// GetStringSlice returns the value associated with the key as a slice of strings.
func GetStringSlice(key string) []string {
	return viper.GetStringSlice(key)
}

// IsSet checks to see if the key has been set in any of the data locations.
// IsSet is case-insensitive for a key.
func IsSet(key string) bool {
	return viper.IsSet(key)
}
package dates

import (
	"fmt"
	"metarr/internal/models"
	logging "metarr/internal/utils/logging"
	"strconv"
	"strings"

	"github.com/araddon/dateparse"
)

// ParseAndFormatDate parses and formats the inputted date string
func ParseStringDate(dateString string) (string, error) {

	t, err := dateparse.ParseAny(dateString)
	if err != nil {
		return "", fmt.Errorf("unable to parse date: %s", dateString)
	}

	return t.Format("2006-01-02"), nil
}

// ParseAndFormatDate parses and formats the inputted date string
func ParseNumDate(dateNum string) (string, error) {

	t, err := dateparse.ParseAny(dateNum)
	if err != nil {
		return "", fmt.Errorf("unable to parse date '%s' to word date", dateNum)
	}
	time := t.Format("01022006")
	if time == "" {
		time = t.Format("010206")
	}

	var day, month, year, dateStr string

	if len(time) < 6 {
		return dateNum, fmt.Errorf("unable to parse date, date '%s' is too short", time)
	}

	if len(time) >= 8 {
		day = time[2:4]
		month = time[:2]
		year = time[4:8]
	} else if len(time) >= 6 {
		day = time[2:4]
		month = time[:2]
		year = time[4:6]
	}

	month = monthStringSwitch(month)
	day = dayStringSwitch(day)

	dateStr = month + " " + day + ", " + year
	logging.S(1, "Made string form date: '%s'", dateStr)

	return dateStr, nil
}

// Convert a numerical month to a word
func monthStringSwitch(month string) string {
	var monthStr string
	switch month {
	case "01":
		monthStr = "Jan"
	case "02":
		monthStr = "Feb"
	case "03":
		monthStr = "Mar"
	case "04":
		monthStr = "Apr"
	case "05":
		monthStr = "May"
	case "06":
		monthStr = "Jun"
	case "07":
		monthStr = "Jul"
	case "08":
		monthStr = "Aug"
	case "09":
		monthStr = "Sep"
	case "10":
		monthStr = "Oct"
	case "11":
		monthStr = "Nov"
	case "12":
		monthStr = "Dec"
	default:
		logging.E(0, "Failed to make month string from month number '%s'", month)
		monthStr = "Jan"
	}
	return monthStr
}

// Affix a numerical day with the appropriate suffix (e.g. '1st', '2nd', '3rd')
func dayStringSwitch(day string) string {
	if thCheck, err := strconv.Atoi(day); err != nil {
		logging.E(0, "Failed to convert date string to number")
		return day
	} else if thCheck > 10 && thCheck < 20 {
		return day + "th"
	}
	switch {
	case strings.HasSuffix(day, "1"):
		return day + "st"
	case strings.HasSuffix(day, "2"):
		return day + "nd"
	case strings.HasSuffix(day, "3"):
		return day + "rd"
	default:
		return day + "th"
	}
}

// YyyyMmDd converts inputted date strings into the user's defined format
func YyyyMmDd(date string) (string, bool) {

	var t string = ""
	if tIdx := strings.Index(date, "T"); tIdx != -1 {
		t = date[tIdx:]
	}

	date = strings.ReplaceAll(date, "-", "")

	if len(date) >= 8 {
		formatted := fmt.Sprintf("%s-%s-%s%s", date[:4], date[4:6], date[6:8], t)
		logging.S(2, "Made date %s", formatted)
		return formatted, true

	} else if len(date) >= 6 {
		formatted := fmt.Sprintf("%s-%s-%s%s", date[:2], date[2:4], date[4:6], t)
		logging.S(2, "Made date %s", formatted)
		return formatted, true
	}
	logging.D(3, "Returning empty or short date element (%s) without formatting", date)
	return date, false
}

// FormatAllDates formats timestamps into a hyphenated form
func FormatAllDates(fd *models.FileData) string {

	var (
		result string
		ok     bool
	)

	d := fd.MDates

	if !ok && d.Originally_Available_At != "" {
		logging.D(2, "Attempting to format originally available date: %v", d.Originally_Available_At)
		result, ok = YyyyMmDd(d.Originally_Available_At)
	}
	if !ok && d.ReleaseDate != "" {
		logging.D(2, "Attempting to format release date: %v", d.ReleaseDate)
		result, ok = YyyyMmDd(d.ReleaseDate)
	}
	if !ok && d.Date != "" {
		logging.D(2, "Attempting to format date: %v", d.Date)
		result, ok = YyyyMmDd(d.Date)
	}
	if !ok && d.UploadDate != "" {
		logging.D(2, "Attempting to format upload date: %v", d.UploadDate)
		result, ok = YyyyMmDd(d.UploadDate)
	}
	if !ok && d.Creation_Time != "" {
		logging.D(3, "Attempting to format creation time: %v", d.Creation_Time)
		result, ok = YyyyMmDd(d.Creation_Time)
	}
	if !ok {
		logging.E(0, "Failed to format dates")
		return ""
	} else {
		logging.D(2, "Exiting with formatted date: %v", result)

		d.FormattedDate = result

		logging.D(2, "Got formatted date '%s' and entering parse to string function...", result)

		var err error
		d.StringDate, err = ParseNumDate(d.FormattedDate)
		if err != nil {
			logging.E(0, err.Error())
		}

		return result
	}
}
package domain

const (
	BrowserChrome  = "chrome"
	BrowserEdge    = "edge"
	BrowserFirefox = "firefox"
	BrowserSafari  = "safari"
)
package domain

// Colors
const (
	ColorReset  = "\033[0m"
	ColorRed    = "\033[91m"
	ColorGreen  = "\033[92m"
	ColorYellow = "\033[93m"
	ColorBlue   = "\033[34m"
	ColorPurple = "\033[35m"
	ColorCyan   = "\033[96m"
	ColorWhite  = "\033[37m"
)

const (
	RedError     string = ColorRed + "[ERROR] " + ColorReset
	GreenSuccess string = ColorGreen + "[Success] " + ColorReset
	YellowDebug  string = ColorYellow + "[Debug] " + ColorReset
	BlueInfo     string = ColorCyan + "[Info] " + ColorReset
)
package domain

// File prefix and suffix
const (
	OldTag  = "_metarrbackup"
	TempTag = "tmp_"
)

var (
	AllVidExtensions = [...]string{".3gp", ".avi", ".f4v", ".flv", ".m4v",
		".mkv", ".mov", ".mp4", ".mpeg", ".mpg",
		".ogm", ".ogv", ".ts", ".vob", ".webm",
		".wmv"}
)

var (
	AllMetaExtensions = [...]string{".json", ".nfo"}
)

// Webpage tags
var (
	// Ensure lengths match
	WebDateTags        = [...]string{"release-date", "upload-date", "date", "date-text", "text-date"}
	WebDescriptionTags = [...]string{"description", "longdescription", "long-description", "summary", "synopsis",
		"check-for-urls"}

	// Credits tags, and nested elements
	WebCreditsTags      = [...]string{"creator", "uploader", "uploaded-by", "uploaded_by", "channel-name", "claim-preview__title"}
	WebCreditsSelectors = map[string]string{
		"claim-preview__title":               "truncated-text",
		`script[type="application/ld+json"]`: "author.name",
	}

	WebTitleTags = [...]string{"video-title", "video-name"}
)
package domain

var (
	ContractionsSpaced = map[string]string{
		"ain t":     "aint",
		"can t":     "cant",
		"don t":     "dont",
		"didn t":    "didnt",
		"hasn t":    "hasnt",
		"haven t":   "havent",
		"won t":     "wont",
		"wouldn t":  "wouldnt",
		"shouldn t": "shouldnt",
		"couldn t":  "couldnt",
		"wasn t":    "wasnt",
		"weren t":   "werent",
		"let s":     "lets",
		"hadn t":    "hadnt",
		"who s":     "whos",
		"what s":    "whats",
		"when s":    "whens",
		"where s":   "wheres",
		"why s":     "whys",
		"how s":     "hows",
		"there s":   "theres",
		"that s":    "thats",
		"it d":      "itd",
		"she d":     "shed",
		"she s":     "shes",
		"he d":      "hed",
		"he s":      "hes",
		"it ll":     "itll",
		"should ve": "shouldve",
		"could ve":  "couldve",
		"would ve":  "wouldve",
	}

	ContractionsUnderscored = map[string]string{
		"ain_t":     "aint",
		"can_t":     "cant",
		"don_t":     "dont",
		"didn_t":    "didnt",
		"hasn_t":    "hasnt",
		"haven_t":   "havent",
		"won_t":     "wont",
		"wouldn_t":  "wouldnt",
		"shouldn_t": "shouldnt",
		"couldn_t":  "couldnt",
		"wasn_t":    "wasnt",
		"weren_t":   "werent",
		"let_s":     "lets",
		"hadn_t":    "hadnt",
		"who_s":     "whos",
		"what_s":    "whats",
		"when_s":    "whens",
		"where_s":   "wheres",
		"why_s":     "whys",
		"how_s":     "hows",
		"there_s":   "theres",
		"that_s":    "thats",
		"it_d":      "itd",
		"she_d":     "shed",
		"she_s":     "shes",
		"he_d":      "hed",
		"he_s":      "hes",
		"it_ll":     "itll",
		"should_ve": "shouldve",
		"could_ve":  "couldve",
		"would_ve":  "wouldve",
	}
)
package domain

// Video extensions
const (
	Ext3GP  = ".3gp"
	Ext3G2  = ".3g2"
	ExtASF  = ".asf"
	ExtAVI  = ".avi"
	ExtFLV  = ".flv"
	ExtM4V  = ".m4v"
	ExtMKV  = ".mkv"
	ExtMOV  = ".mov"
	ExtMP4  = ".mp4"
	ExtMPEG = ".mpeg"
	ExtMPG  = ".mpg"
	ExtMTS  = ".mts"
	ExtRM   = ".rm"
	ExtRMVB = ".rmvb"
	ExtTS   = ".ts"
	ExtWEBM = ".webm"
)

// Metafile extensions
const (
	MExtJSON = ".json"
	MExtNFO  = ".nfo"
)
package domain

// AV codec copy
var (
	AVCodecCopy = [...]string{"-codec", "copy"}
)

// Audio flags
var (
	AudioCodecCopy = [...]string{"-c:a", "copy"}
	AudioToAAC     = [...]string{"-c:a", "aac"}
	AudioBitrate   = [...]string{"-b:a", "256k"}
)

// Video flags
var (
	VideoCodecCopy      = [...]string{"-c:v", "copy"}
	VideoToH264Balanced = [...]string{"-c:v", "libx264", "-crf", "23", "-profile:v", "main"}
	PixelFmtYuv420p     = [...]string{"-pix_fmt", "yuv420p"}
	KeyframeBalanced    = [...]string{"-g", "50", "-keyint_min", "30"}
)

// GPU hardware flags
var (
	NvidiaAccel = [...]string{"-hwaccel", "nvdec"}
	AMDAccel    = [...]string{"-hwaccel", "vulkan"}
	IntelAccel  = [...]string{"-hwaccel", "qsv"}
)
package domain

const (
	JOverrideCredits = "all-credits"
	JActor           = "actor"
	JAuthor          = "author"
	JArtist          = "artist"
	JChannel         = "channel"
	JComposer        = "composer"
	JCreator         = "creator"
	JDirector        = "director"
	JPerformer       = "performer"
	JProducer        = "producer"
	JPublisher       = "publisher"
	JStudio          = "studio"
	JUploader        = "uploader"
	JWriter          = "writer"
)

const (
	JComment          = "comment"
	JDescription      = "description"
	JFulltitle        = "fulltitle"
	JLongDescription  = "longdescription"
	JLong_Description = "long_description"
	JSubtitle         = "subtitle"
	JSummary          = "summary"
	JSynopsis         = "synopsis"
	JTitle            = "title"
)

const (
	JCreationTime        = "creation_time"
	JDate                = "date"
	JFormattedDate       = "formatted_date"
	JOriginallyAvailable = "originally_available_at"
	JReleaseDate         = "release_date"
	JUploadDate          = "upload_date"
	JYear                = "year"
	JReleaseYear         = "release_year"
)

const (
	JDomain        = "domain"
	JReferer       = "referer"
	JURL           = "url"
	JWebpageDomain = "webpage_url_domain"
	JWebpageURL    = "webpage_url"
)
package domain

// Log file keys
const (
	LogFinished = "FINISHED: "
	LogError    = "ERROR: "
	LogFailure  = "FAILED: "
	LogSuccess  = "Success: "
	LogInfo     = "Info: "
	LogWarning  = "Warning: "
	LogBasic    = ""
)
package domain

const (
	// Core Descriptive Metadata
	NTitle         = "title"
	NOriginalTitle = "originaltitle"
	NSortTitle     = "sorttitle"
	NTagline       = "tagline"
	NDescription   = "description"
	NPlot          = "plot"
	NOutline       = "outline"
	NShowTitle     = "showtitle"
	NSubtitle      = "subtitle"

	// Cast and Crew Metadata
	NActors   = "actor"
	NDirector = "director"
	NWriter   = "writer"
	NComposer = "composer"
	NProducer = "producer"

	// Genre, Category, and Rating Metadata
	NGenre      = "genre"
	NMood       = "mood"
	NMPAA       = "mpaa"
	NVotes      = "votes"
	NRatingsURL = "ratingurl"

	// Date and Release Metadata
	NAired        = "aired"
	NPremiereDate = "premiered"
	NYear         = "year"

	// Episodic Metadata
	NSeason       = "season"
	NEpisode      = "episode"
	NEpisodeTitle = "episodetitle"

	// Technical Information
	NCountry   = "country"
	NLanguage  = "language"
	NRated     = "rated"
	NEncodedBy = "encodedby"
	NRuntime   = "runtime"
	NRating    = "rating"

	// Production Metadata
	NProductionCompany = "productioncompany"
	NStudio            = "studio"
	NCoverArtist       = "coverartist"
	NPublisher         = "publisher"
	NCompilation       = "compilation"

	// Artwork, Media Assets, and Related Links
	NThumb    = "thumb"
	NFanart   = "fanart"
	NTrailer  = "trailer"
	NCoverArt = "cover_art"

	// Sorting and Alternate Display Titles
	NShowSortTitle = "showsorttitle"

	// Miscellaneous
	NComment = "comment"
	NTop250  = "top250"
	NTrack   = "track"
	NAlbum   = "album"
	NLicence = "license"
	NRights  = "rights"
	NURL     = "url"
)
package domain

// User selection of filetypes to convert from
type ConvertFromFiletype int

const (
	VID_EXTS_ALL ConvertFromFiletype = iota
	VID_EXTS_MKV
	VID_EXTS_MP4
	VID_EXTS_WEBM
)

// MetaFiletypeFilter filters the metadata files to read from
type MetaFiletypeFilter int

const (
	META_EXTS_ALL MetaFiletypeFilter = iota
	META_EXTS_JSON
	META_EXTS_NFO
)

// User system graphics hardware for transcoding
type SysGPU int

const (
	GPU_NO_HW_ACCEL SysGPU = iota
	GPU_AMD
	GPU_INTEL
	GPU_NVIDIA
)

// Naming syle
type ReplaceToStyle int

const (
	RENAMING_SKIP ReplaceToStyle = iota
	RENAMING_UNDERSCORES
	RENAMING_FIXES_ONLY
	RENAMING_SPACES
)

// Date formats
type DateFormat int

const (
	DATEFMT_SKIP DateFormat = iota
	DATEFMT_YYYY_MM_DD
	DATEFMT_YY_MM_DD
	DATEFMT_YYYY_DD_MM
	DATEFMT_YY_DD_MM
	DATEFMT_DD_MM_YYYY
	DATEFMT_DD_MM_YY
	DATEFMT_MM_DD_YYYY
	DATEFMT_MM_DD_YY
	DATEFMT_DD_MM
	DATEFMT_MM_DD
)

// Date tag location
type MetaDateTagLocation int

const (
	DATE_TAG_LOC_PFX MetaDateTagLocation = iota
	DATE_TAG_LOC_SFX
)

type MetaDateTaggingType int

const (
	DATE_TAG_ADD_OP MetaDateTaggingType = iota
	DATE_TAG_DEL_OP
)

// Web tags
type MetaFiletypeFound int

const (
	METAFILE_JSON MetaFiletypeFound = iota
	METAFILE_NFO
)

// Viper variable types
type ViperVarTypes int

const (
	VIPER_ANY ViperVarTypes = iota
	VIPER_BOOL
	VIPER_INT
	VIPER_STRING
	VIPER_STRING_SLICE
)

// Purge metafile types
type PurgeMetafiles int

const (
	PURGEMETA_ALL PurgeMetafiles = iota
	PURGEMETA_JSON
	PURGEMETA_NFO
	PURGEMETA_NONE
)

// Web tags
type WebClassTags int

const (
	WEBCLASS_DATE WebClassTags = iota
	WEBCLASS_TITLE
	WEBCLASS_DESCRIPTION
	WEBCLASS_CREDITS
	WEBCLASS_WEBINFO
)

// Presets
type SitePresets int

const (
	PRESET_CENSOREDTV SitePresets = iota
)
package domain

// Terminal keys
const (
	VideoDir  string = "video-dir"
	VideoFile string = "video-file"

	JsonDir   string = "json-dir"
	JsonFile  string = "json-file"
	MetaPurge string = "purge-metafile"

	CookiePath string = "cookie-dir"

	InputMetaExts  string = "input-meta-exts"
	InputVideoExts string = "input-video-exts"
	FilePrefixes   string = "prefix"

	Concurrency string = "concurrency"
	GPU         string = "gpu"
	MaxCPU      string = "max-cpu"
	MinMem      string = "min-mem"
	MinMemMB    string = "min-mem-mb"

	InputFilenameReplaceSfx string = "filename-replace-suffix"
	InputFileDatePfx        string = "filename-date-tag"
	RenameStyle             string = "input-rename-style"
	MFilenamePfx            string = "metadata-filename-prefix"

	MetaOps      string = "meta-ops"
	MDescDatePfx string = "desc-date-prefix"
	MDescDateSfx string = "desc-date-suffix"

	DebugLevel      string = "debug-level"
	SkipVideos      string = "skip-videos"
	NoFileOverwrite string = "no-file-overwrite"

	Benchmarking        string = "benchmark"
	OutputFiletypeInput string = "ext"
	MoveOnComplete      string = "output-directory"
	InputPreset         string = "preset"
)

// Primary program
const (
	Context    string = "Context"
	WaitGroup  string = "WaitGroup"
	SingleFile string = "SingleFile"
)

// Files and directories
const (
	OpenVideo      string = "openVideo"
	OpenJson       string = "openJson"
	VideoMap       string = "videoMap"
	MetaMap        string = "metaMap"
	MetaPurgeEnum  string = "metaPurgeEnum"
	OutputFiletype string = "outputFiletype"
)

// Filter for files
const (
	InputVExtsEnum string = "inputVideoExtsEnum"
	InputMExtsEnum string = "inputMetaExtsEnum"
)

// Performance
const (
	GPUEnum string = "gpuEnum"
)

// Filename edits
const (
	Rename             string = "Rename"
	FileDateFmt        string = "filenameDateTag"
	FilenameReplaceSfx string = "filenameReplaceSfx"
)

// Meta edits
const (
	MOverwrite string = "meta-overwrite"
	MPreserve  string = "meta-preserve"

	MAppend      string = "metaAppend"
	MNewField    string = "metaNewField"
	MPrefix      string = "metaPrefix"
	MReplaceText string = "metaReplaceText"
	MTrimPrefix  string = "metaTrimPrefix"
	MTrimSuffix  string = "metaTrimSuffix"

	MDateTagMap    string = "metaDateTagMap"
	MDelDateTagMap string = "metaDelDateTagMap"
)

// Contains the fields which accept multiple entries as string arrays
var MultiEntryFields = []string{
	InputVideoExts,
	InputMetaExts,
	FilePrefixes,
	FilenameReplaceSfx,
}
package regex

import (
	consts "metarr/internal/domain/constants"
	"metarr/internal/models"
	"regexp"
)

var (
	AnsiEscape   *regexp.Regexp
	ExtraSpaces  *regexp.Regexp
	InvalidChars *regexp.Regexp
	SpecialChars *regexp.Regexp

	ContractionMapSpaced      map[string]*models.ContractionPattern
	ContractionMapUnderscored map[string]*models.ContractionPattern
	ContractionMapAll         map[string]*models.ContractionPattern
)

// ContractionMapAllCompile compiles the regex pattern for spaced AND underscored contractions and returns
// a model containing the regex and the replacement
func ContractionMapAllCompile() map[string]*models.ContractionPattern {
	if ContractionMapAll == nil {
		ContractionMapAll = make(map[string]*models.ContractionPattern, len(consts.ContractionsSpaced)+len(consts.ContractionsUnderscored))

		// Spaced map
		for contraction, replacement := range consts.ContractionsSpaced {

			ContractionMapAll[contraction] = &models.ContractionPattern{
				Regexp:      regexp.MustCompile(`\b` + regexp.QuoteMeta(contraction) + `\b`),
				Replacement: replacement,
			}
			ContractionMapAll[contraction].Replacement = replacement
		}

		// Underscored map
		for contraction, replacement := range consts.ContractionsUnderscored {

			ContractionMapAll[contraction] = &models.ContractionPattern{
				Regexp:      regexp.MustCompile(`\b` + regexp.QuoteMeta(contraction) + `\b`),
				Replacement: replacement,
			}
			ContractionMapAll[contraction].Replacement = replacement
		}
	}

	return ContractionMapAll
}

// ContractionMapSpacesCompile compiles the regex pattern for spaced contractions and returns
// a model containing the regex and the replacement
func ContractionMapSpacesCompile() map[string]*models.ContractionPattern {
	if ContractionMapSpaced == nil {
		ContractionMapSpaced = make(map[string]*models.ContractionPattern, len(consts.ContractionsSpaced))

		for contraction, replacement := range consts.ContractionsSpaced {

			ContractionMapSpaced[contraction] = &models.ContractionPattern{
				Regexp:      regexp.MustCompile(`\b` + regexp.QuoteMeta(contraction) + `\b`),
				Replacement: replacement,
			}
			ContractionMapSpaced[contraction].Replacement = replacement
		}
	}
	return ContractionMapSpaced
}

// ContractionMapUnderscoresCompile compiles the regex pattern for underscored contractions and returns
// a model containing the regex and the replacement
func ContractionMapUnderscoresCompile() map[string]*models.ContractionPattern {
	if ContractionMapUnderscored == nil {
		ContractionMapUnderscored = make(map[string]*models.ContractionPattern, len(consts.ContractionsUnderscored))

		for contraction, replacement := range consts.ContractionsUnderscored {

			ContractionMapUnderscored[contraction] = &models.ContractionPattern{
				Regexp:      regexp.MustCompile(`\b` + regexp.QuoteMeta(contraction) + `\b`),
				Replacement: replacement,
			}
			ContractionMapUnderscored[contraction].Replacement = replacement
		}
	}
	return ContractionMapUnderscored
}

// AnsiEscapeCompile compiles regex for ANSI escape codes
func AnsiEscapeCompile() *regexp.Regexp {
	if AnsiEscape == nil {
		AnsiEscape = regexp.MustCompile(`\x1b\[[0-9;]*m`)
	}
	return AnsiEscape
}

// ExtraSpacesCompile compiles regex for extra spaces
func ExtraSpacesCompile() *regexp.Regexp {
	if ExtraSpaces == nil {
		ExtraSpaces = regexp.MustCompile(`\s+`)
	}
	return ExtraSpaces
}

// InvalidCharsCompile compiles regex for invalid characters
func InvalidCharsCompile() *regexp.Regexp {
	if InvalidChars == nil {
		InvalidChars = regexp.MustCompile(`[<>:"/\\|?*\x00-\x1F]`)
	}
	return InvalidChars
}

// SpecialCharsCompile compiles regex for special characters
func SpecialCharsCompile() *regexp.Regexp {
	if SpecialChars == nil {
		SpecialChars = regexp.MustCompile(`[^\w\s-]`)
	}
	return SpecialChars
}
package ffmpeg

import (
	"fmt"
	"metarr/internal/config"
	consts "metarr/internal/domain/constants"
	enums "metarr/internal/domain/enums"
	keys "metarr/internal/domain/keys"
	"metarr/internal/models"
	logging "metarr/internal/utils/logging"
	"path/filepath"
	"strings"
)

// ffCommandBuilder handles FFmpeg command construction
type ffCommandBuilder struct {
	inputFile   string
	outputFile  string
	formatFlags []string
	gpuAccel    []string
	metadataMap map[string]string
}

// newFfCommandBuilder creates a new FFmpeg command builder
func newFfCommandBuilder(fd *models.FileData, outputFile string) *ffCommandBuilder {
	return &ffCommandBuilder{
		inputFile:   fd.OriginalVideoPath,
		outputFile:  outputFile,
		metadataMap: make(map[string]string),
	}
}

// buildCommand constructs the complete FFmpeg command
func (b *ffCommandBuilder) buildCommand(fd *models.FileData, outExt string) ([]string, error) {

	b.setGPUAcceleration()
	b.addAllMetadata(fd)
	b.setFormatFlags(outExt)

	// Return the fully appended argument string
	return b.buildFinalCommand()
}

// addAllMetadata combines all metadata into a single map
func (b *ffCommandBuilder) addAllMetadata(fd *models.FileData) {

	b.addTitlesDescs(fd.MTitleDesc)
	b.addCredits(fd.MCredits)
	b.addDates(fd.MDates)
	b.addShowInfo(fd.MShowData)
	b.addOtherMetadata(fd.MOther)
}

// addTitlesDescs adds all title/description-related metadata
func (b *ffCommandBuilder) addTitlesDescs(t *models.MetadataTitlesDescs) {

	// Prefer fulltitle if possible (also exists in the JSON processing func)
	if t.Title == "" && t.Fulltitle != "" {
		t.Title = t.Fulltitle
	}

	if t.LongDescription == "" && t.Long_Description != "" {
		t.LongDescription = t.Long_Description
	}

	fields := map[string]string{
		"title":           t.Title,
		"subtitle":        t.Subtitle,
		"description":     t.Description,
		"longdescription": t.LongDescription,
		"summary":         t.Summary,
		"synopsis":        t.Synopsis,
	}

	for field, value := range fields {
		if value != "" {
			b.metadataMap[field] = value
		}
	}
}

// addCredits adds all credit-related metadata
func (b *ffCommandBuilder) addCredits(c *models.MetadataCredits) {

	// Single value credits
	fields := map[string]string{
		"actor":     c.Actor,
		"author":    c.Author,
		"artist":    c.Artist,
		"creator":   c.Creator,
		"studio":    c.Studio,
		"publisher": c.Publisher,
		"producer":  c.Producer,
		"performer": c.Performer,
		"composer":  c.Composer,
		"director":  c.Director,
		"writer":    c.Writer,
	}

	for field, value := range fields {
		if value != "" {
			b.metadataMap[field] = value
		}
	}

	// Array credits (length already checked in function)
	b.addArrayMetadata("actor", c.Actors)
	b.addArrayMetadata("composer", c.Composers)
	b.addArrayMetadata("artist", c.Artists)
	b.addArrayMetadata("studio", c.Studios)
	b.addArrayMetadata("performer", c.Performers)
	b.addArrayMetadata("producer", c.Producers)
	b.addArrayMetadata("publisher", c.Publishers)
	b.addArrayMetadata("director", c.Directors)
	b.addArrayMetadata("writer", c.Writers)
}

// addDates adds all date-related metadata
func (b *ffCommandBuilder) addDates(d *models.MetadataDates) {

	fields := map[string]string{
		"creation_time":           d.Creation_Time,
		"date":                    d.Date,
		"originally_available_at": d.Originally_Available_At,
		"release_date":            d.ReleaseDate,
		"upload_date":             d.UploadDate,
		"year":                    d.Year,
	}

	for field, value := range fields {
		if value != "" {
			b.metadataMap[field] = value
		}
	}
}

// addShowInfo adds all show info related metadata
func (b *ffCommandBuilder) addShowInfo(s *models.MetadataShowData) {

	fields := map[string]string{
		"episode_id":    s.Episode_ID,
		"episode_sort":  s.Episode_Sort,
		"season_number": s.Season_Number,
		"season_title":  s.Season_Title,
		"show":          s.Show,
	}

	for field, value := range fields {
		if value != "" {
			b.metadataMap[field] = value
		}
	}
}

// addOtherMetadata adds other related metadata
func (b *ffCommandBuilder) addOtherMetadata(o *models.MetadataOtherData) {

	fields := map[string]string{
		"genre":    o.Genre,
		"hd_video": o.HD_Video,
		"language": o.Language,
	}

	for field, value := range fields {
		if value != "" {
			b.metadataMap[field] = value
		}
	}
}

// addArrayMetadata combines array values with existing metadata
func (b *ffCommandBuilder) addArrayMetadata(key string, values []string) {
	if len(values) == 0 {
		return
	}

	existing, exists := b.metadataMap[key]
	newValue := strings.Join(values, "; ")

	if exists && existing != "" {
		b.metadataMap[key] = existing + "; " + newValue
	} else {
		b.metadataMap[key] = newValue
	}
}

// setGPUAcceleration sets appropriate GPU acceleration flags
func (b *ffCommandBuilder) setGPUAcceleration() {
	if config.IsSet(keys.GPUEnum) {
		gpuFlag, ok := config.Get(keys.GPUEnum).(enums.SysGPU)
		if ok {
			switch gpuFlag {
			case enums.GPU_NVIDIA:
				b.gpuAccel = consts.NvidiaAccel[:]
			case enums.GPU_AMD:
				b.gpuAccel = consts.AMDAccel[:]
			case enums.GPU_INTEL:
				b.gpuAccel = consts.IntelAccel[:]
			}
		}
	}
}

// setFormatFlags adds commands specific for the extension input and output
func (b *ffCommandBuilder) setFormatFlags(outExt string) {
	inExt := strings.ToLower(filepath.Ext(b.inputFile))
	outExt = strings.ToLower(outExt)

	if outExt == "" || strings.EqualFold(inExt, outExt) {
		b.formatFlags = copyPreset.flags
		return
	}

	logging.I("Input extension: '%s', output extension: '%s', File: %s",
		inExt, outExt, b.inputFile)

	// Get format preset from map
	if presets, exists := formatMap[outExt]; exists {
		// Try exact input format match
		if preset, exists := presets[inExt]; exists {
			b.formatFlags = preset.flags
			return
		}
		// Fall back to default preset for this output format
		if preset, exists := presets["*"]; exists {
			b.formatFlags = preset.flags
			return
		}
	}

	// Fall back to copy preset if no mapping found
	b.formatFlags = copyPreset.flags
	logging.D(1, "No format mapping found for %s to %s conversion, using copy preset",
		inExt, outExt)
}

// buildFinalCommand assembles the final FFmpeg command
func (b *ffCommandBuilder) buildFinalCommand() ([]string, error) {

	args := make([]string, 0, calculateCommandCapacity(b))

	if len(b.gpuAccel) > 0 {
		args = append(args, b.gpuAccel...)
	}

	if b.inputFile != "" {
		args = append(args, "-y", "-i", b.inputFile)
	}

	// Add all -metadata commands
	for key, value := range b.metadataMap {
		args = append(args, "-metadata", fmt.Sprintf("%s=%s", key, strings.TrimSpace(value)))
	}

	if len(b.formatFlags) > 0 {
		args = append(args, b.formatFlags...)
	}

	if b.outputFile != "" {
		args = append(args, b.outputFile)
	}

	return args, nil
}

// calculateCommandCapacity determines the total length needed for the command
func calculateCommandCapacity(b *ffCommandBuilder) int {
	const (
		inputFlags   = 2 // "-y", "-i"
		inputFile    = 1 // input file
		formatFlag   = 1 // "-codec"
		outputFile   = 1 // output file
		metadataFlag = 1 // "-metadata" for each metadata entry
		keyValuePair = 1 // "key=value" for each metadata entry
	)

	totalCapacity := len(b.gpuAccel) + // GPU acceleration flags if any
		inputFlags + inputFile + // Input related flags and file
		(len(b.metadataMap) * (metadataFlag + keyValuePair)) + // Metadata entries
		len(b.formatFlags) + // Format flags (like -codec copy)
		outputFile

	logging.D(3, "Total command capacity calculated as: %d", totalCapacity)
	return totalCapacity
}
package ffmpeg

import (
	consts "metarr/internal/domain/constants"
	logging "metarr/internal/utils/logging"
)

// formatPreset holds a pre-calculated set of ffmpeg flags
type formatPreset struct {
	flags []string
}

var (
	// Direct copy preset
	copyPreset = formatPreset{
		flags: consts.AVCodecCopy[:],
	}

	// Standard h264 conversion
	h264Preset = formatPreset{
		flags: concat(
			consts.VideoToH264Balanced[:],
			consts.PixelFmtYuv420p[:],
			consts.AudioToAAC[:],
			consts.AudioBitrate[:],
		),
	}

	// Video copy with AAC audio
	videoCopyAACPreset = formatPreset{
		flags: concat(
			consts.VideoCodecCopy[:],
			consts.AudioToAAC[:],
			consts.AudioBitrate[:],
		),
	}

	// Full webm conversion preset
	webmPreset = formatPreset{
		flags: concat(
			consts.VideoToH264Balanced[:],
			consts.PixelFmtYuv420p[:],
			consts.KeyframeBalanced[:],
			consts.AudioToAAC[:],
			consts.AudioBitrate[:],
		),
	}
)

var formatMap = map[string]map[string]formatPreset{
	consts.ExtAVI: {
		consts.ExtAVI:  copyPreset,
		consts.ExtMP4:  videoCopyAACPreset,
		consts.ExtM4V:  videoCopyAACPreset,
		consts.ExtMOV:  videoCopyAACPreset,
		consts.ExtRM:   webmPreset,
		consts.ExtRMVB: webmPreset,
		"*":            h264Preset, // default preset
	},
	consts.ExtMP4: {
		consts.ExtMP4:  copyPreset,
		consts.ExtMKV:  videoCopyAACPreset,
		consts.ExtWEBM: webmPreset,
		"*":            h264Preset,
	},
	consts.ExtMKV: {
		consts.ExtMKV: copyPreset,
		consts.ExtMP4: videoCopyAACPreset,
		consts.ExtM4V: videoCopyAACPreset,
		"*":           h264Preset,
	},
	consts.ExtWEBM: {
		consts.ExtWEBM: copyPreset,
		"*":            webmPreset,
	},
}

// concat combines multiple string slices into one
func concat(slices ...[]string) []string {
	var totalLen int
	for _, s := range slices {
		totalLen += len(s)
	}

	result := make([]string, 0, totalLen)
	for _, s := range slices {
		result = append(result, s...)
	}

	logging.D(2, "Made format flag array %v", result)
	return result
}
package ffmpeg

import (
	"fmt"
	"metarr/internal/config"
	consts "metarr/internal/domain/constants"
	keys "metarr/internal/domain/keys"
	"metarr/internal/models"
	backup "metarr/internal/utils/fs/backup"
	logging "metarr/internal/utils/logging"
	validate "metarr/internal/utils/validation"
	"os"
	"os/exec"
	"path/filepath"
	"strings"
)

// executeVideo writes metadata to a single video file
func ExecuteVideo(fd *models.FileData) error {
	var (
		tmpOutPath, outExt string
	)

	origPath := fd.OriginalVideoPath
	origExt := filepath.Ext(origPath)

	// Extension validation - now checks length and format immediately
	if config.IsSet(keys.OutputFiletype) {
		if outExt = validate.ValidateExtension(config.GetString(keys.OutputFiletype)); outExt == "" {
			logging.E(0, "Grabbed output extension but extension was empty/invalid, reverting to original: %s", origExt)
			outExt = origExt
		}
	} else {
		if origExt != "" && strings.HasPrefix(origExt, ".") {
			outExt = origExt
			config.Set(keys.OutputFiletype, outExt)
		} else {
			return fmt.Errorf("unable to set file extension, malformed? Input: %s, Output: %s", origExt, outExt)
		}
	}

	if dontProcess(fd, outExt) {
		return nil
	}

	fmt.Printf("\nWriting metadata for file: %s\n", origPath)

	dir := fd.VideoDirectory
	fileBase := strings.TrimSuffix(filepath.Base(origPath), origExt)

	// Make temp output path
	tmpOutPath = filepath.Join(dir, consts.TempTag+fileBase+origExt+outExt)
	logging.D(3, "Orig ext: '%s', Out ext: '%s'", origExt, outExt)

	// Add temp path to data struct
	fd.TempOutputFilePath = tmpOutPath

	defer func() {
		if _, err := os.Stat(tmpOutPath); err == nil {
			os.Remove(tmpOutPath)
		}
	}()

	// Build FFmpeg command
	builder := newFfCommandBuilder(fd, tmpOutPath)
	args, err := builder.buildCommand(fd, outExt)
	if err != nil {
		return err
	}

	command := exec.Command("ffmpeg", args...)
	logging.I("%sConstructed FFmpeg command for%s '%s':\n\n%v\n", consts.ColorCyan, consts.ColorReset, fd.OriginalVideoPath, command.String())

	command.Stdout = os.Stdout
	command.Stderr = os.Stderr

	// Set final video path and base name in model
	fd.FinalVideoBaseName = strings.TrimSuffix(filepath.Base(origPath), filepath.Ext(origPath))
	fd.FinalVideoPath = filepath.Join(fd.VideoDirectory, fd.FinalVideoBaseName) + outExt

	logging.I("Video file path data:\n\nOriginal Video Path: %s\nMetadata File Path: %s\nFinal Video Path: %s\n\nTemp Output Path: %s", origPath,
		fd.JSONFilePath,
		fd.FinalVideoPath,
		fd.TempOutputFilePath)

	// Run the ffmpeg command
	logging.P("%s!!! Starting FFmpeg command for '%s'...\n%s", consts.ColorCyan, fd.FinalVideoBaseName, consts.ColorReset)
	if err := command.Run(); err != nil {
		logging.ErrorArray = append(logging.ErrorArray, err)
		return fmt.Errorf("failed to run FFmpeg command: %w", err)
	}

	// Rename temporary file to overwrite the original video file
	if filepath.Ext(origPath) != filepath.Ext(fd.FinalVideoPath) {
		logging.I("Original file not type %s, removing '%s'", outExt, origPath)

	} else if config.GetBool(keys.NoFileOverwrite) && origPath == fd.FinalVideoPath {
		if err := makeBackup(origPath); err != nil {
			return err
		}
	}

	// Delete original after potential backup ops
	err = os.Remove(origPath)
	if err != nil {
		logging.ErrorArray = append(logging.ErrorArray, err)
		return fmt.Errorf("failed to remove original file (%s). Error: %v", origPath, err)
	}

	//
	err = os.Rename(tmpOutPath, fd.FinalVideoPath)
	if err != nil {
		return fmt.Errorf("failed to rename temp file: %w", err)
	}

	logging.S(0, "Successfully processed video:\n\nOriginal file: %s\nNew file: %s\n\nTitle: %s", origPath,
		fd.FinalVideoPath,
		fd.MTitleDesc.Title)

	return nil
}

// dontProcess determines whether the program should process this video (meta already exists and file extensions are unchanged)
func dontProcess(fd *models.FileData, outExt string) (dontProcess bool) {
	if fd.MetaAlreadyExists {

		logging.I("Metadata already exists in the file, skipping processing...")
		origPath := fd.OriginalVideoPath
		fd.FinalVideoBaseName = strings.TrimSuffix(filepath.Base(origPath), filepath.Ext(origPath))

		// Set the final video path based on output extension
		if outExt == "" {
			outExt = filepath.Ext(fd.OriginalVideoPath)
			config.Set(keys.OutputFiletype, outExt)
		}

		// Save final video path into model
		fd.FinalVideoPath = filepath.Join(fd.VideoDirectory, fd.FinalVideoBaseName) + outExt
		return true
	}
	return dontProcess
}

// makeBackup performs the backup
func makeBackup(origPath string) error {

	origInfo, err := os.Stat(origPath)
	if os.IsNotExist(err) {
		logging.I("File does not exist, safe to proceed overwriting: %s", origPath)
		return nil
	}

	backupPath, err := backup.RenameToBackup(origPath)
	if err != nil {
		return fmt.Errorf("failed to rename original file and preserve file is on, aborting: %w", err)
	}

	backInfo, err := os.Stat(backupPath)
	if os.IsNotExist(err) {
		return fmt.Errorf("backup file was not created, aborting")
	}

	if origInfo.Size() != backInfo.Size() {
		return fmt.Errorf("backup file size does not match original, aborting")
	}

	return nil
}
package metadata

import (
	config "metarr/internal/config"
	consts "metarr/internal/domain/constants"
	enums "metarr/internal/domain/enums"
	keys "metarr/internal/domain/keys"
	"metarr/internal/models"
	browser "metarr/internal/utils/browser"
	logging "metarr/internal/utils/logging"
)

// fillCredits fills in the metadator for credits (e.g. actor, director, uploader)
func fillCredits(fd *models.FileData, data map[string]interface{}) (map[string]interface{}, bool) {

	var dataFilled bool

	c := fd.MCredits
	w := fd.MWebData

	fieldMap := map[string]*string{
		// Order by importance
		consts.JOverrideCredits: &c.Override, // Users can assign to this flag to override all credits
		consts.JCreator:         &c.Creator,
		consts.JPerformer:       &c.Performer,
		consts.JAuthor:          &c.Author,
		consts.JArtist:          &c.Artist, // May be alias for "author" in some systems
		consts.JChannel:         &c.Channel,
		consts.JDirector:        &c.Director,
		consts.JActor:           &c.Actor,
		consts.JStudio:          &c.Studio,
		consts.JProducer:        &c.Producer,
		consts.JWriter:          &c.Writer,
		consts.JUploader:        &c.Uploader,
		consts.JPublisher:       &c.Publisher,
		consts.JComposer:        &c.Composer,
	}

	if dataFilled = unpackJSON("credits", fieldMap, data); dataFilled {
		logging.D(2, "Decoded credits JSON into field map")
	}

	// Check if filled
	for key, val := range fieldMap {

		if val == nil {
			logging.E(0, "Value is null")
			continue
		}

		if *val == "" || config.GetBool(keys.MOverwrite) {
			logging.D(2, "Value for '%s' is empty, attempting to fill by inference...", key)
			*val = fillEmptyCredits(c)
			logging.D(2, "Set value to '%s'", *val)
			if *val != "" {
				dataFilled = true
			}
		} else if *val != "" {
			dataFilled = true
		}
	}

	// Return if data filled or no web data, else scrape
	switch {
	case dataFilled:

		rtn, err := fd.JSONFileRW.WriteJSON(fieldMap)
		switch {
		case err != nil:
			logging.E(0, "Failed to write into JSON file '%s': %v", fd.JSONFilePath, err)
			return data, true
		case rtn != nil:
			data = rtn
			return data, true
		}

	case w.WebpageURL == "":

		logging.I("Page URL not found in metadata, so cannot scrape for missing credits in '%s'", fd.JSONFilePath)
		return data, false
	}

	// Scrape for missing data (write back to file if found)
	credits := browser.ScrapeMeta(w, enums.WEBCLASS_CREDITS)
	if credits != "" {
		for _, value := range fieldMap {
			if *value == "" {
				*value = credits
			}
		}

		rtn, err := fd.JSONFileRW.WriteJSON(fieldMap)
		switch {
		case err != nil:
			logging.E(0, "Failed to write new metadata (%s) into JSON file '%s': %v", credits, fd.JSONFilePath, err)
			return data, true
		case rtn != nil:
			data = rtn
			return data, true
		}

	}
	return data, false
}

// fillEmptyCredits attempts to fill empty fields by inference
func fillEmptyCredits(c *models.MetadataCredits) string {

	// Order by importance
	switch {
	case c.Override != "":
		return c.Override

	case c.Creator != "":
		return c.Creator

	case c.Author != "":
		return c.Author

	case c.Publisher != "":
		return c.Publisher

	case c.Producer != "":
		return c.Producer

	case c.Actor != "":
		return c.Actor

	case c.Channel != "":
		return c.Channel

	case c.Performer != "":
		return c.Performer

	case c.Uploader != "":
		return c.Uploader

	case c.Artist != "":
		return c.Artist

	case c.Director != "":
		return c.Director

	case c.Studio != "":
		return c.Studio

	case c.Writer != "":
		return c.Writer

	case c.Composer != "":
		return c.Composer

	default:
		return ""
	}
}
package metadata

import (
	"metarr/internal/dates"
	consts "metarr/internal/domain/constants"
	enums "metarr/internal/domain/enums"
	"metarr/internal/models"
	browser "metarr/internal/utils/browser"
	logging "metarr/internal/utils/logging"
	print "metarr/internal/utils/print"
	"strings"
)

// fillTimestamps grabs timestamp metadata from JSON
func FillTimestamps(fd *models.FileData, data map[string]interface{}) (map[string]interface{}, bool) {
	var (
		err             error
		gotRelevantDate bool
	)

	t := fd.MDates
	w := fd.MWebData

	fieldMap := map[string]*string{ // Order by importance
		consts.JReleaseDate:         &t.ReleaseDate,
		consts.JOriginallyAvailable: &t.Originally_Available_At,
		consts.JDate:                &t.Date,
		consts.JUploadDate:          &t.UploadDate,
		consts.JReleaseYear:         &t.Year,
		consts.JYear:                &t.Year,
		consts.JCreationTime:        &t.Creation_Time,
	}

	if ok := unpackJSON("date", fieldMap, data); !ok {
		logging.E(1, "Failed to unpack date JSON, no dates currently exist in file?")
	}

	printMap := make(map[string]string, len(fieldMap))

	for key, value := range data {
		if strVal, ok := value.(string); ok {
			if _, exists := fieldMap[key]; exists {

				if len(strVal) >= 6 {
					if formatted, ok := dates.YyyyMmDd(strVal); ok {
						*fieldMap[key] = formatted
						printMap[key] = formatted
						gotRelevantDate = true
						continue

					} else {
						*fieldMap[key] = strVal
						printMap[key] = strVal
						gotRelevantDate = true
						continue
					}
				} else {
					*fieldMap[key] = strVal
					printMap[key] = strVal
					gotRelevantDate = true
					continue
				}
			}
		}
		continue
	}

	if fillEmptyTimestamps(t) {
		gotRelevantDate = true
	}

	switch {
	case gotRelevantDate:

		logging.D(3, "Got a relevant date, proceeding...")
		print.PrintGrabbedFields("time and date", &printMap)
		if t.FormattedDate == "" {
			dates.FormatAllDates(fd)
		} else {
			t.StringDate, err = dates.ParseNumDate(t.FormattedDate)
			if err != nil {
				logging.E(0, err.Error())
			}
		}

		rtn, err := fd.JSONFileRW.WriteJSON(fieldMap)
		if err != nil {
			logging.E(0, "Failed to write into JSON file '%s': %v", fd.JSONFilePath, err)
			return data, true
		} else if rtn != nil {
			data = rtn
			return data, true
		}

	case w.WebpageURL == "":

		logging.I("Page URL not found in metadata, so cannot scrape for missing date in '%s'", fd.JSONFilePath)
		print.PrintGrabbedFields("time and date", &printMap)
		return data, false
	}

	scrapedDate := browser.ScrapeMeta(w, enums.WEBCLASS_DATE)
	logging.D(1, "Scraped date: %s", scrapedDate)

	logging.D(3, "Passed web scrape attempt for date.")

	var date string
	if scrapedDate != "" {
		date, err = dates.ParseStringDate(scrapedDate)
		if err != nil || date == "" {
			logging.E(0, "Failed to parse date '%s': %v", scrapedDate, err)
			return data, false
		} else {
			if t.ReleaseDate == "" {
				t.ReleaseDate = date
			}
			if t.Date == "" {
				t.Date = date
			}
			if t.Creation_Time == "" {
				t.Creation_Time = date + "T00:00:00Z"
			}
			if t.UploadDate == "" {
				t.UploadDate = date
			}
			if t.Originally_Available_At == "" {
				t.Originally_Available_At = date
			}
			if t.FormattedDate == "" {
				t.FormattedDate = date
			}
			if len(date) >= 4 {
				t.Year = date[:4]
			}

			printMap[consts.JReleaseDate] = t.ReleaseDate
			printMap[consts.JDate] = t.Date
			printMap[consts.JYear] = t.Year

			print.PrintGrabbedFields("time and date", &printMap)

			if t.FormattedDate == "" {
				dates.FormatAllDates(fd)
			}
			rtn, err := fd.JSONFileRW.WriteJSON(fieldMap)
			switch {
			case err != nil:
				logging.E(0, "Failed to write new metadata (%s) into JSON file '%s': %v", date, fd.JSONFilePath, err)
				return data, true
			case rtn != nil:
				data = rtn
				return data, true
			}
		}
	}
	return data, false
}

// fillEmptyTimestamps attempts to infer missing timestamps
func fillEmptyTimestamps(t *models.MetadataDates) bool {

	gotRelevantDate := false

	// Infer from originally available date
	if t.Originally_Available_At != "" && len(t.Originally_Available_At) >= 6 {

		gotRelevantDate = true
		if t.Creation_Time == "" {
			if formatted, ok := dates.YyyyMmDd(t.Originally_Available_At); ok {
				if !strings.ContainsRune(formatted, 'T') {
					t.Creation_Time = formatted + "T00:00:00Z"
					t.FormattedDate = formatted
				} else {
					t.Creation_Time = formatted
					t.FormattedDate, _, _ = strings.Cut(formatted, "T")
				}
			} else {
				if formatted, ok := dates.YyyyMmDd(t.Originally_Available_At); ok {
					if !strings.ContainsRune(formatted, 'T') {
						t.Creation_Time = formatted + "T00:00:00Z"
						t.FormattedDate = formatted
					} else {
						t.Creation_Time = formatted
						t.FormattedDate, _, _ = strings.Cut(formatted, "T")
					}
				} else {
					t.Creation_Time = t.Originally_Available_At + "T00:00:00Z"
				}
			}
		}
	}

	// Infer from release date
	if t.ReleaseDate != "" && len(t.ReleaseDate) >= 6 {
		gotRelevantDate = true
		if t.Creation_Time == "" {
			if formatted, ok := dates.YyyyMmDd(t.ReleaseDate); ok {
				t.Creation_Time = formatted + "T00:00:00Z"
				if t.FormattedDate == "" {
					t.FormattedDate = formatted
				}
			} else {
				t.Creation_Time = t.ReleaseDate + "T00:00:00Z"
			}
		}
		if t.Originally_Available_At == "" {
			if formatted, ok := dates.YyyyMmDd(t.ReleaseDate); ok {
				t.Originally_Available_At = formatted
				if t.FormattedDate == "" {
					t.FormattedDate = formatted
				}
			} else {
				t.Originally_Available_At = t.ReleaseDate
			}
		}
	}
	// Infer from date
	if t.Date != "" && len(t.Date) >= 6 {
		gotRelevantDate = true
		if formatted, ok := dates.YyyyMmDd(t.ReleaseDate); ok {
			t.Creation_Time = formatted + "T00:00:00Z"
			if t.FormattedDate == "" {
				t.FormattedDate = formatted
			}
		} else {
			t.Creation_Time = t.Date + "T00:00:00Z"
		}
		if t.Originally_Available_At == "" {
			if formatted, ok := dates.YyyyMmDd(t.ReleaseDate); ok {
				t.Originally_Available_At = formatted
				if t.FormattedDate == "" {
					t.FormattedDate = formatted
				}
			} else {
				t.Originally_Available_At = t.Date
			}
		}
	}

	// Infer from upload date
	if t.UploadDate != "" && len(t.UploadDate) >= 6 {
		if formatted, ok := dates.YyyyMmDd(t.UploadDate); ok {
			t.Creation_Time = formatted + "T00:00:00Z"
			if t.FormattedDate == "" {
				t.FormattedDate = formatted
			}
		} else {
			t.Creation_Time = t.UploadDate + "T00:00:00Z"
		}
		if t.Originally_Available_At == "" {
			t.Originally_Available_At = t.UploadDate
		}
	}
	// Fill empty date
	if t.Date == "" {
		switch {
		case t.ReleaseDate != "":
			t.Date = t.ReleaseDate
			t.Originally_Available_At = t.ReleaseDate

		case t.UploadDate != "":
			t.Date = t.UploadDate
			t.Originally_Available_At = t.UploadDate

		case t.FormattedDate != "":
			t.Date = t.FormattedDate
		}
	}
	// Fill empty year
	if t.Year == "" {
		switch {
		case t.Date != "" && len(t.Date) >= 4:
			t.Year = t.Date[:4]

		case t.UploadDate != "" && len(t.UploadDate) >= 4:
			t.Year = t.UploadDate[:4]

		case t.FormattedDate != "" && len(t.FormattedDate) >= 4:
			t.Year = t.FormattedDate[:4]
		}
	}
	if len(t.Year) > 4 {
		t.Year = t.Year[:4]
	}

	// Try to fix accidentally using upload date if another date is available
	if len(t.Year) == 4 && !strings.HasPrefix(t.Creation_Time, t.Year) && len(t.Creation_Time) >= 4 {

		logging.D(1, "Creation time does not match year tag, seeing if other dates are available...")

		switch {
		case strings.HasPrefix(t.Originally_Available_At, t.Year):
			t.Creation_Time = t.Originally_Available_At + "T00:00:00Z"
			logging.D(1, "Changed creation time to %s", t.Originally_Available_At)

		case strings.HasPrefix(t.ReleaseDate, t.Year):
			t.Creation_Time = t.ReleaseDate + "T00:00:00Z"
			logging.D(1, "Changed creation time to %s", t.ReleaseDate)

		case strings.HasPrefix(t.Date, t.Year):
			t.Creation_Time = t.Date + "T00:00:00Z"
			logging.D(1, "Changed creation time to %s", t.Date)

		case strings.HasPrefix(t.FormattedDate, t.Year):
			t.Creation_Time = t.FormattedDate + "T00:00:00Z"
			logging.D(1, "Changed creation time to %s", t.FormattedDate)

		default:
			logging.D(1, "Could not find a match, directly altering t.Creation_Time for year (month and day may therefore be wrong)")
			t.Creation_Time = t.Year + t.Creation_Time[4:]
			logging.D(1, "Changed creation time's year only. Got '%s'", t.Creation_Time)
		}
	}
	return gotRelevantDate
}
package metadata

import (
	"metarr/internal/config"
	consts "metarr/internal/domain/constants"
	enums "metarr/internal/domain/enums"
	keys "metarr/internal/domain/keys"
	"metarr/internal/models"
	browser "metarr/internal/utils/browser"
	logging "metarr/internal/utils/logging"
	"strings"
)

// fillDescriptions grabs description data from JSON
func fillDescriptions(fd *models.FileData, data map[string]interface{}) (map[string]interface{}, bool) {

	d := fd.MTitleDesc
	w := fd.MWebData
	t := fd.MDates

	fieldMap := map[string]*string{ // Order by importance
		consts.JLongDescription:  &d.LongDescription,
		consts.JLong_Description: &d.Long_Description,
		consts.JDescription:      &d.Description,
		consts.JSynopsis:         &d.Synopsis,
		consts.JSummary:          &d.Summary,
		consts.JComment:          &d.Comment,
	}
	filled := unpackJSON("descriptions", fieldMap, data)

	datePfx := config.GetBool(keys.MDescDatePfx)
	dateSfx := config.GetBool(keys.MDescDateSfx)

	if (datePfx || dateSfx) && t.StringDate != "" {

		for _, value := range fieldMap {
			if value != nil {
				switch {
				case datePfx:
					if !strings.HasPrefix(*value, t.StringDate) {
						*value = t.StringDate + "\n\n" + *value // Prefix string date
					}
					continue
				case dateSfx:
					if !strings.HasSuffix(*value, t.StringDate) {
						*value = *value + "\n\n" + t.StringDate // Suffix string date
					}
					continue
				default:
					logging.D(1, "Unknown issue appending date to description. Condition should be impossible? (reached: %s)", *value)
					continue
				}
			}
		}
	}

	// Attempt to fill empty description fields by inference
	for _, value := range fieldMap {
		if ok := fillEmptyDescriptions(value, d); ok {
			filled = true
		}
	}

	// Check if any values are present
	if !filled {
		for _, val := range fieldMap {
			if val != nil {
				if *val == "" {
					continue
				} else {
					filled = true
				}
			}
		}
	}

	switch {
	case filled:
		rtn, err := fd.JSONFileRW.WriteJSON(fieldMap)
		switch {
		case err != nil:
			logging.E(0, "Failed to write into JSON file '%s': %v", fd.JSONFilePath, err)
			return data, true
		case rtn != nil:
			data = rtn
			return data, true
		}

	case w.WebpageURL == "":
		logging.I("Page URL not found in data, so cannot scrape for missing description in '%s'", fd.JSONFilePath)
		return data, false
	}

	description := browser.ScrapeMeta(w, enums.WEBCLASS_DESCRIPTION)

	// Infer remaining fields from description
	if description != "" {
		for _, value := range fieldMap {
			if *value == "" {
				*value = description
			}
		}

		// Insert new scraped fields into file
		rtn, err := fd.JSONFileRW.WriteJSON(fieldMap)
		if err != nil {
			logging.E(0, "Failed to insert new data (%s) into JSON file '%s': %v", description, fd.JSONFilePath, err)
		} else if rtn != nil {
			data = rtn
		}
		return data, true
	} else {
		return data, false
	}
}

// fillEmptyDescriptions fills empty description fields by inference
func fillEmptyDescriptions(want *string, d *models.MetadataTitlesDescs) bool {

	filled := false
	if want == nil {
		logging.E(0, "Sent in string null, returning...")
		return false
	}
	if *want == "" {
		switch {
		case d.LongDescription != "":
			*want = d.LongDescription
			filled = true

		case d.Long_Description != "":
			*want = d.Long_Description
			filled = true

		case d.Description != "":
			*want = d.Description
			filled = true

		case d.Synopsis != "":
			*want = d.Synopsis
			filled = true

		case d.Summary != "":
			*want = d.Summary
			filled = true

		case d.Comment != "":
			*want = d.Comment
			filled = true
		}
	}
	return filled
}
package metadata

import (
	"metarr/internal/models"
	logging "metarr/internal/utils/logging"
	print "metarr/internal/utils/print"
)

// Primary function to fill out meta fields before writing
func FillMetaFields(fd *models.FileData, data map[string]interface{}) (map[string]interface{}, bool) {

	var (
		ok   bool
		meta map[string]interface{}
	)
	allFilled := true

	if len(fd.MWebData.TryURLs) == 0 {
		if !FillWebpageDetails(fd, data) {
			logging.I("No URL metadata found")
			allFilled = false
		}
	}

	if !fillTitles(fd, data) {
		logging.I("No title metadata found")
		allFilled = false
	}

	if meta, ok = fillCredits(fd, data); !ok {
		logging.I("No credits metadata found")
		allFilled = false
	} else if meta != nil {
		data = meta
	}

	if meta, ok = fillDescriptions(fd, data); !ok {
		logging.I("No description metadata found")
		allFilled = false
	} else if meta != nil {
		data = meta
	}
	return data, allFilled
}

// unpackJSON decodes JSON for metafields
func unpackJSON(fieldType string, fieldMap map[string]*string, metadata map[string]interface{}) bool {

	dataFilled := false
	printMap := make(map[string]string, len(fieldMap))

	// Iterate through the decoded JSON to match fields against
	// the passed in map of fields to fill
	for key, value := range metadata {
		if strVal, ok := value.(string); ok {
			if field, exists := fieldMap[key]; exists && field != nil && *field == "" {

				*field = strVal
				dataFilled = true

				if printMap[key] == "" {
					printMap[key] = strVal
				}
			}
		}
	}
	print.PrintGrabbedFields(fieldType, &printMap)

	return dataFilled
}
package metadata

import (
	consts "metarr/internal/domain/constants"
	enums "metarr/internal/domain/enums"
	"metarr/internal/models"
	browser "metarr/internal/utils/browser"
	print "metarr/internal/utils/print"
)

// fillTitles grabs the fulltitle ("title")
func fillTitles(fd *models.FileData, data map[string]interface{}) bool {

	t := fd.MTitleDesc
	w := fd.MWebData

	printMap := make(map[string]string, len(data))

	for key, value := range data {
		if val, ok := value.(string); ok && val != "" {
			switch {
			case key == consts.JFulltitle:
				t.Fulltitle = val
				printMap[key] = val

			case key == consts.JTitle:
				t.Title = val
				printMap[key] = val

			case key == consts.JSubtitle:
				t.Subtitle = val
				printMap[key] = val
			}
		}
	}

	if t.Fulltitle != "" {
		t.Title = t.Fulltitle
	} else if t.Fulltitle == "" && t.Title != "" {
		t.Fulltitle = t.Title
	}

	if t.Title == "" {
		title := browser.ScrapeMeta(w, enums.WEBCLASS_TITLE)
		if title != "" {
			t.Title = title
		}
	}
	print.PrintGrabbedFields("title", &printMap)

	return t.Title != ""
}
package metadata

import (
	consts "metarr/internal/domain/constants"
	"metarr/internal/models"
	logging "metarr/internal/utils/logging"
	print "metarr/internal/utils/print"
)

// Grabs details necessary to scrape the web for missing metafields
func FillWebpageDetails(fd *models.FileData, data map[string]interface{}) bool {

	var isFilled bool

	w := fd.MWebData

	priorityMap := [5]string{consts.JWebpageURL,
		consts.JURL,
		consts.JReferer,
		consts.JWebpageDomain,
		consts.JDomain}

	printMap := make(map[string]string, len(priorityMap))

	for _, wanted := range priorityMap {
		for key, value := range data {

			if val, ok := value.(string); ok && val != "" {
				if key == wanted {
					switch {
					case key == consts.JWebpageURL:

						logging.D(3, "Got URL: %s", val)

						if w.WebpageURL == "" {
							w.WebpageURL = val
						}
						printMap[key] = val
						w.TryURLs = append(w.TryURLs, val)

						isFilled = true

					case key == consts.JURL:

						logging.D(3, "Got URL: %s", val)

						if w.VideoURL == "" {
							w.VideoURL = val
						}
						printMap[key] = val
						w.TryURLs = append(w.TryURLs, val)

						isFilled = true

					case key == consts.JReferer:

						logging.D(3, "Got URL: %s", val)

						if w.Referer == "" {
							w.Referer = val
						}
						printMap[key] = val
						w.TryURLs = append(w.TryURLs, val)

						isFilled = true

					case key == consts.JWebpageDomain, key == consts.JDomain:

						logging.D(3, "Got URL: %s", val)

						if w.Domain == "" {
							w.Domain = val
						}
						printMap[key] = val

						isFilled = true
					}
				}
			}
		}
	}

	logging.D(2, "Stored URLs for scraping missing fields: %v", w.TryURLs)

	print.PrintGrabbedFields("web details", &printMap)

	return isFilled
}
package metadata

import (
	consts "metarr/internal/domain/constants"
	"metarr/internal/models"
	logging "metarr/internal/utils/logging"
	"strings"
)

// fillNFODescriptions attempts to fill in title info from NFO
func fillNFOCredits(fd *models.FileData) bool {

	c := fd.MCredits
	n := fd.NFOData

	fieldMap := map[string]*string{
		consts.NActors:            &c.Actor,
		consts.NDirector:          &c.Director,
		consts.NProductionCompany: &c.Publisher,
		consts.NStudio:            &c.Studio,
		consts.NWriter:            &c.Writer,
		consts.NProducer:          &c.Producer,
	}

	// Post-unmarshal clean
	cleanEmptyFields(fieldMap)

	if n.Actors != nil {
		for _, actor := range n.Actors {
			c.Actors = append(c.Actors, actor.Name)
		}
		fillSingleCredits(c.Actors, &c.Actor)
	}
	if n.Directors != nil {
		c.Directors = append(c.Directors, n.Directors...)
		fillSingleCredits(c.Directors, &c.Director)
	}
	if n.Producers != nil {
		c.Producers = append(c.Producers, n.Producers...)
		fillSingleCredits(c.Producers, &c.Producer)
	}
	if n.Writers != nil {
		c.Writers = append(c.Writers, n.Writers...)
		fillSingleCredits(c.Writers, &c.Writer)
	}
	if n.Publishers != nil {
		c.Publishers = append(c.Publishers, n.Publishers...)
		fillSingleCredits(c.Publishers, &c.Publisher)
	}
	if n.Studios != nil {
		c.Studios = append(c.Studios, n.Studios...)
		fillSingleCredits(c.Studios, &c.Studio)
	}

	return true
}

// fillSingleCredits fills empty singular credits fields from
// filled arrays
func fillSingleCredits(entries []string, target *string) {

	if target == nil {
		logging.D(1, "Target string is nil, skipping...")
		return
	}

	if *target != "" {
		logging.D(1, "Target string is not empty, skipping...")
		return
	}

	filtered := make([]string, 0, len(entries))
	for _, entry := range entries {
		if entry != "" {
			filtered = append(filtered, entry)
		}
	}

	*target = strings.Join(filtered, ", ")
}

func unpackCredits(fd *models.FileData, creditsData map[string]interface{}) bool {
	c := fd.MCredits
	filled := false

	// Recursive helper to search for "role" within nested maps
	var findRoles func(data map[string]interface{})
	findRoles = func(data map[string]interface{}) {
		// Check each key-value pair within the actor data
		for k, v := range data {
			if k == "role" {
				if role, ok := v.(string); ok {
					logging.D(3, "Adding role '%s' to actors", role)
					c.Actors = append(c.Actors, role)
					filled = true
				}
			} else if nested, ok := v.(map[string]interface{}); ok {
				// Recursive call for further nested maps
				findRoles(nested)
			} else if nestedList, ok := v.([]interface{}); ok {
				// Handle lists of nested elements
				for _, item := range nestedList {
					if nestedMap, ok := item.(map[string]interface{}); ok {
						findRoles(nestedMap)
					}
				}
			}
		}
	}

	// Access the "cast" data to find "actor" entries
	if castData, ok := creditsData["cast"].(map[string]interface{}); ok {
		if actorsData, ok := castData["actor"].([]interface{}); ok {
			for _, actorData := range actorsData {
				if actorMap, ok := actorData.(map[string]interface{}); ok {
					if name, ok := actorMap["name"].(string); ok {
						logging.D(3, "Adding actor name '%s'", name)
						c.Actors = append(c.Actors, name)
						filled = true
					}
					if role, ok := actorMap["role"].(string); ok {
						logging.D(3, "Adding actor role '%s'", role)
						filled = true
					}
				}
			}
		} else {
			logging.D(1, "'actor' key is present but not a valid structure")
		}
	} else {
		logging.D(1, "'cast' key is missing or not a map")
	}

	return filled
}
package metadata

import (
	"metarr/internal/dates"
	consts "metarr/internal/domain/constants"
	enums "metarr/internal/domain/enums"
	"metarr/internal/models"
	browser "metarr/internal/utils/browser"
	logging "metarr/internal/utils/logging"
	print "metarr/internal/utils/print"
)

func fillNFOTimestamps(fd *models.FileData) bool {

	t := fd.MDates
	w := fd.MWebData
	n := fd.NFOData

	fieldMap := map[string]*string{
		consts.NAired:        &t.Date,
		consts.NPremiereDate: &t.ReleaseDate,
		consts.NYear:         &t.Year,
	}

	cleanEmptyFields(fieldMap)

	gotRelevantDate := false
	printMap := make(map[string]string, len(fieldMap))

	if n.Premiered != "" {
		if rtn, ok := dates.YyyyMmDd(n.Premiered); ok && rtn != "" {
			if t.FormattedDate == "" {
				t.FormattedDate = rtn
			}
		}
		printMap[consts.NPremiereDate] = n.Premiered
		gotRelevantDate = true
	}
	if n.ReleaseDate != "" {
		if rtn, ok := dates.YyyyMmDd(n.ReleaseDate); ok && rtn != "" {
			if t.FormattedDate == "" {
				t.FormattedDate = rtn
			}
		}
		printMap[consts.NAired] = n.Premiered
		gotRelevantDate = true
	}
	if n.Year != "" {
		t.Year = n.Year
		printMap[consts.NYear] = n.Year
	}

	if t.FormattedDate != "" {
		if t.Date == "" {
			t.Date = t.FormattedDate
		}
		if t.ReleaseDate == "" {
			t.ReleaseDate = t.FormattedDate
		}
		if t.Creation_Time == "" {
			t.Creation_Time = t.FormattedDate + "T00:00:00Z"
		}
		gotRelevantDate = true
	}

	switch {
	case gotRelevantDate:

		var err error

		logging.D(3, "Got a relevant date, proceeding...")
		print.PrintGrabbedFields("time and date", &printMap)
		if t.FormattedDate == "" {
			dates.FormatAllDates(fd)
		} else {
			t.StringDate, err = dates.ParseNumDate(t.FormattedDate)
			if err != nil {
				logging.E(0, err.Error())
			}
		}

	case w.WebpageURL == "":

		logging.I("Page URL not found in metadata, so cannot scrape for missing date in '%s'", fd.JSONFilePath)
		print.PrintGrabbedFields("time and date", &printMap)
		return false
	}

	scrapedDate := browser.ScrapeMeta(w, enums.WEBCLASS_DATE)
	logging.D(1, "Scraped date: %s", scrapedDate)

	logging.D(3, "Passed web scrape attempt for date.")

	var (
		date string
		err  error
	)
	if scrapedDate != "" {
		date, err = dates.ParseStringDate(scrapedDate)
		if err != nil || date == "" {
			logging.E(0, "Failed to parse date '%s': %v", scrapedDate, err)
			return false
		} else {
			if t.ReleaseDate == "" {
				t.ReleaseDate = date
			}
			if t.Date == "" {
				t.Date = date
			}
			if t.Creation_Time == "" {
				t.Creation_Time = date + "T00:00:00Z"
			}
			if t.UploadDate == "" {
				t.UploadDate = date
			}
			if t.Originally_Available_At == "" {
				t.Originally_Available_At = date
			}
			if t.FormattedDate == "" {
				t.FormattedDate = date
			}
			if len(date) >= 4 {
				t.Year = date[:4]
			}

			printMap[consts.NPremiereDate] = t.ReleaseDate
			printMap[consts.NAired] = t.Date
			printMap[consts.NYear] = t.Year

			print.PrintGrabbedFields("time and date", &printMap)

			if t.FormattedDate == "" {
				dates.FormatAllDates(fd)
			}
		}
	}

	return true
}
package metadata

import (
	consts "metarr/internal/domain/constants"
	"metarr/internal/models"
	print "metarr/internal/utils/print"
)

// fillNFODescriptions attempts to fill in title info from NFO
func fillNFODescriptions(fd *models.FileData) bool {

	d := fd.MTitleDesc
	n := fd.NFOData

	fieldMap := map[string]*string{
		consts.NDescription: &d.Description,
		consts.NPlot:        &d.LongDescription,
	}

	// Post-unmarshal clean
	cleanEmptyFields(fieldMap)

	if n.Description != "" {
		if d.Description == "" {
			d.Description = n.Description
		}
		if d.LongDescription == "" {
			d.LongDescription = n.Description
		}
	}
	if n.Plot != "" {
		if d.Description == "" {
			d.Description = n.Plot
		}
		if d.LongDescription == "" {
			d.LongDescription = n.Plot
		}
	}

	if d.Synopsis == "" {
		switch {
		case n.Plot != "":
			d.Synopsis = n.Plot
		case n.Description != "":
			d.Summary = n.Description
		case d.LongDescription != "":
			d.Synopsis = d.LongDescription
		case d.Description != "":
			d.Synopsis = d.Description
		}
	}
	if d.Summary == "" {
		switch {
		case n.Plot != "":
			d.Summary = n.Plot
		case n.Description != "":
			d.Summary = n.Description
		case d.LongDescription != "":
			d.Summary = d.LongDescription
		case d.Description != "":
			d.Summary = d.Description
		}
	}
	if d.Comment == "" {
		switch {
		case n.Plot != "":
			d.Comment = n.Plot
		case n.Description != "":
			d.Comment = n.Description
		case d.LongDescription != "":
			d.Comment = d.LongDescription
		case d.Description != "":
			d.Comment = d.Description
		}
	}

	print.CreateModelPrintout(fd, fd.NFOFilePath, "Parsing NFO descriptions")
	return true
}
package metadata

import (
	"metarr/internal/models"
	logging "metarr/internal/utils/logging"
	print "metarr/internal/utils/print"
	"strings"
)

// FillNFO is the primary entrypoint for filling NFO metadata
// from an open file's read content
func FillNFO(fd *models.FileData) bool {

	var filled bool

	if ok := fillNFOTimestamps(fd); ok {
		filled = true
	}

	if ok := fillNFOTitles(fd); ok {
		filled = true
	}

	if ok := fillNFODescriptions(fd); ok {
		filled = true
	}

	if ok := fillNFOCredits(fd); ok {
		filled = true
	}

	if ok := fillNFOWebData(fd); ok {
		filled = true
	}

	print.CreateModelPrintout(fd, fd.NFOBaseName, "Fill metadata from NFO for file '%s'", fd.NFOFilePath)

	return filled
}

// Clean up empty fields from fieldmap
func cleanEmptyFields(fieldMap map[string]*string) {
	for _, value := range fieldMap {
		if strings.TrimSpace(*value) == "" {
			*value = ""
		}
	}
}

// nestedLoop parses content recursively and returns a nested map
func nestedLoop(content string) map[string]interface{} {
	nested := make(map[string]interface{})

	logging.D(2, "Parsing content in nestedLoop: %s", content)

	for len(content) > 0 {
		if strings.HasPrefix(content, "<?xml") || strings.HasPrefix(content, "<?") {
			endIdx := strings.Index(content, "?>")
			if endIdx == -1 {
				logging.E(0, "Malformed XML declaration in content: %s", content)
				break
			}
			content = content[endIdx+2:]
			logging.D(2, "Skipping XML declaration, remaining content: %s", content)
			continue
		}

		// Find the opening tag
		openIdx := strings.Index(content, "<")
		if openIdx == -1 {
			break // No more tags
		}

		openIdxClose := strings.Index(content, ">")
		if openIdxClose == -1 {
			logging.E(0, "No valid tag close bracket for entry beginning %s", content[openIdx:])
			break // No closing tag bracket
		}

		// Get the tag name and check if it is self-closing
		tag := content[openIdx+1 : openIdxClose]
		isSelfClosing := strings.HasSuffix(tag, "/")
		tag = strings.TrimSuffix(tag, "/") // Remove trailing / if present

		if isSelfClosing {
			// Self-closing tag; skip over and move to the next
			content = content[openIdxClose+1:]
			logging.D(2, "Skipping self-closing tag: %s", tag)
			continue
		}

		// Look for the corresponding closing tag
		closeTag := "</" + tag + ">"
		closeIdx := strings.Index(content, closeTag)
		if closeIdx == -1 {
			// No closing tag; skip this tag and continue
			content = content[openIdxClose+1:]
			logging.D(2, "Skipping tag without end tag: %s", tag)
			continue
		}

		// Extract the inner content between tags
		innerContent := content[openIdxClose+1 : closeIdx]
		logging.D(2, "Found inner content for tag '%s': %s", tag, innerContent)

		// Recursive call if innerContent contains nested tags
		if strings.Contains(innerContent, "<") && strings.Contains(innerContent, ">") {
			logging.D(2, "Recursively parsing nested content for tag '%s'", tag)
			nested[tag] = nestedLoop(innerContent)
		} else {
			logging.D(2, "Assigning inner content to tag '%s': %s", tag, innerContent)
			nested[tag] = innerContent
		}

		// Move past the processed tag
		content = content[closeIdx+len(closeTag):]
		logging.D(2, "Remaining content after parsing tag '%s': %s", tag, content)
	}

	logging.D(2, "Final parsed structure from nestedLoop: %v", nested)
	return nested
}

// unpackNFO unpacks an NFO map back to the model
func unpackNFO(fd *models.FileData, data map[string]interface{}, fieldMap map[string]*string) {
	logging.D(3, "Unpacking NFO map...")

	// Access the top-level "movie" key
	movieData, ok := data["movie"].(map[string]interface{})
	if !ok {
		logging.E(0, "Missing 'movie' key in data, unable to unpack")
		return
	}

	for field, fieldVal := range fieldMap {
		if fieldVal == nil {
			logging.E(0, "Field value is null, continuing...")
			continue
		}

		// Look for the field in the movie data
		val, exists := movieData[field]
		if !exists {
			continue // Field does not exist in this map
		}

		switch v := val.(type) {
		case string:
			logging.D(3, "Setting field '%s' to '%s'", field, v)
			*fieldVal = v
		case map[string]interface{}:
			switch field {

			case "title":
				logging.D(3, "Unpacking nested 'title' map...")
				unpackTitle(fd, v)
			case "cast":
				logging.D(3, "Unpacking nested 'cast' map...")
				unpackCredits(fd, v)
			}
		default:
			logging.D(1, "Unknown field type for '%s', skipping...", field)
		}
	}
}
package metadata

import (
	consts "metarr/internal/domain/constants"
	"metarr/internal/models"
	logging "metarr/internal/utils/logging"
)

// fillNFOTitles attempts to fill in title info from NFO
func fillNFOTitles(fd *models.FileData) bool {

	t := fd.MTitleDesc
	n := fd.NFOData

	fieldMap := map[string]*string{
		consts.NTitle:         &t.Title,
		consts.NOriginalTitle: &t.Fulltitle,
		consts.NTagline:       &t.Subtitle,
	}

	// Post-unmarshal clean
	cleanEmptyFields(fieldMap)

	logging.I("Grab NFO metadata: %v", t)

	if n.Title.Main != "" {
		if t.Title == "" {
			t.Title = n.Title.Main
		}
	}
	if n.Title.Original != "" {
		if t.Fulltitle == "" {
			t.Fulltitle = n.Title.Original
		}
		if t.Title == "" {
			t.Title = n.Title.Original
		}
	}
	if n.Title.Sub != "" {
		if t.Subtitle == "" {
			t.Subtitle = n.Title.Sub
		}
	}
	if n.Title.PlainText != "" {
		if t.Title == "" {
			t.Title = n.Title.PlainText
		}
	}
	return true
}

// unpackTitle unpacks common nested title elements to the model
func unpackTitle(fd *models.FileData, titleData map[string]interface{}) bool {
	t := fd.MTitleDesc
	filled := false

	for key, value := range titleData {
		switch key {
		case "main":
			if strVal, ok := value.(string); ok {
				logging.D(3, "Setting main title to '%s'", strVal)
				t.Title = strVal
				filled = true
			}
		case "sub":
			if strVal, ok := value.(string); ok {
				logging.D(3, "Setting subtitle to '%s'", strVal)
				t.Subtitle = strVal
				filled = true
			}
		default:
			logging.D(1, "Unknown nested title element '%s', skipping...", key)
		}
	}
	return filled
}
package metadata

import (
	consts "metarr/internal/domain/constants"
	"metarr/internal/models"
	print "metarr/internal/utils/print"
)

// fillNFODescriptions attempts to fill in title info from NFO
func fillNFOWebData(fd *models.FileData) bool {

	w := fd.MWebData
	nw := fd.NFOData.WebpageInfo

	fieldMap := map[string]*string{
		consts.NURL: &w.WebpageURL,
	}

	// Post-unmarshal clean
	cleanEmptyFields(fieldMap)

	if nw.URL != "" {
		if w.WebpageURL == "" {
			w.WebpageURL = nw.URL
		}
	}

	print.CreateModelPrintout(fd, fd.NFOFilePath, "Parsing NFO descriptions")
	return true
}
package metadata

import (
	logging "metarr/internal/utils/logging"
	"strings"
)

type ffprobeFormat struct {
	Tags ffprobeTags `json:"tags"`
}

type ffprobeOutput struct {
	Format ffprobeFormat `json:"format"`
}

type ffprobeTags struct {
	Description  string `json:"description"`
	Synopsis     string `json:"synopsis"`
	Title        string `json:"title"`
	CreationTime string `json:"creation_time"`
	Date         string `json:"date"`
	Artist       string `json:"artist"`
	Composer     string `json:"composer"`
}

// safeGetDatePart safely extracts the date part before 'T' if it exists
func safeGetDatePart(timeStr string) string {
	timeStr = strings.TrimSpace(timeStr)
	if parts := strings.Split(timeStr, "T"); len(parts) > 0 {
		return parts[0]
	}
	return timeStr
}

func printArray(s []string) {
	str := strings.Join(s, ", ")
	logging.I("FFprobe captured %s", str)
}
package metadata

import (
	"encoding/json"
	"fmt"
	consts "metarr/internal/domain/constants"
	"metarr/internal/models"
	logging "metarr/internal/utils/logging"
	"os/exec"
	"strings"
)

// MP4MetaMatches checks FFprobe captured metadata from the video against the metafile
func MP4MetaMatches(fd *models.FileData) bool {

	c := fd.MCredits
	d := fd.MDates
	t := fd.MTitleDesc

	// FFprobe command fetches metadata from the video file
	command := exec.Command(
		"ffprobe",
		"-v", "quiet",
		"-print_format", "json",
		"-show_format",
		fd.OriginalVideoPath,
	)

	logging.I("Made command for FFprobe:\n\n%v", command.String())

	output, err := command.Output()
	if err != nil {
		logging.E(0, "Error running FFprobe command: %v. Will process video.", err)
		return false
	}

	// Parse JSON output
	var ffData ffprobeOutput

	if err := json.Unmarshal(output, &ffData); err != nil {
		logging.E(0, "Error parsing FFprobe output: %v. Will process video.", err)
		return false
	}

	// Map of metadata to check
	metaCheckMap := map[string]struct {
		existing string
		new      string
	}{
		consts.JDescription: {
			existing: strings.TrimSpace(ffData.Format.Tags.Description),
			new:      strings.TrimSpace(t.Description),
		},
		consts.JSynopsis: {
			existing: strings.TrimSpace(ffData.Format.Tags.Synopsis),
			new:      strings.TrimSpace(t.Synopsis),
		},
		consts.JTitle: {
			existing: strings.TrimSpace(ffData.Format.Tags.Title),
			new:      strings.TrimSpace(t.Title),
		},
		consts.JCreationTime: {
			existing: safeGetDatePart(ffData.Format.Tags.CreationTime),
			new:      safeGetDatePart(d.Creation_Time),
		},
		consts.JDate: {
			existing: strings.TrimSpace(ffData.Format.Tags.Date),
			new:      strings.TrimSpace(d.Date),
		},
		consts.JArtist: {
			existing: strings.TrimSpace(ffData.Format.Tags.Artist),
			new:      strings.TrimSpace(c.Artist),
		},
		consts.JComposer: {
			existing: strings.TrimSpace(ffData.Format.Tags.Composer),
			new:      strings.TrimSpace(c.Composer),
		},
	}

	// Collect all metadata for logging
	var ffContent []string
	matches := true

	// Check each field
	for key, values := range metaCheckMap {
		printVals := fmt.Sprintf("Currently in video: Key=%s, Value=%s, New Value=%s", key, values.existing, values.new)
		ffContent = append(ffContent, printVals)

		if values.new != values.existing {
			logging.D(2, "======== Mismatched meta in file: '%s' ========\nMismatch in key '%s':\nNew value: '%s'\nIn video as: '%s'. Will process video.",
				fd.OriginalVideoBaseName, key, values.new, values.existing)
			matches = false
		} else {
			logging.D(2, "Detected key '%s' as being the same.\nFFprobe: '%s'\nMetafile: '%s'", key, values.existing, values.new)
		}
	}

	// Print all captured metadata
	printArray(ffContent)

	return matches
}
package metadata

import (
	"fmt"
	"metarr/internal/config"
	"metarr/internal/dates"
	enums "metarr/internal/domain/enums"
	keys "metarr/internal/domain/keys"
	process "metarr/internal/metadata/process/json"
	check "metarr/internal/metadata/reader/check_existing"
	tags "metarr/internal/metadata/tags"
	jsonRw "metarr/internal/metadata/writer/json"
	"metarr/internal/models"
	"metarr/internal/transformations"
	logging "metarr/internal/utils/logging"
	"os"
	"path/filepath"
	"strings"
	"sync"
)

var (
	mu sync.Mutex
)

// ProcessJSONFile reads a single JSON file and fills in the metadata
func ProcessJSONFile(fd *models.FileData) (*models.FileData, error) {

	if fd == nil {
		return nil, fmt.Errorf("model passed in null")
	}

	logging.D(2, "Beginning JSON file processing...")

	// Function mutex
	mu.Lock()
	defer mu.Unlock()

	filePath := fd.JSONFilePath
	w := fd.MWebData

	// Open the file
	file, err := os.OpenFile(filePath, os.O_RDWR, 0644)
	if err != nil {
		logging.ErrorArray = append(logging.ErrorArray, err)
		return nil, fmt.Errorf("failed to open file: %w", err)
	}
	defer file.Close()

	// Grab and store metadata reader/writer
	jsonRW := jsonRw.NewJSONFileRW(file)
	if jsonRW != nil {
		fd.JSONFileRW = jsonRW
	}

	// Decode metadata from file
	data, err := fd.JSONFileRW.DecodeJSON(file)
	if err != nil {
		return nil, err
	}
	logging.D(3, "%v", data)

	var ok bool

	// Get web data first (before MakeMetaEdits in case of transformation presets)
	if ok = process.FillWebpageDetails(fd, data); ok {
		logging.I("URLs grabbed: %s", w.TryURLs)
	}

	if len(w.TryURLs) > 0 {
		if match := transformations.TryTransPresets(w.TryURLs, fd); match == "" {
			logging.D(1, "No presets found for video '%s' URLs %v", fd.OriginalVideoBaseName, w.TryURLs)
		}
	}

	// Make metadata adjustments per user selection or transformation preset
	if edited, err := fd.JSONFileRW.MakeJSONEdits(file, fd); err != nil {
		return nil, err
	} else if edited {
		logging.D(2, "Refreshing JSON metadata after edits were made...")
		if data, err = fd.JSONFileRW.RefreshJSON(); err != nil {
			return nil, err
		}
	}

	// Fill timestamps and make/delete date tag ammendments
	if data, ok = process.FillTimestamps(fd, data); !ok {
		logging.I("No date metadata found")
	}

	if fd.MDates.FormattedDate == "" {
		dates.FormatAllDates(fd)
	}

	if config.IsSet(keys.MDateTagMap) || config.IsSet(keys.MDelDateTagMap) {
		ok, err = jsonRW.JSONDateTagEdits(file, fd)
		if err != nil {
			logging.E(0, err.Error())
		} else if !ok {
			logging.E(0, "Did not make date tag edits for metadata, tag already exists?")
		}
	} else {
		logging.D(4, "Skipping making metadata date tag edits, key not set")
	}

	// Fill other metafields
	if data, ok = process.FillMetaFields(fd, data); !ok {
		logging.D(2, "Some metafields were unfilled")
	}

	// Make filename date tag
	logging.D(3, "About to make date tag for: %v", file.Name())
	if config.IsSet(keys.FileDateFmt) {

		if dateFmt, ok := config.Get(keys.FileDateFmt).(enums.DateFormat); !ok {
			logging.E(0, "Got null or wrong type for file date format. Got type %T", dateFmt)
		} else if dateFmt != enums.DATEFMT_SKIP {
			fd.FilenameDateTag, err = tags.MakeDateTag(data, fd, dateFmt)
			if err != nil {
				logging.E(0, "Failed to make date tag: %v", err)
			}
		} else {
			logging.D(1, "Set file date tag format to skip, not making date tag for '%s'", file.Name())
		}
	}

	// Add new filename tag for files
	if config.IsSet(keys.MFilenamePfx) {
		logging.D(3, "About to make prefix tag for: %v", file.Name())
		fd.FilenameMetaPrefix = tags.MakeFilenameTag(data, file)
	}

	// Check if metadata is already existent in target file
	if filetypeMetaCheckSwitch(fd) {
		logging.I("Metadata already exists in target file '%s', will skip processing", fd.OriginalVideoBaseName)
		fd.MetaAlreadyExists = true
	}

	return fd, nil
}

func filetypeMetaCheckSwitch(fd *models.FileData) bool {

	logging.D(4, "Entering filetypeMetaCheckSwitch with '%s'", fd.OriginalVideoPath)

	var outExt string
	outFlagSet := config.IsSet(keys.OutputFiletype)

	if outFlagSet {
		outExt = config.GetString(keys.OutputFiletype)
	} else {
		outExt = filepath.Ext(fd.OriginalVideoPath)
		logging.D(2, "Got output extension as %s", outExt)
	}

	currentExt := filepath.Ext(fd.OriginalVideoPath)
	currentExt = strings.TrimSpace(currentExt)

	if outExt != "" && !strings.HasPrefix(outExt, ".") {
		outExt = "." + outExt

		logging.D(2, "Added dot to outExt: %s, currentExt is %s", outExt, currentExt)
	}

	if outFlagSet && outExt != "" && !strings.EqualFold(outExt, currentExt) {
		logging.I("Input format '%s' differs from output format '%s', will not run metadata checks", currentExt, outExt)
		return false
	}

	// Run metadata checks in all other cases
	switch currentExt {
	case ".mp4":
		return check.MP4MetaMatches(fd)
	default:
		logging.I("Checks not currently implemented for this filetype")
		return false
	}
}
package metadata

import (
	"fmt"
	nfo "metarr/internal/metadata/process/nfo"
	nfoRw "metarr/internal/metadata/writer/nfo"
	"metarr/internal/models"
	logging "metarr/internal/utils/logging"
	"os"
)

// ProcessNFOFiles processes NFO files and sends data into the metadata model
func ProcessNFOFiles(fd *models.FileData) (*models.FileData, error) {
	if fd == nil {
		return nil, fmt.Errorf("model passed in null")
	}

	logging.D(2, "Beginning NFO file processing...")

	// Open the file
	file, err := os.OpenFile(fd.NFOFilePath, os.O_RDWR, 0644)
	if err != nil {
		logging.ErrorArray = append(logging.ErrorArray, err)
		return nil, fmt.Errorf("failed to open file: %w", err)
	}
	defer file.Close()

	nfoRW := nfoRw.NewNFOFileRW(file)
	if nfoRW != nil {
		// Store NFO RW in model
		fd.NFOFileRW = nfoRW
	}

	data, err := nfoRW.DecodeMetadata(file)
	if err != nil || data == nil {
		logging.E(0, "Failed to decode metadata from file: %v", err)
	} else {
		// Store NFO data in model
		fd.NFOData = data
	}

	edited, err := nfoRW.MakeMetaEdits(nfoRW.Meta, file, fd)
	if err != nil {
		logging.E(0, "Encountered issue making meta edits: %v", err)
	}
	if edited {
		logging.D(2, "Refreshing NFO metadata after edits were made...")
		data, err := fd.NFOFileRW.RefreshMetadata()
		if err != nil {
			return nil, err
		} else {
			fd.NFOData = data
		}
	}

	// Fill to file metadata
	if ok := nfo.FillNFO(fd); !ok {
		logging.E(0, "No metadata filled from NFO file...")
	}
	return fd, nil
}
package metadata

import (
	"fmt"
	consts "metarr/internal/domain/constants"
	enums "metarr/internal/domain/enums"
	"metarr/internal/models"
	logging "metarr/internal/utils/logging"
	"strconv"
	"strings"
)

// MakeDateTag attempts to create the date tag for files using metafile data
func MakeDateTag(metadata map[string]interface{}, fd *models.FileData, dateFmt enums.DateFormat) (string, error) {

	if dateFmt == enums.DATEFMT_SKIP {
		logging.D(1, "Skip set, not making file date tag for '%s'", fd.OriginalVideoBaseName)
		return "", nil
	}

	var (
		date  string
		found bool
	)

	if fd.MDates.FormattedDate == "" {
		date, found = extractDateFromMetadata(metadata)
		if !found {
			logging.E(0, "No dates found in JSON file")
			return "", nil
		}
	} else {
		date = fd.MDates.FormattedDate
	}

	year, month, day, err := parseDateComponents(date, dateFmt)
	if err != nil {
		return "", fmt.Errorf("failed to parse date components: %w", err)
	}

	dateStr, err := formatDateString(year, month, day, dateFmt)
	if dateStr == "" || err != nil {
		logging.E(0, "Failed to create date string")
		return "", nil
	}

	dateTag := "[" + dateStr + "]"
	if checkTagExists(dateTag, fd.OriginalVideoBaseName) {
		logging.D(2, "Tag '%s' already detected in name, skipping...", dateTag)
		return "", nil
	}

	logging.S(0, "Made date tag '%s' from file '%v'", dateTag, fd.OriginalVideoBaseName)
	return dateTag, nil
}

// MetaTagAlreadyExists determines if the tag already exists in the metadata
func MetaDateTagExists(tag, fieldVal string) bool {
	if strings.Contains(fieldVal, tag) {
		logging.D(2, "Tag '%s' already detected in metafield, skipping...", tag)
		return true
	}
	return false
}

// extractDateFromMetadata attempts to find a date in the metadata using predefined fields
func extractDateFromMetadata(metadata map[string]interface{}) (string, bool) {
	preferredDateFields := []string{
		consts.JReleaseDate,
		"releasedate",
		"released_on",
		consts.JOriginallyAvailable,
		"originally_available",
		"originallyavailable",
		consts.JDate,
		consts.JUploadDate,
		"uploaddate",
		"uploaded_on",
		consts.JCreationTime, // Last resort, may give false positives
		"created_at",
	}

	for _, field := range preferredDateFields {
		if value, found := metadata[field]; found {
			if strVal, ok := value.(string); ok && strVal != "" && len(strVal) > 4 {
				if date, _, found := strings.Cut(strVal, "T"); found {
					return date, true
				}
				return strVal, true
			}
		}
	}
	return "", false
}

// parseDateComponents extracts and validates year, month, and day from the date string
func parseDateComponents(date string, dateFmt enums.DateFormat) (year, month, day string, err error) {
	date = strings.ReplaceAll(date, "-", "")
	date = strings.TrimSpace(date)

	year, month, day, err = getYearMonthDay(date, dateFmt)
	if err != nil {
		return "", "", "", err
	}

	return validateDateComponents(year, month, day)
}

// formatDateString formats the date as a hyphenated string
func formatDateString(year, month, day string, dateFmt enums.DateFormat) (string, error) {
	var parts [3]string

	switch dateFmt {
	case enums.DATEFMT_YYYY_MM_DD, enums.DATEFMT_YY_MM_DD:
		parts = [3]string{year, month, day}
	case enums.DATEFMT_YYYY_DD_MM, enums.DATEFMT_YY_DD_MM:
		parts = [3]string{year, day, month}
	case enums.DATEFMT_DD_MM_YYYY, enums.DATEFMT_DD_MM_YY:
		parts = [3]string{day, month, year}
	case enums.DATEFMT_MM_DD_YYYY, enums.DATEFMT_MM_DD_YY:
		parts = [3]string{month, day, year}
	}

	result := joinNonEmpty(parts)
	if result == "" {
		return "", fmt.Errorf("no valid date components found")
	}
	return result, nil
}

// joinNonEmpty joins non-empty strings from an array with hyphens
func joinNonEmpty(parts [3]string) string {
	nonEmpty := make([]string, 0, len(parts))
	for _, p := range parts {
		if p != "" {
			nonEmpty = append(nonEmpty, p)
		}
	}
	if len(nonEmpty) == 0 {
		return ""
	}
	return strings.Join(nonEmpty, "-")
}

// getYear returns the year digits from the date string
func getYearMonthDay(d string, dateFmt enums.DateFormat) (year, month, day string, err error) {
	d = strings.ReplaceAll(d, "-", "")
	d = strings.TrimSpace(d)

	if len(d) >= 8 {
		switch dateFmt {
		case enums.DATEFMT_DD_MM_YY, enums.DATEFMT_MM_DD_YY, enums.DATEFMT_YY_DD_MM, enums.DATEFMT_YY_MM_DD:
			year = d[2:4]
		default:
			year = d[:4]
		}
		month = d[4:6]
		day = d[6:8]

		return year, month, day, nil
	}
	if len(d) >= 6 {
		year = d[:2]
		month = d[2:4]
		day = d[4:6]

		return year, month, day, nil
	}
	if len(d) == 4 { // Guess year or month-day

		i, err := strconv.Atoi(d[:2])
		if err != nil {
			return "", "", "", fmt.Errorf("invalid date string '%s' threw error: %w", d, err)
		}
		j, err := strconv.Atoi(d[2:4])
		if err != nil {
			return "", "", "", fmt.Errorf("invalid date string '%s' threw error: %w", d, err)
		}

		if (i == 20 || i == 19) && j > 12 { // First guess year
			logging.I("Guessing date string '%s' as year", d)
			switch dateFmt {
			case enums.DATEFMT_DD_MM_YY, enums.DATEFMT_MM_DD_YY, enums.DATEFMT_YY_DD_MM, enums.DATEFMT_YY_MM_DD:
				return d[2:4], "", "", nil
			default:
				return d[:4], "", "", nil
			}
		} else { // Second guess, month-date
			if ddmm, mmdd := maybeDayMonth(i, j); ddmm || mmdd {
				if ddmm {
					logging.I("Guessing date string '%s' as day-month")
					day = d[:2]
					month = d[2:4]

				} else if mmdd {
					logging.I("Guessing date string '%s' as month-day")
					day = d[2:4]
					month = d[:2]
				}
				return "", month, day, nil
			} else if i == 20 || i == 19 { // Final guess year
				logging.I("Guessing date string '%s' as year after failed day-month check", d)
				switch dateFmt {
				case enums.DATEFMT_DD_MM_YY, enums.DATEFMT_MM_DD_YY, enums.DATEFMT_YY_DD_MM, enums.DATEFMT_YY_MM_DD:
					return d[2:4], "", "", nil
				default:
					return d[:4], "", "", nil
				}
			}
		}
	}

	return "", "", "", fmt.Errorf("failed to parse year, month, and day from '%s'", d)
}

// validateDateComponents attempts to fix faulty date arrangements
func validateDateComponents(year, month, day string) (string, string, string, error) {

	if isValidMonth(month) && isValidDay(day, month, year) {
		return year, month, day, nil
	}

	// Attempt swapping day and month
	if isValidMonth(day) && isValidDay(month, day, year) {
		return year, day, month, nil
	}

	// Fail check:
	return "", "", "", fmt.Errorf("invalid date components: year=%s, month=%s, day=%s", year, month, day)
}

// isValidMonth checks if the month inputted is a valid month
func isValidMonth(month string) bool {
	m, err := strconv.Atoi(month)
	if err != nil {
		return false
	}
	return m >= 1 && m <= 12
}

// isValidDay checks if the day inputted is a valid day
func isValidDay(day, month, year string) bool {
	d, err := strconv.Atoi(day)
	if err != nil {
		return false
	}

	m, err := strconv.Atoi(month)
	if err != nil {
		return false
	}

	y, err := strconv.Atoi(year)
	if err != nil {
		return false
	}

	if d < 1 || d > 31 {
		return false
	}

	// Months with 30 days
	if m == 4 || m == 6 || m == 9 || m == 11 {
		return d <= 30
	}

	// February
	if m == 2 {
		// Leap year check
		isLeap := y%4 == 0 && (y%100 != 0 || y%400 == 0)
		if isLeap {
			return d <= 29
		}
		return d <= 28
	}

	return true
}

// maybeDayMonth guesses if the input is a DD-MM or MM-DD format
func maybeDayMonth(i, j int) (ddmm, mmdd bool) {
	if i == 0 || i >= 31 || j == 0 || j >= 31 {
		return false, false
	}

	switch {
	case i <= 31 && j <= 12:
		return ddmm, false
	case j <= 31 && i <= 12:
		return false, mmdd
	default:
		return false, false
	}
}
package metadata

import (
	"metarr/internal/config"
	keys "metarr/internal/domain/keys"
	"metarr/internal/domain/regex"
	logging "metarr/internal/utils/logging"
	"os"
	"path/filepath"
	"strings"
)

// makeFilenameTag creates the metatag string to prefix filenames with
func MakeFilenameTag(metadata map[string]interface{}, file *os.File) string {
	logging.D(5, "Entering makeFilenameTag with data %v", metadata)

	tagFields := config.GetStringSlice(keys.MFilenamePfx)
	if len(tagFields) == 0 {
		return "[]"
	}

	var b strings.Builder
	b.Grow(len(metadata) + len("[2006-02-01]"))
	b.WriteString("[")

	written := false
	for _, field := range tagFields {

		if value, exists := metadata[field]; exists {
			if strVal, ok := value.(string); ok && strVal != "" {

				if written {
					b.WriteString("_")
				}

				b.WriteString(strVal)
				written = true

				logging.D(3, "Added metafield %v to prefix tag (Tag so far: %s)", field, b.String())
			}
		}
	}

	b.WriteString("]")

	tag := b.String()
	tag = strings.TrimSpace(tag)
	tag = strings.ToValidUTF8(tag, "")

	invalidChars := regex.InvalidCharsCompile()
	tag = invalidChars.ReplaceAllString(tag, "")

	logging.D(1, "Made metatag '%s' from file '%s'", tag, file.Name())

	if tag != "[]" {
		if checkTagExists(tag, filepath.Base(file.Name())) {
			logging.D(2, "Tag '%s' already detected in name, skipping...", tag)
			tag = "[]"
		}
	}
	return tag
}

// checkTagExists checks if the constructed tag already exists in the filename
func checkTagExists(tag, filename string) bool {
	logging.D(3, "Checking if tag '%s' exists in filename '%s'", tag, filename)

	return strings.Contains(filename, tag)
}
package metadata

import (
	"encoding/json"
	"fmt"
	"io"
	"metarr/internal/config"
	enums "metarr/internal/domain/enums"
	keys "metarr/internal/domain/keys"
	"metarr/internal/models"
	backup "metarr/internal/utils/fs/backup"
	logging "metarr/internal/utils/logging"
	"os"
	"sync"
)

type JSONFileRW struct {
	mu          sync.RWMutex
	muFileWrite sync.Mutex
	Meta        map[string]interface{}
	File        *os.File
}

// NewJSONFileRW creates a new instance of the JSON file reader/writer
func NewJSONFileRW(file *os.File) *JSONFileRW {
	logging.D(3, "Retrieving new meta writer/rewriter for file '%s'...", file.Name())
	return &JSONFileRW{
		File: file,
		Meta: make(map[string]interface{}),
	}
}

// DecodeJSON parses and stores JSON metadata into a map and returns it
func (rw *JSONFileRW) DecodeJSON(file *os.File) (map[string]interface{}, error) {

	if file == nil {
		return nil, fmt.Errorf("nil file handle provided")
	}

	currentPos, err := file.Seek(0, io.SeekCurrent)
	if err != nil {
		return nil, fmt.Errorf("failed to get current position: %w", err)
	}
	success := false
	defer func() {
		if !success {
			if _, err := file.Seek(currentPos, io.SeekStart); err != nil {
				logging.E(0, err.Error())
			}
		}
	}()

	// Seek start
	if _, err := file.Seek(0, io.SeekStart); err != nil {
		return nil, fmt.Errorf("failed to seek file: %w", err)
	}

	// Decode to map
	decoder := json.NewDecoder(file)
	input := make(map[string]interface{})

	if err := decoder.Decode(&input); err != nil {
		return nil, fmt.Errorf("failed to decode JSON in DecodeMetadata: %w", err)
	}

	switch {
	case len(input) <= 0, input == nil:
		logging.D(3, "Metadata not stored, is blank: %v", input)
		return input, nil
	default:
		rw.updateMeta(input)
		logging.D(5, "Decoded and stored metadata: %v", input)
		return input, nil
	}
}

// RefreshJSON reloads the metadata map from the file after updates
func (rw *JSONFileRW) RefreshJSON() (map[string]interface{}, error) {

	if rw.File == nil {
		return nil, fmt.Errorf("no file handle available")
	}

	return rw.DecodeJSON(rw.File)
}

// WriteJSON inserts metadata into the JSON file from a map
func (rw *JSONFileRW) WriteJSON(fieldMap map[string]*string) (map[string]interface{}, error) {

	if fieldMap == nil {
		return nil, fmt.Errorf("fieldMap cannot be nil")
	}

	// Create a copy of the current metadata
	currentMeta := rw.copyMeta()

	logging.D(4, "Entering WriteMetadata for file '%s'", rw.File.Name())

	// Update metadata with new fields
	updated := false
	for field, value := range fieldMap {
		if field == "all-credits" {
			continue
		}

		if value != nil && *value != "" {

			if currentVal, exists := currentMeta[field]; !exists {
				logging.D(3, "Adding new field '%s' with value '%s'", field, *value)
				currentMeta[field] = *value
				updated = true

			} else if currentStrVal, ok := currentVal.(string); !ok || currentStrVal != *value || config.GetBool(keys.MOverwrite) {
				logging.D(3, "Updating field '%s' from '%v' to '%s'", field, currentVal, *value)
				currentMeta[field] = *value
				updated = true

			} else {
				logging.D(3, "Skipping field '%s' - value unchanged and overwrite not forced", field)
			}
		}
	}

	// Return if no updates
	if !updated {
		logging.D(2, "No fields were updated")
		return currentMeta, nil
	}

	// Backup if option set
	if config.GetBool(keys.NoFileOverwrite) {
		if err := backup.BackupFile(rw.File); err != nil {
			return currentMeta, fmt.Errorf("failed to create backup: %w", err)
		}
	}

	// Write file
	if err := rw.writeJsonToFile(rw.File, currentMeta); err != nil {
		return currentMeta, err
	}

	rw.updateMeta(currentMeta)

	logging.D(3, "Successfully updated JSON file with new metadata")
	return currentMeta, nil
}

// MakeJSONEdits applies a series of transformations and writes the final result to the file
func (rw *JSONFileRW) MakeJSONEdits(file *os.File, fd *models.FileData) (bool, error) {

	currentMeta := rw.copyMeta()

	logging.D(5, "Entering MakeJSONEdits.\nData: %v", currentMeta)

	var (
		edited, ok bool
		trimPfx    []*models.MetaTrimPrefix
		trimSfx    []*models.MetaTrimSuffix

		apnd []*models.MetaAppend
		pfx  []*models.MetaPrefix

		new []*models.MetaNewField

		replace []*models.MetaReplace
	)

	// Replacements
	if len(fd.ModelMReplace) > 0 {
		logging.I("Model for file '%s' making replacements", fd.OriginalVideoBaseName)
		replace = fd.ModelMReplace
	} else if config.IsSet(keys.MReplaceText) {
		if replace, ok = config.Get(keys.MReplaceText).([]*models.MetaReplace); !ok {
			logging.E(0, "Could not retrieve prefix trim, wrong type: '%T'", replace)
		}
	}

	// Field trim
	if len(fd.ModelMTrimPrefix) > 0 {
		logging.I("Model for file '%s' trimming prefixes", fd.OriginalVideoBaseName)
		trimPfx = fd.ModelMTrimPrefix
	} else if config.IsSet(keys.MTrimPrefix) {
		if trimPfx, ok = config.Get(keys.MTrimPrefix).([]*models.MetaTrimPrefix); !ok {
			logging.E(0, "Could not retrieve prefix trim, wrong type: '%T'", trimPfx)
		}
	}

	if len(fd.ModelMTrimSuffix) > 0 {
		logging.I("Model for file '%s' trimming suffixes", fd.OriginalVideoBaseName)
		trimSfx = fd.ModelMTrimSuffix
	} else if config.IsSet(keys.MTrimSuffix) {
		if trimSfx, ok = config.Get(keys.MTrimSuffix).([]*models.MetaTrimSuffix); !ok {
			logging.E(0, "Could not retrieve suffix trim, wrong type: '%T'", trimSfx)
		}
	}

	// Append and prefix
	if len(fd.ModelMAppend) > 0 {
		logging.I("Model for file '%s' adding appends", fd.OriginalVideoBaseName)
		apnd = fd.ModelMAppend
	} else if config.IsSet(keys.MAppend) {
		if apnd, ok = config.Get(keys.MAppend).([]*models.MetaAppend); !ok {
			logging.E(0, "Could not retrieve appends, wrong type: '%T'", apnd)
		}
	}

	if len(fd.ModelMPrefix) > 0 {
		logging.I("Model for file '%s' adding prefixes", fd.OriginalVideoBaseName)
		pfx = fd.ModelMPrefix
	} else if config.IsSet(keys.MPrefix) {
		if pfx, ok = config.Get(keys.MPrefix).([]*models.MetaPrefix); !ok {
			logging.E(0, "Could not retrieve prefix, wrong type: '%T'", pfx)
		}
	}

	// New fields
	if len(fd.ModelMNewField) > 0 {
		logging.I("Model for file '%s' applying preset new field additions", fd.OriginalVideoBaseName)
		new = fd.ModelMNewField
	} else if config.IsSet(keys.MNewField) {
		if new, ok = config.Get(keys.MNewField).([]*models.MetaNewField); !ok {
			logging.E(0, "Could not retrieve new fields, wrong type: '%T'", pfx)
		}
	}

	// Make edits:
	// Replace
	if len(replace) > 0 {
		if ok, err := rw.replaceJson(currentMeta, replace); err != nil {
			logging.E(0, err.Error())
		} else if ok {
			edited = true
		}
	}

	// Trim
	if len(trimPfx) > 0 {
		if ok, err := rw.trimJsonPrefix(currentMeta, trimPfx); err != nil {
			logging.E(0, err.Error())
		} else if ok {
			edited = true
		}
	}

	if len(trimSfx) > 0 {
		if ok, err := rw.trimJsonSuffix(currentMeta, trimSfx); err != nil {
			logging.E(0, err.Error())
		} else if ok {
			edited = true
		}
	}

	// Append and prefix
	if len(apnd) > 0 {
		if ok, err := rw.jsonAppend(currentMeta, apnd); err != nil {
			logging.E(0, err.Error())
		} else if ok {
			edited = true
		}
	}

	if len(pfx) > 0 {
		if ok, err := rw.jsonPrefix(currentMeta, pfx); err != nil {
			logging.E(0, err.Error())
		} else if ok {
			edited = true
		}
	}

	// Add new
	if len(new) > 0 {
		if ok, err := rw.setJsonField(currentMeta, fd.ModelMOverwrite, new); err != nil {
			logging.E(0, err.Error())
		} else if ok {
			edited = true
		}
	}

	if !edited {
		logging.D(3, "No JSON metadata edits made")
		return false, nil
	}

	// Write new metadata to file
	if err := rw.writeJsonToFile(file, currentMeta); err != nil {
		return false, fmt.Errorf("failed to write updated JSON to file: %w", err)
	}

	rw.updateMeta(currentMeta)

	fmt.Println()
	logging.S(0, "Successfully applied metadata edits to: %v", file.Name())

	return edited, nil
}

// JSONDateTagEdits is a public function to add date tags into the metafile, this is useful because
// the dates may not yet be scraped when the initial MakeMetaEdits runs
func (rw *JSONFileRW) JSONDateTagEdits(file *os.File, fd *models.FileData) (edited bool, err error) {

	logging.D(4, "Entering MakeDateTagEdits for file '%s'", file.Name())

	currentMeta := rw.copyMeta()

	logging.D(4, "About to perform MakeDateTagEdits operations for file '%s'", file.Name())

	// Delete date tag first, user's may want to delete and re-build
	if config.IsSet(keys.MDelDateTagMap) {
		logging.D(3, "Stripping metafield date tag...")
		if delDateTagMap, ok := config.Get(keys.MDelDateTagMap).(map[string]*models.MetaDateTag); ok {

			if len(delDateTagMap) > 0 {

				if ok, err := rw.jsonFieldDateTag(currentMeta, delDateTagMap, fd, enums.DATE_TAG_DEL_OP); err != nil {
					logging.E(0, err.Error())
				} else if ok {
					edited = true
				}
			} else {
				logging.E(0, "delDateTagMap grabbed empty")
			}
		} else {
			logging.E(0, "Got null or wrong type for %s: %T", keys.MDelDateTagMap, delDateTagMap)
		}
	}

	// Add date tag
	if config.IsSet(keys.MDateTagMap) {
		logging.D(3, "Adding metafield date tag...")
		if dateTagMap, ok := config.Get(keys.MDateTagMap).(map[string]*models.MetaDateTag); ok {

			if len(dateTagMap) > 0 {

				if ok, err := rw.jsonFieldDateTag(currentMeta, dateTagMap, fd, enums.DATE_TAG_ADD_OP); err != nil {
					logging.E(0, err.Error())
				} else if ok {
					edited = true
				}
			} else {
				logging.E(0, "dateTagMap grabbed empty")
			}
		} else {
			logging.E(0, "Got null or wrong type for %s: %T", keys.MDateTagMap, dateTagMap)
		}
	}

	if !edited {
		logging.D(2, "No date tag edits made, returning...")
		return false, nil
	}

	// Write back to file
	if err = rw.writeJsonToFile(file, currentMeta); err != nil {
		return false, fmt.Errorf("failed to write updated JSON to file: %w", err)
	}

	rw.updateMeta(currentMeta)

	fmt.Println()
	logging.S(0, "Successfully applied date tag JSON edits to: %v", file.Name())

	return edited, nil
}
package metadata

import (
	"encoding/json"
	"fmt"
	"io"
	logging "metarr/internal/utils/logging"
	"os"
	"strings"
)

// writeJsonToFile is a private metadata writing helper function
func (rw *JSONFileRW) writeJsonToFile(file *os.File, data map[string]interface{}) error {

	if file == nil {
		return fmt.Errorf("nil file handle provided")
	}
	if data == nil {
		return fmt.Errorf("nil data provided")
	}

	// Marshal data
	updatedFileContent, err := json.MarshalIndent(data, "", "  ")
	if err != nil {
		return fmt.Errorf("failed to marshal updated JSON: %w", err)
	}

	// Begin file ops
	rw.muFileWrite.Lock()
	defer rw.muFileWrite.Unlock()

	currentPos, err := file.Seek(0, io.SeekCurrent)
	if err != nil {
		return fmt.Errorf("failed to get current position: %w", err)
	}
	success := false
	defer func() {
		if !success {
			if _, err := file.Seek(currentPos, io.SeekStart); err != nil {
				logging.E(0, err.Error())
			}
		}
	}()

	// Seek file start
	if _, err := file.Seek(0, io.SeekStart); err != nil {
		return fmt.Errorf("failed to seek to beginning of file: %w", err)
	}

	// File ops
	if err := file.Truncate(0); err != nil {
		return fmt.Errorf("failed to truncate file: %w", err)
	}

	if _, err := file.Write(updatedFileContent); err != nil {
		return fmt.Errorf("failed to write to file: %w", err)
	}

	// Ensure changes are persisted
	if err := file.Sync(); err != nil {
		return fmt.Errorf("failed to sync file: %w", err)
	}

	success = true
	return nil
}

// cleanFieldValue trims leading/trailing whitespaces after deletions
func cleanFieldValue(value string) string {
	cleaned := strings.TrimSpace(value)
	cleaned = strings.Join(strings.Fields(cleaned), " ")
	return cleaned
}

// copyMeta creates a deep copy of the metadata map under read lock
func (rw *JSONFileRW) copyMeta() map[string]interface{} {
	rw.mu.RLock()
	defer rw.mu.RUnlock()

	if rw.Meta == nil {
		return make(map[string]interface{})
	}

	currentMeta := make(map[string]interface{}, len(rw.Meta))
	for k, v := range rw.Meta {
		currentMeta[k] = v
	}
	return currentMeta
}

// updateMeta safely updates the metadata map under write lock
func (rw *JSONFileRW) updateMeta(newMeta map[string]interface{}) {
	if newMeta == nil {
		newMeta = make(map[string]interface{})
	}

	rw.mu.Lock()
	defer rw.mu.Unlock()
	rw.Meta = newMeta
}
package metadata

import (
	"context"
	"fmt"
	"metarr/internal/config"
	enums "metarr/internal/domain/enums"
	keys "metarr/internal/domain/keys"
	tags "metarr/internal/metadata/tags"
	"metarr/internal/models"
	logging "metarr/internal/utils/logging"
	prompt "metarr/internal/utils/prompt"
	"strings"
)

// replaceJson makes user defined JSON replacements
func (rw *JSONFileRW) replaceJson(data map[string]interface{}, replace []*models.MetaReplace) (bool, error) {

	logging.D(5, "Entering replaceJson with data: %v", data)

	if len(replace) == 0 {
		logging.E(0, "Called replaceJson without replacements")
		return false, nil
	}

	edited := false
	for _, replacement := range replace {
		if replacement.Field == "" || replacement.Value == "" {
			continue
		}

		if val, exists := data[replacement.Field]; exists {

			if strVal, ok := val.(string); ok {
				logging.D(3, "Identified field '%s', replacing '%s' with '%s'", replacement.Field, replacement.Value, replacement.Replacement)
				data[replacement.Field] = strings.ReplaceAll(strVal, replacement.Value, replacement.Replacement)
				edited = true
			}
		}
	}
	logging.D(5, "After JSON replace: %v", data)
	return edited, nil
}

// trimJsonPrefix trims defined prefixes from specified fields
func (rw *JSONFileRW) trimJsonPrefix(data map[string]interface{}, trimPfx []*models.MetaTrimPrefix) (bool, error) {

	logging.D(5, "Entering trimJsonPrefix with data: %v", data)

	if len(trimPfx) == 0 {
		logging.E(0, "Called trimJsonPrefix without prefixes to trim")
		return false, nil
	}

	edited := false
	for _, prefix := range trimPfx {
		if prefix.Field == "" || prefix.Prefix == "" {
			continue
		}

		if val, exists := data[prefix.Field]; exists {

			if strVal, ok := val.(string); ok {
				logging.D(3, "Identified field '%s', trimming '%s'", prefix.Field, prefix.Prefix)
				data[prefix.Field] = strings.TrimPrefix(strVal, prefix.Prefix)
				edited = true
			}
		}
	}
	logging.D(5, "After prefix trim: %v", data)
	return edited, nil
}

// trimJsonSuffix trims defined suffixes from specified fields
func (rw *JSONFileRW) trimJsonSuffix(data map[string]interface{}, trimSfx []*models.MetaTrimSuffix) (bool, error) {

	logging.D(5, "Entering trimJsonSuffix with data: %v", data)

	if len(trimSfx) == 0 {
		logging.E(0, "Called trimJsonSuffix without prefixes to trim")
		return false, nil
	}

	edited := false
	for _, suffix := range trimSfx {
		if suffix.Field == "" || suffix.Suffix == "" {
			continue
		}

		if val, exists := data[suffix.Field]; exists {

			if strVal, ok := val.(string); ok {
				logging.D(3, "Identified field '%s', trimming '%s'", suffix.Field, suffix.Suffix)
				data[suffix.Field] = strings.TrimSuffix(strVal, suffix.Suffix)
				edited = true
			}
		}
	}
	logging.D(5, "After suffix trim: %v", data)
	return edited, nil
}

// jsonAppend appends to the fields in the JSON data
func (rw *JSONFileRW) jsonAppend(data map[string]interface{}, apnd []*models.MetaAppend) (bool, error) {

	logging.D(5, "Entering jsonAppend with data: %v", data)

	if len(apnd) == 0 {
		logging.E(0, "No new suffixes to append", keys.MAppend)
		return false, nil // No replacements to apply
	}

	edited := false
	for _, suffix := range apnd {
		if suffix.Field == "" || suffix.Suffix == "" {
			continue
		}

		if value, exists := data[suffix.Field]; exists {

			if strVal, ok := value.(string); ok {

				logging.D(3, "Identified input JSON field '%v', appending '%v'", suffix.Field, suffix.Suffix)
				strVal += suffix.Suffix
				data[suffix.Field] = strVal
				edited = true
			}
		}
	}
	logging.D(5, "After JSON suffix append: %v", data)

	return edited, nil
}

// metaPrefix applies prefixes to the fields in the JSON data
func (rw *JSONFileRW) jsonPrefix(data map[string]interface{}, pfx []*models.MetaPrefix) (bool, error) {

	logging.D(5, "Entering jsonPrefix with data: %v", data)

	if len(pfx) == 0 {
		logging.E(0, "No new prefix replacements found", keys.MPrefix)
		return false, nil // No replacements to apply
	}

	edited := false
	for _, prefix := range pfx {
		if prefix.Field == "" || prefix.Prefix == "" {
			continue
		}

		if value, found := data[prefix.Field]; found {

			if strVal, ok := value.(string); ok {
				logging.D(3, "Identified input JSON field '%v', adding prefix '%v'", prefix.Field, prefix.Prefix)
				strVal = prefix.Prefix + strVal
				data[prefix.Field] = strVal
				edited = true

			}
		}
	}
	logging.D(5, "After adding prefixes: %v", data)

	return edited, nil
}

// setJsonField can insert a new field which does not yet exist into the metadata file
func (rw *JSONFileRW) setJsonField(data map[string]interface{}, modelOW bool, new []*models.MetaNewField) (bool, error) {

	if len(new) == 0 {
		logging.E(0, "No new field additions found", keys.MNewField)
		return false, nil
	}

	var (
		metaOW,
		metaPS bool
	)

	if !config.IsSet(keys.MOverwrite) && !config.IsSet(keys.MPreserve) {
		metaOW = modelOW
	} else {
		metaOW = config.GetBool(keys.MOverwrite)
		metaPS = config.GetBool(keys.MPreserve)
	}

	logging.D(3, "Retrieved additions for new field data: %v", new)
	processedFields := make(map[string]bool, len(new))

	newAddition := false
	ctx := context.Background()
	for _, addition := range new {
		if addition.Field == "" || addition.Value == "" {
			continue
		}

		// If field doesn't exist at all, add it
		if _, exists := data[addition.Field]; !exists {
			data[addition.Field] = addition.Value
			processedFields[addition.Field] = true
			newAddition = true
			continue
		}
		if !metaOW {

			// Check for context cancellation before proceeding
			select {
			case <-ctx.Done():
				logging.I("Operation canceled for field: %s", addition.Field)
				return false, fmt.Errorf("operation canceled")
			default:
				// Proceed
			}
			if _, alreadyProcessed := processedFields[addition.Field]; alreadyProcessed {
				continue
			}

			if existingValue, exists := data[addition.Field]; exists {

				if !metaOW && !metaPS {
					promptMsg := fmt.Sprintf("Field '%s' already exists with value '%v' in file '%v'. Overwrite? (y/n) to proceed, (Y/N) to apply to whole queue", addition.Field, existingValue, rw.File.Name())

					reply, err := prompt.PromptMetaReplace(promptMsg, metaOW, metaPS)
					if err != nil {
						logging.E(0, err.Error())
					}
					switch reply {
					case "Y":
						logging.D(2, "Received meta overwrite reply as 'Y' for %s in %s, falling through to 'y'", existingValue, rw.File.Name())
						config.Set(keys.MOverwrite, true)
						metaOW = true
						fallthrough
					case "y":
						logging.D(2, "Received meta overwrite reply as 'y' for %s in %s", existingValue, rw.File.Name())
						addition.Field = strings.TrimSpace(addition.Field)
						logging.D(3, "Adjusted field from '%s' to '%s'\n", data[addition.Field], addition.Field)

						data[addition.Field] = addition.Value
						processedFields[addition.Field] = true
						newAddition = true

					case "N":
						logging.D(2, "Received meta overwrite reply as 'N' for %s in %s, falling through to 'n'", existingValue, rw.File.Name())
						config.Set(keys.MPreserve, true)
						metaPS = true
						fallthrough
					case "n":
						logging.D(2, "Received meta overwrite reply as 'n' for %s in %s", existingValue, rw.File.Name())
						logging.P("Skipping field '%s'\n", addition.Field)
						processedFields[addition.Field] = true
					}
				} else if metaOW { // FieldOverwrite is set

					data[addition.Field] = addition.Value
					processedFields[addition.Field] = true
					newAddition = true

				} else if metaPS { // FieldPreserve is set
					continue
				}
			}
		} else {
			// Add the field if it doesn't exist yet, or overwrite is true
			data[addition.Field] = addition.Value
			processedFields[addition.Field] = true
			newAddition = true
		}
	}
	logging.D(3, "JSON after transformations: %v", data)

	return newAddition, nil
}

// jsonFieldDateTag sets date tags in designated meta fields
func (rw *JSONFileRW) jsonFieldDateTag(data map[string]interface{}, dateTagMap map[string]*models.MetaDateTag, fd *models.FileData, op enums.MetaDateTaggingType) (bool, error) {

	logging.D(2, "Making metadata date tag for '%s'...", fd.OriginalVideoBaseName)

	if len(dateTagMap) == 0 {
		logging.D(3, "No date tag operations to perform")
		return false, nil
	}
	if fd == nil {
		return false, fmt.Errorf("jsonFieldDateTag called with null FileData model")
	}

	edited := false
	for field, dateTag := range dateTagMap {
		if dateTag == nil {
			logging.E(0, "Nil date tag configuration for field '%s'", field)
			continue
		}

		val, exists := data[field]
		if !exists {
			logging.D(3, "Field '%s' not found in metadata", field)
			continue
		}

		strVal, ok := val.(string)
		if !ok {
			logging.D(3, "Field '%s' is not a string value, type: %T", field, val)
			continue
		}

		// Generate the date tag
		tag, err := tags.MakeDateTag(data, fd, dateTag.Format)
		if err != nil {
			return false, fmt.Errorf("failed to generate date tag for field '%s': %w", field, err)
		}
		if len(tag) < 3 {
			return false, fmt.Errorf("generated date tag too short for field '%s': '%s'", field, tag)
		}

		// Check if it already exists
		if op == enums.DATE_TAG_ADD_OP {
			if tags.MetaDateTagExists(tag, strVal) {
				return false, nil
			}
		}

		// Apply the tag based on location
		switch dateTag.Loc {
		case enums.DATE_TAG_LOC_PFX:

			switch op {
			case enums.DATE_TAG_DEL_OP:
				before := strVal
				data[field] = cleanFieldValue(strings.TrimPrefix(strVal, tag))
				if data[field] != before {
					logging.I("Deleted date tag '%s' prefix from field '%s'", tag, field)
					edited = true
				} else {
					logging.I("Failed to strip date tag from '%s'", before)
				}

			case enums.DATE_TAG_ADD_OP:
				data[field] = tag + " " + strVal
				logging.I("Added date tag '%s' as prefix to field '%s'", tag, field)
				edited = true
			}

		case enums.DATE_TAG_LOC_SFX:

			switch op {
			case enums.DATE_TAG_DEL_OP:
				before := strVal
				data[field] = cleanFieldValue(strings.TrimSuffix(strVal, tag))
				if data[field] != before {
					logging.I("Deleted date tag '%s' suffix from field '%s'", tag, field)
					edited = true
				} else {
					logging.I("Failed to strip date tag from '%s'", before)
				}

			case enums.DATE_TAG_ADD_OP:
				data[field] = strVal + " " + tag
				logging.I("Added date tag '%s' as suffix to field '%s'", tag, field)
				edited = true
			}

		default:
			return false, fmt.Errorf("invalid date tag location enum: %v", dateTag.Loc)
		}
	}

	return edited, nil
}
package metadata

import (
	"bufio"
	"context"
	"encoding/xml"
	"fmt"
	"io"
	"metarr/internal/config"
	keys "metarr/internal/domain/keys"
	"metarr/internal/models"
	logging "metarr/internal/utils/logging"
	prompt "metarr/internal/utils/prompt"
	"os"
	"strings"
	"sync"
)

type NFOFileRW struct {
	mu    sync.RWMutex
	Model *models.NFOData
	Meta  string
	File  *os.File
}

// NewNFOFileRW creates a new instance of the NFO file reader/writer
func NewNFOFileRW(file *os.File) *NFOFileRW {
	logging.D(3, "Retrieving new meta writer/rewriter for file '%s'...", file.Name())
	return &NFOFileRW{
		File: file,
	}
}

// DecodeMetadata decodes XML from a file into a map, stores, and returns it
func (rw *NFOFileRW) DecodeMetadata(file *os.File) (*models.NFOData, error) {
	rw.mu.Lock()
	defer rw.mu.Unlock()

	// Read the entire file content first
	content, err := io.ReadAll(file)
	if err != nil {
		return nil, fmt.Errorf("failed to read file: %w", err)
	}

	rtn := rw.ensureXMLStructure(string(content))
	if rtn != "" {
		content = []byte(rtn)
	}

	// Store the raw content
	rw.Meta = string(content)

	// Reset file pointer
	if _, err := file.Seek(0, io.SeekStart); err != nil {
		return nil, fmt.Errorf("failed to seek file: %w", err)
	}

	// Single decode for the model
	decoder := xml.NewDecoder(file)
	var input *models.NFOData
	if err := decoder.Decode(&input); err != nil {
		return nil, fmt.Errorf("failed to decode XML: %w", err)
	}

	rw.Model = input
	logging.D(3, "Decoded metadata: %v", rw.Model)

	return rw.Model, nil
}

// RefreshMetadata reloads the metadata map from the file after updates
func (rw *NFOFileRW) RefreshMetadata() (*models.NFOData, error) {

	rw.mu.RLock()
	defer rw.mu.RUnlock()

	if _, err := rw.File.Seek(0, io.SeekStart); err != nil {
		return nil, fmt.Errorf("failed to seek file: %w", err)
	}

	// Decode metadata
	decoder := xml.NewDecoder(rw.File)

	if err := decoder.Decode(&rw.Model); err != nil {
		return nil, fmt.Errorf("failed to decode xml: %w", err)
	}

	logging.D(3, "Decoded metadata: %v", rw.Model)

	return rw.Model, nil
}

// MakeMetaEdits applies a series of transformations and writes the final result to the file
func (rw *NFOFileRW) MakeMetaEdits(data string, file *os.File, fd *models.FileData) (bool, error) {
	// Ensure we have valid XML
	if !strings.Contains(data, "<movie>") {
		return false, fmt.Errorf("invalid XML: missing movie tag")
	}

	var (
		edited, ok bool
		newContent string
		err        error

		trimPfx []*models.MetaTrimPrefix
		trimSfx []*models.MetaTrimSuffix

		apnd []*models.MetaAppend
		pfx  []*models.MetaPrefix

		new []*models.MetaNewField

		replace []*models.MetaReplace
	)

	// Replacements
	if len(fd.ModelMReplace) > 0 {
		logging.I("Model for file '%s' making replacements", fd.OriginalVideoBaseName)
		replace = fd.ModelMReplace
	} else if config.IsSet(keys.MReplaceText) {
		if replace, ok = config.Get(keys.MReplaceText).([]*models.MetaReplace); !ok {
			logging.E(0, "Count not retrieve prefix trim, wrong type: '%T'", replace)
		}
	}

	// Field trim
	if len(fd.ModelMTrimPrefix) > 0 {
		logging.I("Model for file '%s' trimming prefixes", fd.OriginalVideoBaseName)
		trimPfx = fd.ModelMTrimPrefix
	} else if config.IsSet(keys.MTrimPrefix) {
		if trimPfx, ok = config.Get(keys.MTrimPrefix).([]*models.MetaTrimPrefix); !ok {
			logging.E(0, "Count not retrieve prefix trim, wrong type: '%T'", trimPfx)
		}
	}

	if len(fd.ModelMTrimSuffix) > 0 {
		logging.I("Model for file '%s' trimming suffixes", fd.OriginalVideoBaseName)
		trimSfx = fd.ModelMTrimSuffix
	} else if config.IsSet(keys.MTrimSuffix) {
		if trimSfx, ok = config.Get(keys.MTrimSuffix).([]*models.MetaTrimSuffix); !ok {
			logging.E(0, "Count not retrieve suffix trim, wrong type: '%T'", trimSfx)
		}
	}

	// Append and prefix
	if len(fd.ModelMAppend) > 0 {
		logging.I("Model for file '%s' adding appends", fd.OriginalVideoBaseName)
		apnd = fd.ModelMAppend
	} else if config.IsSet(keys.MAppend) {
		if apnd, ok = config.Get(keys.MAppend).([]*models.MetaAppend); !ok {
			logging.E(0, "Count not retrieve appends, wrong type: '%T'", apnd)
		}
	}

	if len(fd.ModelMPrefix) > 0 {
		logging.I("Model for file '%s' adding prefixes", fd.OriginalVideoBaseName)
		pfx = fd.ModelMPrefix
	} else if config.IsSet(keys.MPrefix) {
		if pfx, ok = config.Get(keys.MPrefix).([]*models.MetaPrefix); !ok {
			logging.E(0, "Count not retrieve prefix, wrong type: '%T'", pfx)
		}
	}

	// New fields
	if len(fd.ModelMNewField) > 0 {
		logging.I("Model for file '%s' applying preset new field additions", fd.OriginalVideoBaseName)
		new = fd.ModelMNewField
	} else if config.IsSet(keys.MNewField) {
		if new, ok = config.Get(keys.MNewField).([]*models.MetaNewField); !ok {
			logging.E(0, "Could not retrieve new fields, wrong type: '%T'", pfx)
		}
	}

	// Make edits:
	// Replace
	if len(replace) > 0 {
		if newContent, ok, err = rw.replaceXml(data, replace); err != nil {
			logging.E(0, err.Error())
		} else if ok {
			edited = true
		}
	}

	// Trim
	if len(trimPfx) > 0 {
		if newContent, ok, err = rw.trimXmlPrefix(data, trimPfx); err != nil {
			logging.E(0, err.Error())
		} else if ok {
			edited = true
		}
	}

	if len(trimSfx) > 0 {
		if newContent, ok, err = rw.trimXmlSuffix(data, trimSfx); err != nil {
			logging.E(0, err.Error())
		} else if ok {
			edited = true
		}
	}

	// Append and prefix
	if len(apnd) > 0 {
		if newContent, ok, err = rw.xmlAppend(data, apnd); err != nil {
			logging.E(0, err.Error())
		} else if ok {
			edited = true
		}
	}

	if len(pfx) > 0 {
		if newContent, ok, err = rw.xmlPrefix(data, pfx); err != nil {
			logging.E(0, err.Error())
		} else if ok {
			edited = true
		}
	}

	// Add new
	if len(new) > 0 {
		if newContent, ok, err = rw.addNewXmlFields(data, fd.ModelMOverwrite, new); err != nil {
			logging.E(0, err.Error())
		} else if ok {
			edited = true
		}
	}

	// Only write if changes were made
	if edited {
		if err := rw.writeMetadataToFile(file, []byte(newContent)); err != nil {
			return false, fmt.Errorf("failed to refresh metadata: %w", err)
		}
	}

	return edited, nil
}

// Helper function to ensure XML structure
func (rw *NFOFileRW) ensureXMLStructure(content string) string {
	// Ensure XML declaration
	if !strings.HasPrefix(content, "<?xml") {
		content = `<?xml version="1.0" encoding="UTF-8"?>` + "\n" + content
	}

	// Ensure movie tag exists
	if !strings.Contains(content, "<movie>") {
		content = strings.TrimSpace(content)
		content = content + "\n<movie>\n</movie>"
	}

	return content
}

// refreshMetadataInternal is a private metadata refresh function
func (rw *NFOFileRW) refreshMetadataInternal(file *os.File) error {

	if _, err := file.Seek(0, io.SeekStart); err != nil {
		return fmt.Errorf("failed to seek file: %w", err)
	}

	if rw.Model == nil {
		return fmt.Errorf("NFOFileRW's stored metadata map is empty or null, did you forget to decode?")
	}

	decoder := xml.NewDecoder(file)
	if err := decoder.Decode(&rw.Model); err != nil {
		return fmt.Errorf("failed to decode xml: %w", err)
	}

	return nil
}

// writeMetadataToFile is a private metadata writing helper function
func (rw *NFOFileRW) writeMetadataToFile(file *os.File, content []byte) error {

	if err := file.Truncate(0); err != nil {
		return fmt.Errorf("truncate file: %w", err)
	}

	if _, err := file.Seek(0, io.SeekStart); err != nil {
		return fmt.Errorf("seek file: %w", err)
	}

	// Use buffered writer for efficiency
	writer := bufio.NewWriter(file)
	if _, err := writer.Write(content); err != nil {
		return fmt.Errorf("write content: %w", err)
	}

	if err := rw.refreshMetadataInternal(file); err != nil {
		return fmt.Errorf("failed to refresh metadata: %w", err)
	}

	return writer.Flush()
}

// replaceMeta applies meta replacement to the fields in the xml data
func (rw *NFOFileRW) replaceXml(data string, replace []*models.MetaReplace) (string, bool, error) {

	logging.D(5, "Entering replaceXml with data: %v", string(data))

	if len(replace) == 0 {
		return data, false, nil // No replacements to apply
	}

	edited := false
	for _, replacement := range replace {
		if replacement.Field == "" || replacement.Value == "" {
			continue
		}

		startTag := fmt.Sprintf("<%s>", replacement.Field)
		endTag := fmt.Sprintf("</%s>", replacement.Field)

		startIdx := strings.Index(data, startTag)
		endIdx := strings.Index(data, endTag)
		if startIdx == -1 || endIdx == -1 {
			continue // One or both tags missing
		}

		contentStart := startIdx + len(startTag)
		content := strings.TrimSpace(data[contentStart:endIdx])

		logging.D(2, "Identified input xml field '%s', replacing '%s' with '%s'", replacement.Field, replacement.Value, replacement.Replacement)

		content = strings.ReplaceAll(content, replacement.Value, replacement.Replacement)
		data = data[:contentStart] + content + data[endIdx:]
		edited = true
	}
	logging.D(5, "After meta replacements: %v", data)
	return data, edited, nil
}

// trimMetaPrefix applies meta replacement to the fields in the xml data
func (rw *NFOFileRW) trimXmlPrefix(data string, trimPfx []*models.MetaTrimPrefix) (string, bool, error) {

	logging.D(5, "Entering trimXmlPrefix with data: %v", string(data))

	if len(trimPfx) == 0 {
		return data, false, nil // No replacements to apply
	}

	edited := false
	for _, prefix := range trimPfx {
		if prefix.Field == "" || prefix.Prefix == "" {
			continue
		}

		startTag := fmt.Sprintf("<%s>", prefix.Field)
		endTag := fmt.Sprintf("</%s>", prefix.Field)

		startIdx := strings.Index(data, startTag)
		endIdx := strings.Index(data, endTag)
		if startIdx == -1 || endIdx == -1 {
			continue // One or both tags missing
		}

		contentStart := startIdx + len(startTag)
		content := strings.TrimSpace(data[contentStart:endIdx])

		logging.D(2, "Identified input xml field '%s', trimming prefix '%s'", prefix.Field, prefix.Prefix)

		content = strings.TrimPrefix(content, prefix.Prefix)
		data = data[:contentStart] + content + data[endIdx:]
		edited = true
	}
	logging.D(5, "After trimming prefixes: %v", data)
	return data, edited, nil
}

// trimMetaSuffix trims specified
func (rw *NFOFileRW) trimXmlSuffix(data string, trimSfx []*models.MetaTrimSuffix) (string, bool, error) {

	logging.D(5, "Entering trimXmlSuffix with data: %v", string(data))

	if len(trimSfx) == 0 {
		return data, false, nil // No replacements to apply
	}

	edited := false
	for _, suffix := range trimSfx {
		if suffix.Field == "" || suffix.Suffix == "" {
			continue
		}

		startTag := fmt.Sprintf("<%s>", suffix.Field)
		endTag := fmt.Sprintf("</%s>", suffix.Field)

		startIdx := strings.Index(data, startTag)
		endIdx := strings.Index(data, endTag)
		if startIdx == -1 || endIdx == -1 {
			continue // One or both tags missing
		}

		contentStart := startIdx + len(startTag)
		content := strings.TrimSpace(data[contentStart:endIdx])

		logging.D(2, "Identified input xml field '%s', trimming suffix '%s'", suffix.Field, suffix.Suffix)

		content = strings.TrimSuffix(content, suffix.Suffix)
		data = data[:contentStart] + content + data[endIdx:]
		edited = true
	}
	logging.D(5, "After meta replacements: %v", data)
	return data, edited, nil
}

// trimMetaPrefix applies meta replacement to the fields in the xml data
func (rw *NFOFileRW) xmlPrefix(data string, pfx []*models.MetaPrefix) (string, bool, error) {

	logging.D(5, "Entering xmlPrefix with data: %v", string(data))

	if len(pfx) == 0 {
		return data, false, nil // No replacements to apply
	}

	edited := false
	for _, prefix := range pfx {
		if prefix.Field == "" || prefix.Prefix == "" {
			continue
		}

		startTag := fmt.Sprintf("<%s>", prefix.Field)
		endTag := fmt.Sprintf("</%s>", prefix.Field)

		startIdx := strings.Index(data, startTag)
		endIdx := strings.Index(data, endTag)
		if startIdx == -1 || endIdx == -1 {
			continue // One or both tags missing
		}

		contentStart := startIdx + len(startTag)
		content := strings.TrimSpace(data[contentStart:endIdx])

		logging.D(2, "Identified input xml field '%s', adding prefix '%s'", prefix.Field, prefix.Prefix)

		data = data[:contentStart] + prefix.Prefix + content + data[endIdx:]
		edited = true
	}
	logging.D(5, "After trimming prefixes: %v", data)
	return data, edited, nil
}

// trimMetaSuffix trims specified
func (rw *NFOFileRW) xmlAppend(data string, apnd []*models.MetaAppend) (string, bool, error) {

	logging.D(5, "Entering xmlAppend with data: %v", string(data))

	if len(apnd) == 0 {
		return data, false, nil // No replacements to apply
	}

	edited := false
	for _, append := range apnd {
		if append.Field == "" || append.Suffix == "" {
			continue
		}

		startTag := fmt.Sprintf("<%s>", append.Field)
		endTag := fmt.Sprintf("</%s>", append.Field)

		startIdx := strings.Index(data, startTag)
		endIdx := strings.Index(data, endTag)
		if startIdx == -1 || endIdx == -1 {
			continue // One or both tags missing
		}

		contentStart := startIdx + len(startTag)
		content := strings.TrimSpace(data[contentStart:endIdx])

		logging.D(2, "Identified input xml field '%s', appending suffix '%s'", append.Field, append.Suffix)

		data = data[:contentStart] + content + append.Suffix + data[endIdx:]
		edited = true
	}
	logging.D(5, "After meta replacements: %v", data)
	return data, edited, nil
}

// addNewField can insert a new field which does not yet exist into the metadata file
func (rw *NFOFileRW) addNewXmlFields(data string, ow bool, new []*models.MetaNewField) (string, bool, error) {

	var (
		metaOW,
		metaPS bool
	)

	logging.D(5, "Entering addNewXmlFields with data: %v", string(data))

	if len(new) == 0 {
		return data, false, nil // No replacements to apply
	}

	if ow {
		metaOW = true
	} else {
		metaOW = config.GetBool(keys.MOverwrite)
		metaPS = config.GetBool(keys.MPreserve)
	}

	logging.D(3, "Retrieved additions for new field data: %v", new)

	newAddition := false
	ctx := context.Background()

	for _, addition := range new {
		if addition.Field == "" || addition.Value == "" {
			continue
		}

		// Special handling for actor fields
		if addition.Field == "actor" {
			// Check if actor already exists
			flatData := rw.flattenField(data)
			actorNameCheck := fmt.Sprintf("<name>%s</name>", rw.flattenField(addition.Value))

			if strings.Contains(flatData, actorNameCheck) {
				logging.I("Actor '%s' is already inserted in the metadata, no need to add...", addition.Value)
			} else {
				if modified, ok := rw.addNewActorField(data, addition.Value); ok {
					data = modified
					newAddition = true
				}
			}
			continue
		}

		// Handle non-actor fields
		tagStart := fmt.Sprintf("<%s>", addition.Field)
		tagEnd := fmt.Sprintf("</%s>", addition.Field)

		startIdx := strings.Index(data, tagStart)
		if startIdx == -1 {
			// Field doesn't exist, add it
			if modified, ok := rw.addNewField(data, fmt.Sprintf("%s%s%s", tagStart, addition.Value, tagEnd)); ok {
				data = modified
				newAddition = true
			}
			continue
		}

		// Field exists, handle overwrite
		if !metaOW {
			startContent := startIdx + len(tagStart)
			endIdx := strings.Index(data, tagEnd)
			content := strings.TrimSpace(data[startContent:endIdx])

			// Check for context cancellation
			select {
			case <-ctx.Done():
				logging.I("Operation canceled for field: %s", addition.Field)
				return data, false, fmt.Errorf("operation canceled")
			default:
				// Proceed
			}

			if !metaOW && !metaPS {
				promptMsg := fmt.Sprintf("Field '%s' already exists with value '%v' in file '%v'. Overwrite? (y/n) to proceed, (Y/N) to apply to whole queue",
					addition.Field, content, rw.File.Name())

				reply, err := prompt.PromptMetaReplace(promptMsg, metaOW, metaPS)
				if err != nil {
					logging.E(0, err.Error())
				}

				switch reply {
				case "Y":
					config.Set(keys.MOverwrite, true)
					metaOW = true
					fallthrough
				case "y":
					data = data[:startContent] + addition.Value + data[endIdx:]
					newAddition = true
				case "N":
					config.Set(keys.MPreserve, true)
					metaPS = true
					fallthrough
				case "n":
					logging.D(2, "Skipping field: %s", addition.Field)
				}
			} else if metaOW {
				data = data[:startContent] + addition.Value + data[endIdx:]
				newAddition = true
			}
		}
	}

	return data, newAddition, nil
}

// addNewField adds a new field into the NFO
func (rw *NFOFileRW) addNewField(data, addition string) (string, bool) {

	insertIdx := strings.Index(data, "<movie>")
	insertAfter := insertIdx + len("<movie>")

	if insertIdx != -1 {
		data = data[:insertAfter] + "\n" + addition + "\n" + data[insertAfter:]
	}
	return data, true
}

// addNewActorField adds a new actor into the file
func (rw *NFOFileRW) addNewActorField(data, name string) (string, bool) {
	castStart := strings.Index(data, "<cast>")
	castEnd := strings.Index(data, "</cast>")

	if castStart == -1 && castEnd == -1 {
		// No cast tag exists, create new structure
		movieStart := strings.Index(data, "<movie>")
		if movieStart == -1 {
			logging.E(0, "Invalid XML structure: no movie tag found")
			return data, false
		}

		movieEnd := strings.Index(data, "</movie>")
		if movieEnd == -1 {
			logging.E(0, "Invalid XML structure: no closing movie tag found")
			return data, false
		}

		// Create new cast section
		newCast := fmt.Sprintf("    <cast>\n        <actor>\n            <name>%s</name>\n        </actor>\n    </cast>", name)

		// Find the right spot to insert
		contentStart := movieStart + len("<movie>")
		if contentStart >= len(data) {
			logging.E(0, "Invalid XML structure: movie tag at end of data")
			return data, false
		}

		return data[:contentStart] + "\n" + newCast + "\n" + data[contentStart:], true
	}

	// Cast exists, validate indices
	if castStart == -1 || castEnd == -1 || castStart >= len(data) || castEnd > len(data) {
		logging.E(0, "Invalid XML structure: mismatched cast tags")
		return data, false
	}

	// Insert new actor
	newActor := fmt.Sprintf("    <actor>\n            <name>%s</name>\n        </actor>", name)

	if castEnd-castStart > 1 {
		// Cast has content, insert with proper spacing
		return data[:castEnd] + newActor + "\n    " + data[castEnd:], true
	} else {
		// Empty cast tag
		insertPoint := castStart + len("<cast>")
		return data[:insertPoint] + newActor + "\n    " + data[insertPoint:], true
	}
}

// flattenField flattens the metadata field for comparison
func (rw *NFOFileRW) flattenField(s string) string {

	rtn := strings.TrimSpace(s)
	rtn = strings.ReplaceAll(rtn, " ", "")
	rtn = strings.ReplaceAll(rtn, "\n", "")
	rtn = strings.ReplaceAll(rtn, "\r", "")
	rtn = strings.ReplaceAll(rtn, "\t", "")

	return rtn
}
package models

import (
	enums "metarr/internal/domain/enums"
	"os"
)

func NewFileData() *FileData {
	return &FileData{
		MTitleDesc: &MetadataTitlesDescs{},
		MCredits:   &MetadataCredits{},
		MDates:     &MetadataDates{},
		MShowData:  &MetadataShowData{},
		MWebData:   &MetadataWebData{},
		MOther:     &MetadataOtherData{},
	}
}

type FileData struct {
	// Files & dirs
	VideoDirectory        string   `json:"-" xml:"-"`
	OriginalVideoPath     string   `json:"-" xml:"-"`
	OriginalVideoBaseName string   `json:"-" xml:"-"`
	TempOutputFilePath    string   `json:"-" xml:"-"`
	FinalVideoPath        string   `json:"-" xml:"-"`
	FinalVideoBaseName    string   `json:"-" xml:"-"`
	VideoFile             *os.File `json:"-" xml:"-"`

	// Transformations
	FilenameMetaPrefix string `json:"-" xml:"-"`
	FilenameDateTag    string `json:"-" xml:"-"`
	RenamedVideoPath   string `json:"-" xml:"-"`
	RenamedMetaPath    string `json:"-" xml:"-"`

	// JSON paths
	JSONDirectory string `json:"-" xml:"-"`
	JSONFilePath  string `json:"-" xml:"-"`
	JSONBaseName  string `json:"-" xml:"-"`

	// NFO paths
	NFOBaseName  string `json:"-" xml:"-"`
	NFODirectory string `json:"-" xml:"-"`
	NFOFilePath  string `json:"-" xml:"-"`

	// Metadata
	MCredits   *MetadataCredits     `json:"meta_credits" xml:"credits"`
	MTitleDesc *MetadataTitlesDescs `json:"meta_title_description" xml:"titles"`
	MDates     *MetadataDates       `json:"meta_dates" xml:"dates"`
	MShowData  *MetadataShowData    `json:"meta_show_data" xml:"show"`
	MWebData   *MetadataWebData     `json:"meta_web_data" xml:"web"`
	MOther     *MetadataOtherData   `json:"meta_other_data" xml:"other"`
	NFOData    *NFOData

	// File writers
	JSONFileRW JSONFileRW
	NFOFileRW  NFOFileRW

	// Own transformations
	ModelMAppend     []*MetaAppend
	ModelMNewField   []*MetaNewField
	ModelMPrefix     []*MetaPrefix
	ModelMReplace    []*MetaReplace
	ModelMTrimPrefix []*MetaTrimPrefix
	ModelMTrimSuffix []*MetaTrimSuffix

	ModelFileSfxReplace []*FilenameReplaceSuffix

	// Misc
	MetaFileType      enums.MetaFiletypeFound `json:"-" xml:"-"`
	MetaAlreadyExists bool                    `json:"-" xml:"-"`
	ModelMOverwrite   bool
}
package models

import "os"

// Metadata read/write interface
type JSONFileRW interface {
	DecodeJSON(file *os.File) (map[string]interface{}, error)
	RefreshJSON() (map[string]interface{}, error)
	WriteJSON(fieldMap map[string]*string) (map[string]interface{}, error)
	MakeJSONEdits(file *os.File, fd *FileData) (bool, error)
	JSONDateTagEdits(file *os.File, fd *FileData) (edited bool, err error)
}

// Metadata read/write interface
type NFOFileRW interface {
	DecodeMetadata(file *os.File) (*NFOData, error)
	RefreshMetadata() (*NFOData, error)
	MakeMetaEdits(data string, file *os.File, fd *FileData) (bool, error)
}
package models

import (
	enums "metarr/internal/domain/enums"
	"net/http"
)

// func NewMetaReplaceSuffix(f, s, r string) *MetaReplaceSuffix {
// 	return &MetaReplaceSuffix{
// 		Field:       f,
// 		Suffix:      s,
// 		Replacement: r,
// 	}
// }

type MetaAppend struct {
	Field  string
	Suffix string
}

type MetaPrefix struct {
	Field  string
	Prefix string
}

type MetaTrimPrefix struct {
	Field  string
	Prefix string
}

type MetaTrimSuffix struct {
	Field  string
	Suffix string
}

type MetaNewField struct {
	Field string
	Value string
}

type MetaDateTag struct {
	Loc    enums.MetaDateTagLocation
	Format enums.DateFormat
}

type MetaReplace struct {
	Field       string
	Value       string
	Replacement string
}

type FilenameDatePrefix struct {
	YearLength  int
	MonthLength int
	DayLength   int
	Order       enums.DateFormat
}

type FilenameReplaceSuffix struct {
	Suffix      string
	Replacement string
}

type MetadataCredits struct {
	Override  string `json:"-"`
	Actor     string `json:"actor" xml:"actor"`
	Author    string `json:"author" xml:"author"`
	Artist    string `json:"artist" xml:"artist"`
	Channel   string `json:"channel" xml:"channel"`
	Creator   string `json:"creator" xml:"creator"`
	Studio    string `json:"studio" xml:"studio"`
	Publisher string `json:"publisher" xml:"publisher"`
	Producer  string `json:"producer" xml:"producer"`
	Performer string `json:"performer" xml:"performer"`
	Uploader  string `json:"uploader" xml:"uploader"`
	Composer  string `json:"composer" xml:"composer"`
	Director  string `json:"director" xml:"director"`
	Writer    string `json:"writer" xml:"writer"`

	Actors     []string
	Artists    []string
	Studios    []string
	Publishers []string
	Producers  []string
	Performers []string
	Composers  []string
	Directors  []string
	Writers    []string
}

type MetadataTitlesDescs struct {
	Fulltitle        string `json:"fulltitle" xml:"title"`
	Title            string `json:"title" xml:"originaltitle"`
	Subtitle         string `json:"subtitle" xml:"subtitle"`
	Description      string `json:"description" xml:"description"`
	LongDescription  string `json:"longdescription" xml:"plot"`
	Long_Description string `json:"long_description" xml:"long_description"`
	Synopsis         string `json:"synopsis" xml:"synopsis"`
	Summary          string `json:"summary" xml:"summary"`
	Comment          string `json:"comment" xml:"comment"`
}

type MetadataDates struct {
	FormattedDate           string `json:"-" xml:"-"`
	UploadDate              string `json:"upload_date" xml:"upload_date"`
	ReleaseDate             string `json:"release_date" xml:"release_date"`
	Date                    string `json:"date" xml:"date"`
	Year                    string `json:"year" xml:"year"`
	Originally_Available_At string `json:"originally_available_at" xml:"originally_available_at"`
	Creation_Time           string `json:"creation_time" xml:"creation_time"`
	StringDate              string `json:"-"`
}

type MetadataWebData struct {
	WebpageURL string         `json:"webpage_url" xml:"webpage_url"`
	VideoURL   string         `json:"url" xml:"url"`
	Domain     string         `json:"webpage_url_domain" xml:"domain"`
	Referer    string         `json:"referer" xml:"referer"`
	Cookies    []*http.Cookie `json:"-" xml:"-"`
	TryURLs    []string       `json:"-"`
}

type MetadataShowData struct {
	Show          string `json:"show" xml:"show"`
	Episode_ID    string `json:"episode_id" xml:"episode_id"`
	Episode_Sort  string `json:"episode_sort" xml:"episode_sort"`
	Season_Number string `json:"season_number" xml:"season_number"`
	Season_Title  string `json:"season_title" xml:"seasontitle"`
}

type MetadataOtherData struct {
	Language string `json:"language" xml:"language"`
	Genre    string `json:"genre" xml:"genre"`
	HD_Video string `json:"hd_video" xml:"hd_video"`
}
package models

import "encoding/xml"

// NFOData represents the complete NFO file structure
type NFOData struct {
	XMLName     xml.Name    `xml:"movie"`
	Title       Title       `xml:"title"`
	Plot        string      `xml:"plot"`
	Description string      `xml:"description"`
	Actors      []Person    `xml:"cast>actor"`
	Directors   []string    `xml:"director"`
	Producers   []string    `xml:"producer"`
	Publishers  []string    `xml:"publisher"`
	Writers     []string    `xml:"writer"`
	Studios     []string    `xml:"studio"`
	Year        string      `xml:"year"`
	Premiered   string      `xml:"premiered"`
	ReleaseDate string      `xml:"releasedate"`
	ShowInfo    ShowInfo    `xml:"showinfo"`
	WebpageInfo WebpageInfo `xml:"web"`
}

// Title represents nested title information
type Title struct {
	Main      string `xml:"main"`
	Original  string `xml:"original"`
	Sort      string `xml:"sort"`
	Sub       string `xml:"sub"`
	PlainText string `xml:",chardata"` // For non-nested titles
}

// Person represents a credited person with optional role
type Person struct {
	Name  string `xml:"name"`
	Role  string `xml:"role"`
	Order int    `xml:"order"`
	Thumb string `xml:"thumb"`
}

// ShowInfo represents TV show specific information
type ShowInfo struct {
	Show         string `xml:"show"`
	SeasonNumber string `xml:"season>number"`
	EpisodeID    string `xml:"episode>number"`
	EpisodeTitle string `xml:"episode>title"`
}

// ShowInfo represents TV show specific information
type WebpageInfo struct {
	URL    string `xml:"url"`
	Fanart string `xml:"fanart"`
	Thumb  string `xml:"thumb"`
}
package models

import "regexp"

type ContractionPattern struct {
	Regexp      *regexp.Regexp
	Replacement string
}
package models

// SelectorRule holds rules for specific websites for use in scrapers
type SelectorRule struct {
	Selector string
	Attr     string // empty for text content, otherwise attribute name
	Process  func(string) string
	JsonPath []string
}

type CustomCookieSource struct {
	Browser string
	Dir     string
}
package processing

import (
	"context"
	"fmt"
	"metarr/internal/config"
	consts "metarr/internal/domain/constants"
	enums "metarr/internal/domain/enums"
	keys "metarr/internal/domain/keys"
	"metarr/internal/ffmpeg"
	reader "metarr/internal/metadata/reader"
	"metarr/internal/models"
	"metarr/internal/transformations"
	fsRead "metarr/internal/utils/fs/read"
	logging "metarr/internal/utils/logging"
	"os"
	"sync"
	"sync/atomic"
)

var (
	totalMetaFiles,
	totalVideoFiles,
	processedMetaFiles,
	processedVideoFiles int32

	processedDataArray []*models.FileData
	failedVideos       []failedVideo
)

type failedVideo struct {
	filename string
	err      string
}

// processFiles is the main program function to process folder entries
func ProcessFiles(ctx context.Context, cancel context.CancelFunc, wg *sync.WaitGroup, cleanupChan chan os.Signal, openVideo, openMeta *os.File) {

	skipVideos := config.GetBool(keys.SkipVideos)

	var (
		videoMap,
		metaMap,
		matchedFiles map[string]*models.FileData
		err error
	)

	// Process metadata, checking if it’s a directory or a single file
	if openMeta != nil {
		fileInfo, _ := openMeta.Stat()

		if fileInfo.IsDir() {
			if fileInfo.Size() == 0 {
				failedVideos = append(failedVideos, failedVideo{
					filename: openMeta.Name(),
					err:      "Meta directory size is 0",
				})
			} else {
				metaMap, err = fsRead.GetMetadataFiles(openMeta)
			}
		} else {
			if fileInfo.Size() == 0 {
				failedVideos = append(failedVideos, failedVideo{
					filename: openMeta.Name(),
					err:      "Meta file size is 0",
				})
			} else {
				metaMap, err = fsRead.GetSingleMetadataFile(openMeta)
			}
		}
		if err != nil {
			logging.E(0, "Error: %v", err)

			failedVideos = append(failedVideos, failedVideo{
				filename: openMeta.Name(),
				err:      err.Error(),
			})

			os.Exit(1)
		}
	}
	// Process video files, checking if it’s a directory or a single file
	if openVideo != nil {
		fileInfo, _ := openVideo.Stat()

		if fileInfo.IsDir() {
			if fileInfo.Size() == 0 {
				failedVideos = append(failedVideos, failedVideo{
					filename: openVideo.Name(),
					err:      "Video directory size is 0",
				})
			} else {
				videoMap, err = fsRead.GetVideoFiles(openVideo)
			}
		}

	} else if !skipVideos {
		fileInfo, _ := openVideo.Stat()

		if fileInfo.Size() == 0 {
			failedVideos = append(failedVideos, failedVideo{
				filename: openVideo.Name(),
				err:      "Video file size is 0",
			})
		} else {
			videoMap, err = fsRead.GetSingleVideoFile(openVideo)
		}
	}
	if err != nil {
		logging.E(0, "Error fetching video files: %v", err)

		failedVideos = append(failedVideos, failedVideo{
			filename: openVideo.Name(),
			err:      err.Error(),
		})

		os.Exit(1)
	}

	// Match video and metadata files
	if !skipVideos {
		matchedFiles, err = fsRead.MatchVideoWithMetadata(videoMap, metaMap)
		if err != nil {
			logging.E(0, "Error matching videos with metadata: %v", err)
			os.Exit(1)
		}
	} else {
		matchedFiles = metaMap
	}

	config.Set(keys.VideoMap, videoMap)
	config.Set(keys.MetaMap, metaMap)

	atomic.StoreInt32(&totalMetaFiles, int32(len(metaMap)))
	atomic.StoreInt32(&totalVideoFiles, int32(len(videoMap)))

	logging.I("Found %d file(s) to process in the directoryfmt.Printf", totalMetaFiles+totalVideoFiles)
	logging.D(3, "Matched metafiles: %v", matchedFiles)

	for _, fileData := range matchedFiles {
		var (
			processedData *models.FileData
			err           error
		)

		if !config.IsSet(keys.SkipVideos) || metaChanges() {
			switch fileData.MetaFileType {
			case enums.METAFILE_JSON:
				logging.D(3, "File: %s: Meta file type in model as %v", fileData.JSONFilePath, fileData.MetaFileType)
				processedData, err = reader.ProcessJSONFile(fileData)

			case enums.METAFILE_NFO:
				logging.D(3, "File: %s: Meta file type in model as %v", fileData.NFOFilePath, fileData.MetaFileType)
				processedData, err = reader.ProcessNFOFiles(fileData)
			}
			if err != nil {
				logging.ErrorArray = append(logging.ErrorArray, err)
				errMsg := fmt.Errorf("error processing metadata for file '%s': %w", fileData.OriginalVideoPath, err)
				logging.E(0, errMsg.Error())

				failedVideos = append(failedVideos, failedVideo{
					filename: fileData.OriginalVideoPath,
					err:      errMsg.Error(),
				})

				continue
			}
			processedDataArray = append(processedDataArray, processedData)
		} else {
			processedDataArray = append(processedDataArray, fileData)
		}
	}

	// Goroutine to handle signals and cleanup
	go func() {
		<-cleanupChan

		fmt.Println("\nSignal received, cleaning up temporary files...")
		cancel()

		err = cleanupTempFiles(videoMap)
		if err != nil {
			logging.ErrorArray = append(logging.ErrorArray, err)
			fmt.Printf("\nFailed to cleanup temp files: %v", err)
			logging.E(0, "Failed to cleanup temp files", err)
		}
		logging.I("Process was interrupted by a syscall", nil)

		if len(failedVideos) > 0 {
			logging.P(consts.RedError + "Failed videos:")
			for _, failed := range failedVideos {
				fmt.Println()
				logging.P("Filename: %v", failed.filename)
				logging.P("Error: %v", failed.err)
			}
			fmt.Println()
		}

		wg.Wait()
		os.Exit(0)
	}()

	sem := make(chan struct{}, config.GetInt(keys.Concurrency))

	for fileName, fileData := range matchedFiles {
		executeFile(ctx, wg, sem, fileName, fileData)
	}

	wg.Wait()

	err = cleanupTempFiles(videoMap)
	if err != nil {
		logging.ErrorArray = append(logging.ErrorArray, err)
		logging.E(0, "Failed to cleanup temp files: %v", err)
	}

	var (
		replaceToStyle enums.ReplaceToStyle
		ok             bool
	)

	if config.IsSet(keys.Rename) {
		if replaceToStyle, ok = config.Get(keys.Rename).(enums.ReplaceToStyle); !ok {
			logging.E(0, "Received wrong type for rename style. Got %T", replaceToStyle)
		} else {
			logging.D(2, "Got rename style as %T index %v", replaceToStyle, replaceToStyle)
		}
	}

	inputVideoDir := config.GetString(keys.JsonDir)

	err = transformations.FileRename(processedDataArray, replaceToStyle)
	if err != nil {
		logging.ErrorArray = append(logging.ErrorArray, err)
		logging.E(0, "Failed to rename files: %v", err)
	} else {
		logging.S(0, "Successfully formatted file names in directory: %v", inputVideoDir)
	}

	if len(logging.ErrorArray) == 0 || logging.ErrorArray == nil {

		logging.S(0, "Successfully processed all videos in directory (%v) with no errors.", inputVideoDir)
		fmt.Println()
	} else {

		logging.E(0, "Program finished, but some errors were encountered: %v", logging.ErrorArray)

		if len(failedVideos) > 0 {
			logging.P(consts.RedError + "Failed videos:")
			for _, failed := range failedVideos {
				fmt.Println()
				logging.P("Filename: %v", failed.filename)
				logging.P("Error: %v", failed.err)
			}
		}
		fmt.Println()
	}
}

// processFile handles processing for both video and metadata files
func executeFile(ctx context.Context, wg *sync.WaitGroup, sem chan struct{}, fileName string, fileData *models.FileData) {
	wg.Add(1)
	go func(fileName string, fileData *models.FileData) {

		defer wg.Done()

		currentFile := atomic.AddInt32(&processedMetaFiles, 1)
		total := atomic.LoadInt32(&totalMetaFiles)

		fmt.Printf("\n====================================================\n")
		fmt.Printf("    Processed metafile %d of %d\n", currentFile, total)
		fmt.Printf("    Remaining: %d\n", total-currentFile)
		fmt.Printf("====================================================\n\n")

		sem <- struct{}{}
		defer func() {
			<-sem
		}()

		select {
		case <-ctx.Done():
			fmt.Printf("Skipping processing for %s due to cancellation\n", fileName)
			return
		default:
		}

		sysResourceLoop(fileName)

		skipVideos := config.GetBool(keys.SkipVideos)
		isVideoFile := fileData.OriginalVideoPath != ""

		if isVideoFile {
			logging.I("Processing file: %s", fileName)
		} else {
			logging.I("Processing metadata file: %s", fileName)
		}

		if isVideoFile && !skipVideos {
			err := ffmpeg.ExecuteVideo(fileData)
			if err != nil {
				logging.ErrorArray = append(logging.ErrorArray, err)
				errMsg := fmt.Errorf("failed to process video '%v': %w", fileName, err)
				logging.E(0, errMsg.Error())

				failedVideos = append(failedVideos, failedVideo{
					filename: fileName,
					err:      errMsg.Error(),
				})

			} else {
				logging.S(0, "Successfully processed video %s", fileName)
			}
		} else {
			logging.S(0, "Successfully processed metadata for %s", fileName)
		}

		currentFile = atomic.AddInt32(&processedVideoFiles, 1)
		total = atomic.LoadInt32(&totalVideoFiles)

		fmt.Printf("\n====================================================\n")
		fmt.Printf("    Processed video file %d of %d\n", currentFile, total)
		fmt.Printf("    Remaining: %d\n", total-currentFile)
		fmt.Printf("====================================================\n\n")

	}(fileName, fileData)
}
package processing

import (
	"fmt"
	"metarr/internal/config"
	keys "metarr/internal/domain/keys"
	"metarr/internal/models"
	logging "metarr/internal/utils/logging"
	"os"
	"sync"
	"time"

	"github.com/shirou/gopsutil/cpu"
	"github.com/shirou/gopsutil/mem"
)

var (
	muResource sync.Mutex
)

// sysResourceLoop checks the system resources, controlling whether a new routine should be spawned
func sysResourceLoop(fileStr string) {
	var (
		resourceMsg bool
		backoff     = time.Second
		maxBackoff  = 10 * time.Second
	)

	memoryThreshold := config.GetUint64(keys.MinMemMB)

	for {
		// Fetch system resources and determine if processing can proceed
		muResource.Lock()
		proceed, availableMemory, CPUUsage, err := checkSysResources(memoryThreshold)
		muResource.Unlock()

		if err != nil {
			logging.ErrorArray = append(logging.ErrorArray, err)
			logging.E(0, "Error checking system resources: %v", err)

			time.Sleep(backoff)
			backoff *= 2
			if backoff > maxBackoff {
				backoff = maxBackoff
			}
			continue
		}

		if proceed {
			resourceMsg = false
			break
		}

		// Log resource info only once when insufficient resources are detected
		if !resourceMsg {
			logging.I("Not enough system resources to process %s, waiting...", fileStr)
			logging.D(1, "Memory available: %.2f MB\tCPU usage: %.2f%%\n", float64(availableMemory)/(1024*1024), CPUUsage)
			resourceMsg = true
		}

		time.Sleep(backoff)
		backoff *= 2
		if backoff > maxBackoff {
			backoff = maxBackoff
		}
	}
}

// checkAvailableMemory checks if enough memory is available (at least the threshold).
func checkSysResources(requiredMemory uint64) (bool, uint64, float64, error) {
	vMem, err := mem.VirtualMemory()
	if err != nil {
		return false, 0, 0, err
	}

	cpuPct, err := cpu.Percent(0, false)
	if err != nil {
		return false, 0, 0, err
	}

	maxCpuUsage := config.GetFloat64(keys.MaxCPU)
	return (vMem.Available >= requiredMemory && cpuPct[0] <= maxCpuUsage), vMem.Available, cpuPct[0], nil
}

// cleanupTempFiles removes temporary files
func cleanupTempFiles(files map[string]*models.FileData) error {

	var (
		errReturn error
		path      string
	)

	for _, data := range files {
		path = data.TempOutputFilePath
		if _, err := os.Stat(path); err == nil {
			fmt.Printf("Removing temp file: %s\n", path)
			err = os.Remove(path)
			if err != nil {
				errReturn = fmt.Errorf("error removing temp file: %w", err)
			}
		}
	}
	return errReturn
}

// metaChanges determines if metadata should be processed
func metaChanges() bool {
	if config.IsSet(keys.MAppend) {
		return true
	}
	if config.IsSet(keys.MPrefix) {
		return true
	}
	if config.IsSet(keys.MNewField) {
		return true
	}
	if config.IsSet(keys.MTrimPrefix) {
		return true
	}
	if config.IsSet(keys.MTrimSuffix) {
		return true
	}
	if config.IsSet(keys.FileDateFmt) {
		return true
	}
	return false
}
package transformations

import (
	config "metarr/internal/config"
	keys "metarr/internal/domain/keys"
	"metarr/internal/models"
	logging "metarr/internal/utils/logging"
)

// CensoredTvTransformations adds preset transformations to
// files for censored.tv videos
func CensoredTvTransformations(fd *models.FileData) {

	logging.I("Making preset censored.tv meta replacements")

	censoredTvTrimSuffixes(fd)
	censoredTvFSuffixes(fd)
}

// censoredTvMSuffixes adds meta suffix replacements
func censoredTvTrimSuffixes(fd *models.FileData) {

	var (
		trimSfx []*models.MetaTrimSuffix
		ok      bool
	)

	if config.IsSet(keys.MTrimSuffix) {
		trimSfx, ok = config.Get(keys.MTrimSuffix).([]*models.MetaTrimSuffix)
		if !ok {
			logging.E(2, "Got type %T, may be null", trimSfx)
		}
	}

	var new = make([]*models.MetaTrimSuffix, 0, 4)

	new = append(new, &models.MetaTrimSuffix{
		Field:  "title",
		Suffix: " (1)",
	})

	new = append(new, &models.MetaTrimSuffix{
		Field:  "fulltitle",
		Suffix: " (1)",
	})

	new = append(new, &models.MetaTrimSuffix{
		Field:  "id",
		Suffix: "-1",
	})

	new = append(new, &models.MetaTrimSuffix{
		Field:  "display_id",
		Suffix: "-1",
	})

	for _, newSuffix := range new {
		exists := false
		for _, existingSuffix := range trimSfx {
			if existingSuffix.Field == newSuffix.Field {
				exists = true
				break
			}
		}
		if !exists {
			logging.I("Adding new censored.tv meta suffix replacement: %v", newSuffix)
			trimSfx = append(trimSfx, newSuffix)
		}
	}

	if logging.Level >= 2 {
		var entries []string
		for _, entry := range trimSfx {
			entries = append(entries, "("+entry.Field+":")
			entries = append(entries, entry.Suffix+")")
		}
		logging.I("After adding preset suffixes, suffixes to be trimmed for '%s': %v", fd.OriginalVideoBaseName, entries)
	}

	fd.ModelMTrimSuffix = trimSfx
}

// censoredTvFSuffixes adds filename suffix replacements
func censoredTvFSuffixes(fd *models.FileData) {

	var sfx []*models.FilenameReplaceSuffix

	v := fd.OriginalVideoBaseName

	if config.IsSet(keys.FilenameReplaceSfx) {
		existingSfx, ok := config.Get(keys.FilenameReplaceSfx).([]*models.FilenameReplaceSuffix)
		if !ok {
			logging.E(2, "Unexpected type %T, initializing new suffix list.", existingSfx)
		} else {
			sfx = existingSfx
		}
	}

	logging.D(3, "Retrieved file name: %s", v)
	vExt := ""
	if len(v) > 1 {
		check := v[len(v)-2:]
		logging.D(3, "Got last element of file name: %s", check)
		switch check {
		case " 1", "_1":
			vExt = check
			logging.D(2, "Found file name suffix: %s", vExt)
		}
	}

	// Check if suffix is already present
	alreadyExists := false
	for _, existingSuffix := range sfx {
		if existingSuffix.Suffix == vExt && existingSuffix.Replacement == "" {
			alreadyExists = true
			break
		}
	}

	// Add suffix if it does not already exist
	if !alreadyExists {
		sfx = append(sfx, &models.FilenameReplaceSuffix{
			Suffix:      "_1",
			Replacement: "",
		})
		logging.I("Added filename suffix replacement '%s'", vExt)
	}

	fd.ModelFileSfxReplace = sfx
	logging.I("Total filename suffix replacements: %d", len(sfx))
}
package transformations

import (
	"fmt"
	"metarr/internal/config"
	enums "metarr/internal/domain/enums"
	keys "metarr/internal/domain/keys"
	"metarr/internal/models"
	writefs "metarr/internal/utils/fs/write"
	logging "metarr/internal/utils/logging"
	validate "metarr/internal/utils/validation"
	"path/filepath"
	"strings"
)

// FileRename formats the file names
func FileRename(dataArray []*models.FileData, style enums.ReplaceToStyle) error {
	var (
		vidExt,
		renamedVideo,
		renamedMeta string
	)

	skipVideos := config.GetBool(keys.SkipVideos)

	for _, fd := range dataArray {
		metaBase, metaDir, originalMPath := getMetafileData(fd)
		metaExt := filepath.Ext(originalMPath)

		videoBase := fd.FinalVideoBaseName
		originalVPath := fd.FinalVideoPath

		// Ensure we have the proper video extension
		if config.IsSet(keys.OutputFiletype) {
			vidExt = validate.ValidateExtension(config.GetString(keys.OutputFiletype))
			if vidExt == "" {
				vidExt = filepath.Ext(originalVPath)
			}
		} else {
			vidExt = filepath.Ext(originalVPath)
		}

		if !skipVideos {
			renamedVideo = renameFile(videoBase, style, fd)
			renamedMeta = renamedVideo // Use video name as base to ensure best filename consistency
			logging.D(2, "Renamed video to '%s' with extension '%s'", renamedVideo, vidExt)
		} else {
			renamedMeta = renameFile(metaBase, style, fd)
		}

		var err error
		if renamedVideo, renamedMeta, err = fixContractions(renamedVideo, renamedMeta, style); err != nil {
			return fmt.Errorf("failed to fix contractions for %s. error: %v", renamedVideo, err)
		}

		// Add the metatag to the front of the filenames
		renamedVideo, renamedMeta = addTags(renamedVideo, renamedMeta, fd, style)

		// Trim trailing spaces
		renamedVideo = strings.TrimSpace(renamedVideo)
		renamedMeta = strings.TrimSpace(renamedMeta)

		logging.D(2, "Rename replacements:\nVideo: %v\nMetafile: %v", renamedVideo, renamedMeta)

		// Construct final output filepaths - ensure video gets its extension
		renamedVPath := filepath.Join(fd.VideoDirectory, renamedVideo+vidExt) // Add extension here
		renamedMPath := filepath.Join(metaDir, renamedMeta+metaExt)

		// Log the complete paths to verify extension
		logging.D(1, "Final paths with extensions:\nVideo: %s\nMeta: %s", renamedVPath, renamedMPath)

		// Save into model
		fd.RenamedVideoPath = renamedVPath
		fd.RenamedMetaPath = renamedMPath

		fsWriter := writefs.NewFSFileWriter(skipVideos, renamedVPath, originalVPath, renamedMPath, originalMPath)

		if err := fsWriter.WriteResults(); err != nil {
			return err
		}

		var deletedMeta bool
		if config.IsSet(keys.MetaPurge) {
			if err, deletedMeta = fsWriter.DeleteMetafile(renamedMPath); err != nil {
				logging.E(0, "Failed to purge metafile: %v", err)
			}
		}

		if config.IsSet(keys.MoveOnComplete) {
			if err := fsWriter.MoveFile(deletedMeta); err != nil {
				logging.E(0, "Failed to move to destination folder: %v", err)
			}
		}
	}
	return nil
}

// Performs name transformations for metafiles
func renameFile(fileBase string, style enums.ReplaceToStyle, fd *models.FileData) string {
	logging.D(2, "Processing metafile base name: %q", fileBase)

	var (
		suffixes []*models.FilenameReplaceSuffix
		ok       bool
	)

	if len(fd.ModelFileSfxReplace) > 0 {
		suffixes = fd.ModelFileSfxReplace
	} else if config.IsSet(keys.FilenameReplaceSfx) {
		suffixes, ok = config.Get(keys.FilenameReplaceSfx).([]*models.FilenameReplaceSuffix)
		if !ok && len(fd.ModelFileSfxReplace) == 0 {
			logging.E(0, "Got wrong type %T for filename replace suffixes", suffixes)
			return fileBase
		}
	}

	if len(suffixes) == 0 && style == enums.RENAMING_SKIP {
		return fileBase
	} else if len(suffixes) > 0 {
		fileBase = replaceSuffix(fileBase, suffixes)
	}

	if style != enums.RENAMING_SKIP {
		fileBase = applyNamingStyle(style, fileBase)
	} else {
		logging.D(1, "No naming style selected, skipping rename style")
	}
	return fileBase
}
package transformations

import (
	"fmt"
	enums "metarr/internal/domain/enums"
	"metarr/internal/domain/regex"
	"metarr/internal/models"
	presets "metarr/internal/transformations/presets"
	logging "metarr/internal/utils/logging"
	"strings"
	"unicode"
)

// TryTransPresets checks if any URLs in the video metadata have a known match.
// Applies preset transformations for those which match.
func TryTransPresets(urls []string, fd *models.FileData) (matches string) {

	for _, url := range urls {
		switch {
		case strings.Contains(url, "censored.tv"):
			presets.CensoredTvTransformations(fd)
			logging.I("Found transformation preset for URL '%s'", url)
			return url
		}
	}
	return ""
}

// getMetafileData retrieves meta type specific data.
func getMetafileData(m *models.FileData) (metaBase, metaDir, metaPath string) {

	switch m.MetaFileType {
	case enums.METAFILE_JSON:
		return m.JSONBaseName, m.JSONDirectory, m.JSONFilePath
	case enums.METAFILE_NFO:
		return m.NFOBaseName, m.NFODirectory, m.NFOFilePath
	default:
		logging.E(0, "No metafile type set in model %v", m)
		return "", "", ""
	}
}

// applyNamingStyle applies renaming conventions.
func applyNamingStyle(style enums.ReplaceToStyle, input string) (output string) {

	switch style {
	case enums.RENAMING_SPACES:
		output = strings.ReplaceAll(input, "_", " ")
	case enums.RENAMING_UNDERSCORES:
		output = strings.ReplaceAll(input, " ", "_")
	default:
		logging.I("Skipping space or underscore renaming conventions...")
		output = input
	}
	return output
}

// addTags handles the tagging of the video files where necessary.
func addTags(renamedVideo, renamedMeta string, m *models.FileData, style enums.ReplaceToStyle) (string, string) {

	if len(m.FilenameMetaPrefix) > 2 {
		renamedVideo = fmt.Sprintf("%s %s", m.FilenameMetaPrefix, renamedVideo)
		renamedMeta = fmt.Sprintf("%s %s", m.FilenameMetaPrefix, renamedMeta)
	}

	if len(m.FilenameDateTag) > 2 {
		renamedVideo = fmt.Sprintf("%s %s", m.FilenameDateTag, renamedVideo)
		renamedMeta = fmt.Sprintf("%s %s", m.FilenameDateTag, renamedMeta)
	}

	if style == enums.RENAMING_UNDERSCORES {
		renamedVideo = strings.ReplaceAll(renamedVideo, " ", "_")
		renamedMeta = strings.ReplaceAll(renamedMeta, " ", "_")
	}

	return renamedVideo, renamedMeta
}

// fixContractions fixes the contractions created by FFmpeg's restrict-filenames flag.
func fixContractions(videoFilename, metaFilename string, style enums.ReplaceToStyle) (string, string, error) {

	contractionsMap := make(map[string]*models.ContractionPattern)
	// Rename style map to use
	switch style {

	case enums.RENAMING_SPACES:
		contractionsMap = regex.ContractionMapSpacesCompile()

	case enums.RENAMING_UNDERSCORES:
		contractionsMap = regex.ContractionMapUnderscoresCompile()

	case enums.RENAMING_FIXES_ONLY:
		contractionsMap = regex.ContractionMapAllCompile()

	default:
		return videoFilename, metaFilename, nil
	}

	videoFilename = replaceLoneS(videoFilename, style)
	metaFilename = replaceLoneS(metaFilename, style)

	fmt.Printf("After replacement - Video: %s, Meta: %s\n", videoFilename, metaFilename)

	// Function to replace contractions in a filename
	replaceContractions := func(filename string) string {
		for _, replacement := range contractionsMap {
			repIdx := replacement.Regexp.FindStringIndex(strings.ToLower(filename))
			if repIdx == nil {
				continue
			}

			var b strings.Builder
			b.Grow(len(replacement.Replacement))
			originalContraction := filename[repIdx[0]:repIdx[1]]

			// Match original case for each character in the replacement
			for i, char := range replacement.Replacement {
				if i < len(originalContraction) && unicode.IsUpper(rune(originalContraction[i])) {
					b.WriteString(strings.ToUpper(string(char)))
				} else {
					b.WriteString(string(char))
				}
			}

			// Replace in filename with adjusted case
			filename = filename[:repIdx[0]] + b.String() + filename[repIdx[1]:]
			b.Reset()
		}

		logging.D(2, "Made contraction replacements for file '%s'", filename)
		return filename
	}

	// Replace contractions in both filenames
	videoFilename = strings.TrimSpace(videoFilename)
	metaFilename = strings.TrimSpace(metaFilename)
	return replaceContractions(videoFilename),
		replaceContractions(metaFilename),
		nil
}

// replaceSuffix applies configured suffix replacements to a filename.
func replaceSuffix(filename string, suffixes []*models.FilenameReplaceSuffix) string {

	logging.D(2, "Received filename %s", filename)

	if len(suffixes) == 0 {
		logging.D(1, "No suffix replacements configured, keeping original filename: %q", filename)
		return filename
	}

	logging.D(2, "Processing filename %s with suffixes: %v", filename, suffixes)

	var result string
	for _, suffix := range suffixes {
		logging.D(2, "Checking suffix '%s' against filename '%s'", suffix.Suffix, filename)

		if strings.HasSuffix(filename, suffix.Suffix) {
			result = strings.TrimSuffix(filename, suffix.Suffix) + suffix.Replacement
			logging.D(2, "Applied suffix replacement: %q -> %q", suffix.Suffix, suffix.Replacement)
		}
	}

	if result != "" {
		logging.D(2, "Suffix replacement complete: %s -> %s", filename, result)
		return result
	}

	return filename
}

// replaceLoneS performs replacements without regex
func replaceLoneS(f string, style enums.ReplaceToStyle) string {
	if style == enums.RENAMING_SKIP {
		return f
	}

	prevString := ""

	// Keep replacing until no more changes occur
	// fixes accidental double spaces or double underscores
	// in the "s" contractions
	for f != prevString {
		prevString = f

		if style == enums.RENAMING_SPACES || style == enums.RENAMING_FIXES_ONLY {
			if strings.HasSuffix(f, " s") {
				f = f[:len(f)-2] + "s"
			}

			f = strings.ReplaceAll(f, " s ", "s ")
			f = strings.ReplaceAll(f, " s.", "s.")
			f = strings.ReplaceAll(f, " s[", "s[")
			f = strings.ReplaceAll(f, " s(", "s(")
			f = strings.ReplaceAll(f, " s)", "s)")
			f = strings.ReplaceAll(f, " s]", "s]")
			f = strings.ReplaceAll(f, " s-", "s-")
			f = strings.ReplaceAll(f, " s_", "s_")
			f = strings.ReplaceAll(f, " s,", "s,")
			f = strings.ReplaceAll(f, " s!", "s!")
			f = strings.ReplaceAll(f, " s'", "s'")
			f = strings.ReplaceAll(f, " s&", "s&")
			f = strings.ReplaceAll(f, " s=", "s=")
			f = strings.ReplaceAll(f, " s;", "s;")
			f = strings.ReplaceAll(f, " s#", "s#")
			f = strings.ReplaceAll(f, " s@", "s@")
			f = strings.ReplaceAll(f, " s$", "s$")
			f = strings.ReplaceAll(f, " s%", "s%")
			f = strings.ReplaceAll(f, " s+", "s+")
			f = strings.ReplaceAll(f, " s{", "s{")
			f = strings.ReplaceAll(f, " s}", "s}")
		}

		if style == enums.RENAMING_UNDERSCORES || style == enums.RENAMING_FIXES_ONLY {
			if strings.HasSuffix(f, "_s") {
				f = f[:len(f)-2] + "s"
			}

			f = strings.ReplaceAll(f, "_s_", "s_")
			f = strings.ReplaceAll(f, "_s.", "s.")
			f = strings.ReplaceAll(f, "_s[", "s[")
			f = strings.ReplaceAll(f, "_s(", "s(")
			f = strings.ReplaceAll(f, "_s)", "s)")
			f = strings.ReplaceAll(f, "_s]", "s]")
			f = strings.ReplaceAll(f, "_s-", "s-")
			f = strings.ReplaceAll(f, "_s ", "s ")
			f = strings.ReplaceAll(f, "_s,", "s,")
			f = strings.ReplaceAll(f, "_s!", "s!")
			f = strings.ReplaceAll(f, "_s'", "s'")
			f = strings.ReplaceAll(f, "_s&", "s&")
			f = strings.ReplaceAll(f, "_s=", "s=")
			f = strings.ReplaceAll(f, "_s;", "s;")
			f = strings.ReplaceAll(f, "_s#", "s#")
			f = strings.ReplaceAll(f, "_s@", "s@")
			f = strings.ReplaceAll(f, "_s$", "s$")
			f = strings.ReplaceAll(f, "_s%", "s%")
			f = strings.ReplaceAll(f, "_s+", "s+")
			f = strings.ReplaceAll(f, "_s{", "s{")
			f = strings.ReplaceAll(f, "_s}", "s}")
		}
	}
	return f
}
package utils

import (
	consts "metarr/internal/domain/constants"
)

// cookieFilePatterns holds the various filenames for cookie/auth related files by browser
var cookieFilePatterns = map[string][]string{

	consts.BrowserFirefox: {
		// Core cookie and storage files
		"cookies.sqlite",
		"cookies.sqlite-shm",
		"cookies.sqlite-wal",
		"storage.sqlite",
		"storage.sqlite-shm",
		"storage.sqlite-wal",
		"webappsstore.sqlite",

		// Encryption and security
		"key*.db",
		"key*.db-journal",
		"cert9.db",
		"cert8.db",
		"pkcs11.txt",
		"secmod.db",
		"logins.json",
		"passwords.json",
		"signons.sqlite",

		// History and places
		"places.sqlite",
		"places.sqlite-shm",
		"places.sqlite-wal",
		"favicons.sqlite",
		"favicons.sqlite-shm",
		"favicons.sqlite-wal",

		// Session and preferences
		"prefs.js",
		"user.js",
		"sessionstore.js",
		"sessionstore.jsonlz4",
		"sessionCheckpoints.json",
		"permissions.sqlite",
		"extensions.json",
		"containers.json",
	},

	consts.BrowserChrome: {
		// Core files
		"Cookies",
		"Cookies-journal",
		"Login Data",
		"Login Data-journal",
		"Web Data",
		"Web Data-journal",

		// Local storage
		"Local Storage/*",
		"IndexedDB/*",

		// Preferences and sync
		"Preferences",
		"Secure Preferences",
		"Session Storage/*",

		// History
		"History",
		"History-journal",
		"Visited Links",

		// Authentication
		"Origin Bound Certs",
		"*.jwt", // Auth token
		"Network/*",
		"Extension Cookies",
		"Extension Cookies-journal",
	},

	consts.BrowserEdge: { // Similar to Chrome as it's Chromium-based
		"Cookies",
		"Cookies-journal",
		"Login Data",
		"Login Data-journal",
		"Web Data",
		"Web Data-journal",
		"Local Storage/*",
		"IndexedDB/*",
		"Preferences",
		"Secure Preferences",
		"Session Storage/*",
		"History",
		"History-journal",
		"Network/*",
		"Extension Cookies",
		"Extension Cookies-journal",
		"*.jwt",           // Edge-specific
		"Microsoft Edge*", // Edge-specific files
	},

	consts.BrowserSafari: { // macOS only
		"Cookies.binarycookies",
		"Cookies.plist",
		"LastSession.plist",
		"History.db",
		"History.db-shm",
		"History.db-wal",
		"Downloads.plist",
		"Extensions.plist",
		"Databases/*",
		"LocalStorage/*",
		"WebKit/WebsiteData/*",
		"Preferences.plist",
		"TopSites.plist",
		"ApplicationCache.db",
		"Cache.db",
		"PerSite/*",
		"SafeBrowsing/*",
	},
}
package utils

import (
	"fmt"
	"metarr/internal/config"
	keys "metarr/internal/domain/keys"
	"metarr/internal/models"
	logging "metarr/internal/utils/logging"
	"net/http"
	"net/url"
	"os"
	"path/filepath"
	"strings"

	"github.com/browserutils/kooky"
	_ "github.com/browserutils/kooky/browser/all"
	"github.com/browserutils/kooky/browser/chrome"
	"github.com/browserutils/kooky/browser/firefox"
	"github.com/browserutils/kooky/browser/safari"
)

var (
	allStores  []kooky.CookieStore
	allCookies []*http.Cookie
)

// initializeCookies initializes all browser cookie stores
func initializeCookies() {
	allStores = kooky.FindAllCookieStores()
	allCookies = []*http.Cookie{}
}

// GetBrowserCookies retrieves cookies for a given URL, using a specified cookie file if provided.
func getBrowserCookies(url string) ([]*http.Cookie, error) {
	baseURL, err := extractBaseDomain(url)
	if err != nil {
		return nil, fmt.Errorf("failed to extract base domain: %v", err)
	}

	cookieFilePath := config.GetString(keys.CookiePath)

	// If a cookie file path is provided, use it
	if cookieFilePath != "" {
		logging.D(2, "Reading cookies from specified file: %s", cookieFilePath)
		kookyCookies, err := readCookieFile(cookieFilePath)
		if err != nil {
			return nil, fmt.Errorf("failed to read cookies from file: %v", err)
		}
		return convertToHTTPCookies(kookyCookies), nil
	}

	// Otherwise, proceed to use browser cookie stores
	if allStores == nil || allCookies == nil || len(allCookies) == 0 {
		initializeCookies()
	}

	attemptedBrowsers := make(map[string]bool, len(allStores))

	for _, store := range allStores {
		browserName := store.Browser()
		logging.D(2, "Attempting to read cookies from %s", browserName)
		attemptedBrowsers[browserName] = true

		cookies, err := store.ReadCookies(kooky.Valid, kooky.Domain(baseURL))
		if err != nil {
			logging.D(2, "Failed to read cookies from %s: %v", browserName, err)
			continue
		}

		if len(cookies) > 0 {
			logging.I("Successfully read %d cookies from %s for domain %s", len(cookies), browserName, baseURL)
			allCookies = append(allCookies, convertToHTTPCookies(cookies)...)
		} else {
			logging.D(2, "No cookies found for %s", browserName)
		}
	}

	// Log summary of attempted browsers
	logging.I("Attempted to read cookies from the following browsers: %v", keysFromMap(attemptedBrowsers))

	if len(allCookies) == 0 {
		logging.I("No cookies found for '%s', proceeding without cookies", url)
	} else {
		logging.I("Found a total of %d cookies for '%s'", len(allCookies), url)
	}

	return allCookies, nil
}

// convertToHTTPCookies converts kooky cookies to http.Cookie format
func convertToHTTPCookies(kookyCookies []*kooky.Cookie) []*http.Cookie {
	httpCookies := make([]*http.Cookie, len(kookyCookies))
	for i, c := range kookyCookies {
		httpCookies[i] = &http.Cookie{
			Name:   c.Name,
			Value:  c.Value,
			Path:   c.Path,
			Domain: c.Domain,
			Secure: c.Secure,
		}
	}
	return httpCookies
}

// extractBaseDomain parses a URL and extracts its base domain
func extractBaseDomain(urlString string) (string, error) {
	parsedURL, err := url.Parse(urlString)
	if err != nil {
		return "", err
	}

	parts := strings.Split(parsedURL.Hostname(), ".")
	if len(parts) > 2 {
		return strings.Join(parts[len(parts)-2:], "."), nil
	}
	return parsedURL.Hostname(), nil
}

// keysForMap helper function to get keys from a map
func keysFromMap(m map[string]bool) []string {
	keys := make([]string, 0, len(m))
	for k := range m {
		keys = append(keys, k)
	}
	return keys
}

// readCookieFile reads cookies from the specified cookie file
func readCookieFile(cookieFilePath string) ([]*kooky.Cookie, error) {
	var store kooky.CookieStore
	var err error

	// Attempt to identify and read cookies based on known browser stores
	if strings.Contains(cookieFilePath, "firefox") || strings.Contains(cookieFilePath, "cookies.sqlite") {
		store, err = firefox.CookieStore(cookieFilePath)
	} else if strings.Contains(cookieFilePath, "safari") || strings.Contains(cookieFilePath, "Cookies.binarycookies") {
		store, err = safari.CookieStore(cookieFilePath)
	} else if strings.Contains(cookieFilePath, "chrome") || strings.Contains(cookieFilePath, "Cookies") {
		store, err = chrome.CookieStore(cookieFilePath)
	} else {
		return nil, fmt.Errorf("unsupported cookie file format")
	}

	if err != nil {
		return nil, fmt.Errorf("failed to create cookie store: %w", err)
	}

	// Read cookies from the store
	cookies, err := store.ReadCookies()
	if err != nil {
		return nil, fmt.Errorf("failed to read cookies: %w", err)
	}

	return cookies, nil
}

// newCustomCookieSource validates and retrieves a custom cookie source profile
func newCustomCookieSource() (*models.CustomCookieSource, error) {

	if !config.IsSet(keys.CookiePath) {
		logging.D(2, "No custom cookie directory sent in")
		return nil, nil
	}

	cDir := config.GetString(keys.CookiePath)
	cDir = filepath.Clean(cDir)

	info, err := os.Stat(cDir)
	if err != nil {
		if os.IsNotExist(err) {
			return nil, fmt.Errorf("cookie directory does not exist: %w", err)
		}
		return nil, err
	}

	if !info.IsDir() {
		return nil, fmt.Errorf("cookie directory sent in as file, should be directory")
	}

	dirContents, err := os.ReadDir(cDir)
	if err != nil {
		return nil, err
	}

	var cSource models.CustomCookieSource
	foundFiles := make(map[string][]string, len(cookieFilePatterns))

	for browser, patterns := range cookieFilePatterns {
		for _, pattern := range patterns {
			// For each file in directory
			for _, dirFile := range dirContents {

				fileName := dirFile.Name()
				match, err := filepath.Match(pattern, fileName)
				if err != nil {
					logging.D(2, "Pattern matching error: %v", err)
					continue
				}

				if match {
					foundFiles[browser] = append(foundFiles[browser], fileName)
				}
			}
			if len(foundFiles[browser]) == len(cookieFilePatterns[browser]) {
				logging.S(0, "Got all required cookie and auth files for browser %s", browser)
				cSource.Browser = browser
				cSource.Dir = cDir
				break
			}
		}
	}

	if cSource.Browser == "" {
		for browser, files := range foundFiles {
			if len(files) > 0 {
				logging.D(2, "Found %d files for %s: %v", len(files), browser, files)

				cSource.Browser = browser
				cSource.Dir = cDir
			}
		}
	}

	return &cSource, nil
}
package utils

import (
	enums "metarr/internal/domain/enums"
	"metarr/internal/models"
	logging "metarr/internal/utils/logging"
	"strconv"
	"strings"
	"time"
)

// BitchuteComRules holds rules for scraping bitchute.com
var BitchuteComRules = map[enums.WebClassTags][]*models.SelectorRule{
	enums.WEBCLASS_CREDITS: {

		{Selector: "q-item__label ellipsis text-subtitle1 ellipsis", Process: strings.TrimSpace},
	},
	enums.WEBCLASS_DATE: {
		{
			Selector: "span[data-v-3c3cf957]",
			Attr:     "data-v-3c3cf957",
			Process:  BitchuteComParseDate,
		},
	},
	enums.WEBCLASS_DESCRIPTION: {

		{Selector: `meta[name="description"]`, Attr: "content", Process: strings.TrimSpace},
		{Selector: `meta[property="og:description"]`, Attr: "content", Process: strings.TrimSpace},
		{
			Selector: `meta[itemprop="name"]`,
			Attr:     "content",
			Process:  strings.TrimSpace,
		},
	},
	enums.WEBCLASS_TITLE: {
		{
			Selector: `meta[itemprop="name"]`,
			Attr:     "content",
			Process:  strings.TrimSpace,
		},
	},
}

// BitchuteComParseDate attempts to parse dates like "9 hours ago" (etc.)
func BitchuteComParseDate(date string) string {
	date = strings.TrimSpace(date)

	dateSplit := strings.Split(date, " ")

	var (
		unit  string
		digit int
		err   error
	)

	if len(dateSplit) >= 3 {
		digit, err = strconv.Atoi(dateSplit[0])
		if err != nil {
			logging.E(0, "Failed to convert string to digits: %v", err)
		}
		unit = strings.TrimSuffix(strings.ToLower(dateSplit[1]), "s") // handles both "hour" and "hours"

		var duration time.Duration
		now := time.Now()

		switch unit {
		case "second":
			duration = time.Duration(digit) * time.Second
			return now.Add(-duration).Format(time.RFC3339)
		case "minute":
			duration = time.Duration(digit) * time.Minute
			return now.Add(-duration).Format(time.RFC3339)
		case "hour":
			duration = time.Duration(digit) * time.Hour
			return now.Add(-duration).Format(time.RFC3339)
		case "day":
			duration = time.Duration(digit) * time.Hour * 24
			return now.Add(-duration).Format(time.RFC3339)
		case "week":
			duration = time.Duration(digit) * time.Hour * 24 * 7
			return now.Add(-duration).Format(time.RFC3339)
		case "month":
			return now.AddDate(0, -digit, 0).Format(time.RFC3339)
		case "year":
			return now.AddDate(-digit, 0, 0).Format(time.RFC3339)
		default:
			logging.E(0, "Unknown time unit: %s", unit)
			return ""
		}
	}
	logging.E(0, "Wrong date length passed in")
	return ""
}
package utils

import (
	enums "metarr/internal/domain/enums"
	"metarr/internal/models"
	logging "metarr/internal/utils/logging"
	"strings"

	"golang.org/x/text/cases"
	"golang.org/x/text/language"
)

var CensoredTvRules = map[enums.WebClassTags][]*models.SelectorRule{
	enums.WEBCLASS_DATE: {
		{Selector: ".main-episode-player-container p.text-muted.text-right.text-date.mb-0", Process: strings.TrimSpace},
		{Selector: ".text-date", Process: strings.TrimSpace},
	},
	enums.WEBCLASS_DESCRIPTION: {
		{Selector: ".p-3 check-for-urls", Process: strings.TrimSpace},
		{Selector: `meta[name="description"]`, Attr: "content", Process: strings.TrimSpace},
	},
	enums.WEBCLASS_TITLE: {
		{Selector: ".p-3 h4", Process: strings.TrimSpace},
		{Selector: "[title]", Attr: "title", Process: strings.TrimSpace},
	},
}

// censoredTvChannelName gets the channel name from the URL string
func CensoredTvChannelName(url string) string {
	if url == "" {
		logging.E(0, "url passed in empty")
		return ""
	}
	urlSplit := strings.Split(url, "/")

	var channel string
	for i, seg := range urlSplit {
		if strings.HasSuffix(seg, "shows") && len(urlSplit) > i+1 {
			channel = urlSplit[i+1]
		}
	}

	if channel == "" {
		logging.E(0, "failed to fill channel name from url, out of bounds?")
	}
	channel = strings.ReplaceAll(channel, "-", " ")

	caser := cases.Title(language.English)
	channel = caser.String(channel)

	if strings.ToLower(channel) == "atheism is unstoppable" {
		channel = "Atheism-is-Unstoppable"
	}
	return channel
}
package utils

import (
	enums "metarr/internal/domain/enums"
	"metarr/internal/models"
	"strings"
)

// OdyseeComRules holds rules for scraping odysee.com
var OdyseeComRules = map[enums.WebClassTags][]*models.SelectorRule{
	enums.WEBCLASS_CREDITS: {
		{
			Selector: "script[type='application/ld+json']",
			JsonPath: []string{"author", "name"},
			Process:  strings.TrimSpace,
		},
	},
	enums.WEBCLASS_DATE: {
		{
			Selector: "script[type='application/ld+json']",
			JsonPath: []string{"uploadDate"},
			Process:  strings.TrimSpace,
		},
		{Selector: `meta[property="og:video:release_date"]`, Attr: "content", Process: strings.TrimSpace},
	},
	enums.WEBCLASS_DESCRIPTION: {
		{
			Selector: "script[type='application/ld+json']",
			JsonPath: []string{"description"},
			Process:  strings.TrimSpace,
		},
		{Selector: `meta[name="description"]`, Attr: "content", Process: strings.TrimSpace},
		{Selector: `meta[property="og:description"]`, Attr: "content", Process: strings.TrimSpace},
	},
	enums.WEBCLASS_TITLE: {

		{Selector: "title", Process: strings.TrimSpace},
		{
			Selector: "script[type='application/ld+json']",
			JsonPath: []string{"name"},
			Process:  strings.TrimSpace,
		},
	},
}
package utils

import (
	enums "metarr/internal/domain/enums"
	"metarr/internal/models"
	"strings"
)

// RumbleComRules holds rules for scraping rumble.com
var RumbleComRules = map[enums.WebClassTags][]*models.SelectorRule{
	enums.WEBCLASS_CREDITS: {

		{Selector: ".media-subscribe-and-notify", Attr: "data-title", Process: strings.TrimSpace},
		{Selector: ".media-by--a .media-heading-name", Process: strings.TrimSpace},
	},
	enums.WEBCLASS_DATE: {
		{Selector: "time", Attr: "datetime", Process: strings.TrimSpace},
		{
			Selector: "script[type='application/ld+json']",
			JsonPath: []string{"uploadDate"},
			Process:  strings.TrimSpace,
		},
	},
	enums.WEBCLASS_DESCRIPTION: {
		{
			Selector: "script[type='application/ld+json']",
			JsonPath: []string{"description"},
			Process:  strings.TrimSpace,
		},
		{Selector: `meta[name="description"]`, Attr: "content", Process: strings.TrimSpace},
		{Selector: `meta[property="og:description"]`, Attr: "content", Process: strings.TrimSpace},
	},
	enums.WEBCLASS_TITLE: {

		{Selector: "title", Process: strings.TrimSpace},
		{
			Selector: "script[type='application/ld+json']",
			JsonPath: []string{"name"},
			Process:  strings.TrimSpace,
		},
	},
}
package utils

import (
	"encoding/json"
	"fmt"
	consts "metarr/internal/domain/constants"
	enums "metarr/internal/domain/enums"
	"metarr/internal/models"
	presets "metarr/internal/utils/browser/presets"
	logging "metarr/internal/utils/logging"
	"net/http"
	"regexp"
	"strings"
	"time"

	"github.com/gocolly/colly"
)

// scrapeMeta gets cookies for a given URL and returns a grabbed string
func ScrapeMeta(w *models.MetadataWebData, find enums.WebClassTags) string {

	var (
		err  error
		data string
	)

	w.Cookies, err = getBrowserCookies(w.WebpageURL)
	if err != nil {
		logging.E(2, "Was unable to grab browser cookies: %v", err)
	}
	for _, try := range w.TryURLs {
		data, err = scrape(try, w.Cookies, find, false)
		if err != nil {
			logging.E(0, "Failed to scrape '%s' for requested metadata: %v", try, err)
		} else {
			break
		}
	}
	return data
}

// scrape searches relevant URLs to try and fill missing metadata
func scrape(url string, cookies []*http.Cookie, tag enums.WebClassTags, skipPresets bool) (string, error) {

	var (
		result      string
		scrapeError error
		custom      bool
	)

	// Initialize the collector
	c := colly.NewCollector(
		colly.AllowURLRevisit(),
		colly.MaxDepth(1),
		colly.Async(true),
	)
	c.SetRequestTimeout(15 * time.Second)

	if len(cookies) > 0 {
		c.SetCookies(url, cookies)
	}

	// Define preset scraping rules if the URL matches a known pattern
	switch {
	case strings.Contains(url, "bitchute.com") && !skipPresets:

		custom = true
		logging.I("Using bitchute.com preset scraper")
		setupPresetScraping(c, tag, presets.BitchuteComRules, &result, url)

	case strings.Contains(url, "censored.tv") && !skipPresets:

		custom = true
		logging.I("Using censored.tv preset scraper")
		if tag == enums.WEBCLASS_CREDITS {
			return presets.CensoredTvChannelName(url), nil
		}
		setupPresetScraping(c, tag, presets.CensoredTvRules, &result, url)

	case strings.Contains(url, "rumble.com") && !skipPresets:

		custom = true
		logging.I("Using rumble.com preset scraper")
		setupPresetScraping(c, tag, presets.RumbleComRules, &result, url)

	case strings.Contains(url, "odysee.com") && !skipPresets:

		custom = true
		logging.I("Using odysee.com preset scraper")
		setupPresetScraping(c, tag, presets.OdyseeComRules, &result, url)

	default:
		logging.I("Generic scrape attempt...")
		setupGenericScraping(c, tag, &result, url)
	}

	// Error handler
	c.OnError(func(r *colly.Response, err error) {
		scrapeError = fmt.Errorf("failed to scrape %s: %v", r.Request.URL, err)
	})

	// Attempt visit and wait for async scraping
	if err := c.Visit(url); err != nil {
		return "", fmt.Errorf("unable to visit given web page")
	}
	c.Wait()

	if scrapeError != nil {
		switch result {
		case "":
			return "", scrapeError
		default:
			logging.E(0, "Error during scrape (%v) but got result anyway. Returning result '%s'...", scrapeError, result)
			return result, nil
		}
	}

	// If custom preset was used and failed, try again with default
	if result == "" && custom {
		return scrape(url, cookies, tag, true)
	}

	return result, nil
}

// setupPresetScraping applies specific scraping rules for known sites
func setupPresetScraping(c *colly.Collector, tag enums.WebClassTags, rules map[enums.WebClassTags][]*models.SelectorRule, result *string, url string) {
	if result == nil {
		return
	}
	if ruleSet, exists := rules[tag]; exists {
		for _, rule := range ruleSet {
			c.OnHTML(rule.Selector, func(h *colly.HTMLElement) {
				if *result != "" {
					return
				}
				var value string
				if len(rule.JsonPath) > 0 {
					if jsonVal, err := jsonExtractor([]byte(h.Text), rule.JsonPath); err == nil {
						value = jsonVal
					}
				} else if rule.Attr != "" {
					value = h.Attr(rule.Attr)
				} else {
					value = h.Text
				}

				if value != "" {
					logging.S(0, "Grabbed value '%s' for URL '%s' using preset scraper", value, url)
					*result = rule.Process(value)
				}
			})
		}
	}
}

// setupGenericScraping defines a generic scraping approach for non-preset sites
func setupGenericScraping(c *colly.Collector, tag enums.WebClassTags, result *string, url string) {
	if result == nil {
		return
	}

	var tags []string

	// Determine the appropriate tags based on the metadata being fetched
	switch tag {
	case enums.WEBCLASS_DATE:
		tags = consts.WebDateTags[:]
	case enums.WEBCLASS_DESCRIPTION:
		tags = consts.WebDescriptionTags[:]
	case enums.WEBCLASS_CREDITS:
		tags = consts.WebCreditsTags[:]
	case enums.WEBCLASS_TITLE:
		tags = consts.WebTitleTags[:]
	default:
		return
	}

	// Set up the HTML scraper for each tag
	c.OnHTML("*", func(e *colly.HTMLElement) {
		if *result != "" {
			return
		}

		classAttr := strings.ToLower(e.Attr("class"))
		idAttr := strings.ToLower(e.Attr("id"))
		text := strings.TrimSpace(e.Text)

		if classAttr != "" {
			logging.D(2, "Checking element with class: '%s'", classAttr)
		}

		for _, t := range tags {
			if (e.Name == "p" && strings.Contains(idAttr, t)) ||
				strings.Contains(classAttr, t) ||
				strings.Contains(idAttr, t) {

				if tag == enums.WEBCLASS_DATE && !looksLikeDate(text) {
					continue
				}

				*result = text
				logging.I("Found '%s' in element with class '%s' and id '%s' for URL '%s'",
					result, classAttr, idAttr, url)
				return
			}
		}
	})
}

// jsonExtractor helps extract values from nested JSON structures
func jsonExtractor(data []byte, path []string) (string, error) {
	var result map[string]interface{}
	if err := json.Unmarshal(data, &result); err != nil {
		return "", err
	}
	current := result
	for _, key := range path[:len(path)-1] {
		if next, ok := current[key].(map[string]interface{}); ok {
			current = next
		} else {
			return "", fmt.Errorf("invalid JSON path at %s", key)
		}
	}
	if val, ok := current[path[len(path)-1]].(string); ok {
		return val, nil
	}
	return "", fmt.Errorf("value at path is not a string")
}

// looksLikeDate validates if the text appears to be a date
func looksLikeDate(text string) bool {
	text = strings.TrimSpace(strings.ToLower(text))

	// Common date patterns
	datePatterns := []string{
		`\d{4}-\d{2}-\d{2}`,       // YYYY-MM-DD
		`\d{1,2}/\d{1,2}/\d{2,4}`, // M/D/YY or MM/DD/YYYY
		`(?i)(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\s+\d{1,2},?\s+\d{4}`, // Month DD, YYYY
	}

	for _, pattern := range datePatterns {
		matched, err := regexp.MatchString(pattern, text)
		if err == nil && matched {
			return true
		}
	}

	// Additional date indicators
	dateIndicators := []string{"uploaded", "published", "created", "date:", "on"}
	for _, indicator := range dateIndicators {
		if strings.Contains(text, indicator) {
			return true
		}
	}

	return false
}
package utils

import (
	"fmt"
	"io"
	consts "metarr/internal/domain/constants"
	logging "metarr/internal/utils/logging"
	"os"
	"path/filepath"
	"strings"
	"sync"
)

var (
	muBackup sync.Mutex
)

// createBackup creates a backup copy of the original file before modifying it.
func BackupFile(file *os.File) error {

	originalFilePath := file.Name()

	backupFilePath := generateBackupFilename(originalFilePath)
	logging.D(3, "Creating backup of file '%s' as '%s'", originalFilePath, backupFilePath)

	// Current position
	currentPos, err := file.Seek(0, io.SeekCurrent)
	if err != nil {
		return fmt.Errorf("failed to get current file position: %w", err)
	}
	defer func() {
		file.Seek(currentPos, io.SeekStart)
	}()

	muBackup.Lock()
	defer muBackup.Unlock()

	// Seek to start for backup
	if _, err := file.Seek(0, io.SeekStart); err != nil {
		return fmt.Errorf("failed to seek to beginning of original file: %w", err)
	}

	// Open the backup file for writing
	backupFile, err := os.Create(backupFilePath)
	if err != nil {
		return fmt.Errorf("failed to create backup file: %w", err)
	}
	defer backupFile.Close()

	// Copy the content of the original file to the backup file
	buf := make([]byte, 4*1024*1024)
	_, err = io.CopyBuffer(backupFile, file, buf)
	if err != nil {
		return fmt.Errorf("failed to copy content to backup file: %w", err)
	}

	logging.D(3, "Backup successfully created at '%s'", backupFilePath)
	return nil
}

// generateBackupFilename creates a backup filename by appending "_backup" to the original filename
func generateBackupFilename(originalFilePath string) string {
	ext := filepath.Ext(originalFilePath)
	base := strings.TrimSuffix(originalFilePath, ext)
	return fmt.Sprintf(base + consts.OldTag + ext)
}

// RenameToBackup renames the passed in file
func RenameToBackup(filename string) (backupName string, err error) {

	if filename == "" {
		logging.E(0, "filename was passed in to backup empty")
	}

	backupName = generateBackupFilename(filename)

	if err := os.Rename(filename, backupName); err != nil {
		return "", fmt.Errorf("failed to backup filename '%s' to '%s'", filename, backupName)
	}
	return backupName, nil
}
package utils

import (
	"fmt"
	"metarr/internal/config"
	consts "metarr/internal/domain/constants"
	enums "metarr/internal/domain/enums"
	keys "metarr/internal/domain/keys"
	"metarr/internal/domain/regex"
	"metarr/internal/models"
	logging "metarr/internal/utils/logging"
	"os"
	"path/filepath"
	"strings"
)

// Variable cache
var (
	videoExtensions,
	metaExtensions,
	inputPrefixes []string
)

// InitFetchFilesVars sets up the cached variables to be used in file fetching ops
func InitFetchFilesVars() error {

	if inVExts, ok := config.Get(keys.InputVExtsEnum).([]enums.ConvertFromFiletype); ok {
		logging.D(2, "Received video extensions enum: %v", inVExts)
		videoExtensions = setVideoExtensions(inVExts)
	} else {
		return fmt.Errorf("wrong type sent in. Received type %T", inVExts)
	}

	if inMExts, ok := config.Get(keys.InputMExtsEnum).([]enums.MetaFiletypeFilter); ok {
		logging.D(2, "Received video extensions enum: %v", inMExts)
		metaExtensions = setMetaExtensions(inMExts)
	} else {
		return fmt.Errorf("wrong type sent in. Received type %T", inMExts)
	}

	inputPrefixes = SetPrefixFilter(config.GetStringSlice(keys.FilePrefixes))
	logging.D(2, "Setting prefix filter: %v", inputPrefixes)

	return nil
}

// GetVideoFiles fetches video files from a directory
func GetVideoFiles(videoDir *os.File) (map[string]*models.FileData, error) {
	files, err := videoDir.ReadDir(-1)
	if err != nil {
		return nil, fmt.Errorf("error reading video directory: %w", err)
	}

	logging.P("\n\nFiltering directory '%s':\n\nFile extensions: %v\nFile prefixes: %v\n\n", videoDir.Name(), videoExtensions, inputPrefixes)

	videoFiles := make(map[string]*models.FileData, len(files))

	for _, file := range files {
		if !file.IsDir() && HasFileExtension(file.Name(), videoExtensions) {

			if config.IsSet(keys.FilePrefixes) {
				if !HasPrefix(file.Name(), inputPrefixes) {
					continue
				}
			}
			filenameBase := filepath.Base(file.Name())

			m := models.NewFileData()
			m.OriginalVideoPath = filepath.Join(videoDir.Name(), file.Name())
			m.OriginalVideoBaseName = strings.TrimSuffix(filenameBase, filepath.Ext(file.Name()))
			m.VideoDirectory = videoDir.Name()

			if !strings.HasSuffix(m.OriginalVideoBaseName, consts.OldTag) {
				videoFiles[file.Name()] = m
				logging.I("Added video to queue: %v", filenameBase)
			} else {
				logging.I("Skipping file '%s' containing backup tag ('%s')", m.OriginalVideoBaseName, consts.OldTag)
			}
		}
	}

	if len(videoFiles) == 0 {
		return nil, fmt.Errorf("no video files with extensions: %v and prefixes: %v found in directory: %s", videoExtensions, inputPrefixes, videoDir.Name())
	}
	return videoFiles, nil
}

// GetMetadataFiles fetches metadata files from a directory
func GetMetadataFiles(metaDir *os.File) (map[string]*models.FileData, error) {
	files, err := metaDir.ReadDir(-1)
	if err != nil {
		return nil, fmt.Errorf("error reading metadata directory: %w", err)
	}

	metaFiles := make(map[string]*models.FileData, len(files))

	for _, file := range files {
		if !file.IsDir() {
			ext := filepath.Ext(file.Name())

			logging.D(3, "Checking file '%s' with extension '%s'", file.Name(), ext)

			if config.IsSet(keys.FilePrefixes) {
				if !HasPrefix(file.Name(), inputPrefixes) {
					continue
				}
			}

			var match bool
			for _, mExt := range metaExtensions {
				if ext != mExt {
					logging.D(3, "Extension '%s' does not match '%s'", ext, mExt)
					continue
				}
				logging.S(3, "Extension '%s' matches input meta extensions '%s'", ext, mExt)
				match = true
				break
			}
			if !match {
				continue
			}

			filenameBase := filepath.Base(file.Name())
			baseName := strings.TrimSuffix(filenameBase, ext)

			m := models.NewFileData()
			filePath := filepath.Join(metaDir.Name(), file.Name())

			switch ext {
			case consts.MExtJSON:

				logging.D(1, "Detected JSON file '%s'", file.Name())
				m.JSONFilePath = filePath
				m.JSONBaseName = baseName
				m.JSONDirectory = metaDir.Name()
				m.MetaFileType = enums.METAFILE_JSON

			case consts.MExtNFO:

				logging.D(1, "Detected NFO file '%s'", file.Name())
				m.NFOFilePath = filePath
				m.NFOBaseName = baseName
				m.NFODirectory = metaDir.Name()
				m.MetaFileType = enums.METAFILE_NFO
			}

			if !strings.Contains(baseName, consts.OldTag) {
				metaFiles[file.Name()] = m
			} else {
				logging.I("Skipping file '%s' containing backup tag ('%s')", baseName, consts.OldTag)
			}
		}
	}

	if len(metaFiles) == 0 {
		return nil, fmt.Errorf("no meta files with extensions: %v and prefixes: %v found in directory: %s", metaExtensions, inputPrefixes, metaDir.Name())
	}

	logging.D(3, "Returning meta files %v", metaFiles)
	return metaFiles, nil
}

// GetSingleVideoFile handles a single video file
func GetSingleVideoFile(videoFile *os.File) (map[string]*models.FileData, error) {
	videoMap := make(map[string]*models.FileData, 1)
	filename := filepath.Base(videoFile.Name())

	videoData := models.NewFileData()
	videoData.OriginalVideoPath = videoFile.Name()
	videoData.OriginalVideoBaseName = strings.TrimSuffix(filename, filepath.Ext(filename))
	videoData.VideoDirectory = filepath.Dir(videoFile.Name())
	videoData.VideoFile = videoFile

	logging.D(3, "Created video file data for single file: %s", filename)

	videoMap[filename] = videoData
	return videoMap, nil
}

// GetSingleMetadataFile handles a single metadata file
func GetSingleMetadataFile(metaFile *os.File) (map[string]*models.FileData, error) {
	metaMap := make(map[string]*models.FileData, 1)
	filename := filepath.Base(metaFile.Name())

	fileData := models.NewFileData()
	ext := filepath.Ext(metaFile.Name())

	switch ext {
	case consts.MExtJSON:

		fileData.MetaFileType = enums.METAFILE_JSON
		fileData.JSONFilePath = metaFile.Name()
		fileData.JSONBaseName = strings.TrimSuffix(filename, ext)
		fileData.JSONDirectory = filepath.Dir(metaFile.Name())
		logging.D(3, "Created JSON metadata file data for single file: %s", filename)

	case consts.MExtNFO:

		fileData.MetaFileType = enums.METAFILE_NFO
		fileData.NFOFilePath = metaFile.Name()
		fileData.NFOBaseName = strings.TrimSuffix(filename, ext)
		fileData.NFODirectory = filepath.Dir(metaFile.Name())
		logging.D(3, "Created NFO metadata file data for single file: %s", filename)

	default:
		return nil, fmt.Errorf("unsupported metadata file type: %s", ext)
	}

	metaMap[filename] = fileData
	return metaMap, nil
}

// MatchVideoWithMetadata matches video files with their corresponding metadata files
func MatchVideoWithMetadata(videoFiles, metaFiles map[string]*models.FileData) (map[string]*models.FileData, error) {
	logging.D(3, "Entering metadata and video file matching loop...")

	matchedFiles := make(map[string]*models.FileData, len(videoFiles))

	specialChars := regex.SpecialCharsCompile()
	extraSpaces := regex.ExtraSpacesCompile()

	// Pre-process metaFiles into a lookup map
	metaLookup := make(map[string]*models.FileData, len(metaFiles))
	for metaName, metaData := range metaFiles {
		baseKey := NormalizeFilename(TrimMetafileSuffixes(metaName, ""), specialChars, extraSpaces)
		metaLookup[baseKey] = metaData
	}

	for videoName := range videoFiles {
		videoBase := strings.TrimSuffix(videoName, filepath.Ext(videoName))
		normalizedVideoBase := NormalizeFilename(videoBase, specialChars, extraSpaces)

		if metaData, exists := metaLookup[normalizedVideoBase]; exists { // This checks if the key exists in the metaLookup map
			matchedFiles[videoName] = videoFiles[videoName]
			matchedFiles[videoName].MetaFileType = metaData.MetaFileType

			switch metaData.MetaFileType {
			case enums.METAFILE_JSON:
				matchedFiles[videoName].JSONFilePath = metaData.JSONFilePath
				matchedFiles[videoName].JSONBaseName = metaData.JSONBaseName
				matchedFiles[videoName].JSONDirectory = metaData.JSONDirectory

			case enums.METAFILE_NFO:
				matchedFiles[videoName].NFOFilePath = metaData.NFOFilePath
				matchedFiles[videoName].NFOBaseName = metaData.NFOBaseName
				matchedFiles[videoName].NFODirectory = metaData.NFODirectory
			}
		}
	}

	if len(matchedFiles) == 0 {
		return nil, fmt.Errorf("no matching metadata files found for any videos")
	}

	return matchedFiles, nil
}
package utils

import (
	consts "metarr/internal/domain/constants"
	enums "metarr/internal/domain/enums"
	logging "metarr/internal/utils/logging"
	"os"
	"path/filepath"
	"regexp"
	"strings"
)

// hasVideoExtension checks if the file has a valid video extension
func HasFileExtension(fileName string, extensions []string) bool {

	if extensions == nil {
		logging.E(0, "No extensions picked.")
		return false
	}

	for _, ext := range extensions {
		if strings.HasSuffix(strings.ToLower(fileName), strings.ToLower(ext)) {
			return true
		}
	}
	return false
}

// hasPrefix determines if the input file has the desired prefix
func HasPrefix(fileName string, prefixes []string) bool {

	if prefixes == nil {
		prefixes = append(prefixes, "")
	}

	for _, data := range prefixes {
		if strings.HasPrefix(strings.ToLower(fileName), strings.ToLower(data)) {
			return true
		}
	}
	return false
}

// setVideoExtensions creates a list of extensions to filter
func setVideoExtensions(exts []enums.ConvertFromFiletype) []string {

	videoExtensions := make([]string, 0, len(consts.AllVidExtensions))

	for _, arg := range exts {
		switch arg {
		case enums.VID_EXTS_MKV:
			videoExtensions = append(videoExtensions, ".mkv")
		case enums.VID_EXTS_MP4:
			videoExtensions = append(videoExtensions, ".mp4")
		case enums.VID_EXTS_WEBM:
			videoExtensions = append(videoExtensions, ".webm")
		case enums.VID_EXTS_ALL:
			return consts.AllVidExtensions[:]
		}
	}

	if len(videoExtensions) == 0 {
		return consts.AllVidExtensions[:]
	}

	return videoExtensions
}

// setMetaExtensions creates a lists of meta extensions to filter
func setMetaExtensions(exts []enums.MetaFiletypeFilter) []string {

	metaExtensions := make([]string, 0, len(consts.AllMetaExtensions))

	for _, arg := range exts {
		switch arg {
		case enums.META_EXTS_JSON:
			metaExtensions = append(metaExtensions, ".json")
		case enums.META_EXTS_NFO:
			metaExtensions = append(metaExtensions, ".nfo")
		case enums.META_EXTS_ALL:
			return consts.AllMetaExtensions[:]
		}
	}

	if len(metaExtensions) == 0 {
		return consts.AllMetaExtensions[:]
	}

	return metaExtensions
}

// setPrefixFilter sets a list of prefixes to filter
func SetPrefixFilter(inputPrefixFilters []string) []string {

	prefixFilters := make([]string, 0, len(inputPrefixFilters))
	prefixFilters = append(prefixFilters, inputPrefixFilters...)

	return prefixFilters
}

// GetDirStats returns the number of video or metadata files in a directory, so maps/slices can be suitable sized
func GetDirStats(dir string) (vidCount, metaCount int) {

	// Quick initial scan just counting files, not storing anything
	entries, err := os.ReadDir(dir)
	if err != nil {
		return 0, 0
	}
	for _, entry := range entries {
		if !entry.IsDir() {
			ext := strings.ToLower(filepath.Ext(entry.Name()))

			for _, entry := range consts.AllVidExtensions {
				if ext == entry {
					vidCount++
					continue
				}
				switch ext {
				case consts.MExtJSON, consts.MExtNFO:
					metaCount++
					continue
				}
			}
		}
	}
	return vidCount, metaCount
}

// normalizeFilename removes special characters and normalizes spacing
func NormalizeFilename(filename string, specialChars, extraSpaces *regexp.Regexp) string {

	normalized := strings.ToLower(filename)
	normalized = specialChars.ReplaceAllString(normalized, "")
	normalized = extraSpaces.ReplaceAllString(normalized, " ")
	normalized = strings.TrimSpace(normalized)

	return normalized
}

// trimJsonSuffixes normalizes away common json string suffixes
// e.g. ".info" for yt-dlp outputted JSON files
func TrimMetafileSuffixes(metaBase, videoBase string) string {

	switch {

	case strings.HasSuffix(metaBase, ".info.json"): // FFmpeg
		if !strings.HasSuffix(videoBase, ".info") {
			metaBase = strings.TrimSuffix(metaBase, ".info.json")
		} else {
			metaBase = strings.TrimSuffix(metaBase, consts.MExtJSON)
		}

	case strings.HasSuffix(metaBase, ".metadata.json"): // Angular
		if !strings.HasSuffix(videoBase, ".metadata") {
			metaBase = strings.TrimSuffix(metaBase, ".metadata.json")
		} else {
			metaBase = strings.TrimSuffix(metaBase, consts.MExtJSON)
		}

	case strings.HasSuffix(metaBase, ".model.json"):
		if !strings.HasSuffix(videoBase, ".model") {
			metaBase = strings.TrimSuffix(metaBase, ".model.json")
		} else {
			metaBase = strings.TrimSuffix(metaBase, consts.MExtJSON)
		}

	case strings.HasSuffix(metaBase, ".manifest.cdfd.json"):
		if !strings.HasSuffix(videoBase, ".manifest.cdm") {
			metaBase = strings.TrimSuffix(metaBase, ".manifest.cdfd.json")
		} else {
			metaBase = strings.TrimSuffix(metaBase, consts.MExtJSON)
		}

	default:
		switch {
		case !strings.HasSuffix(videoBase, consts.MExtJSON): // Edge cases where metafile extension is in the suffix of the video file
			metaBase = strings.TrimSuffix(metaBase, consts.MExtJSON)

		case !strings.HasSuffix(videoBase, consts.MExtNFO):
			metaBase = strings.TrimSuffix(metaBase, consts.MExtNFO)

		default:
			logging.D(1, "Common suffix not found for metafile (%s)", metaBase)
		}
	}
	return metaBase
}
package utils

import (
	"bufio"
	"bytes"
	"fmt"
	"io"
	"metarr/internal/config"
	consts "metarr/internal/domain/constants"
	enums "metarr/internal/domain/enums"
	keys "metarr/internal/domain/keys"
	logging "metarr/internal/utils/logging"
	"os"
	"path/filepath"
	"strings"
	"sync"
)

type FSFileWriter struct {
	SkipVids  bool
	DestVideo string
	SrcVideo  string
	DestMeta  string
	SrcMeta   string
	muFs      sync.RWMutex
}

func NewFSFileWriter(skipVids bool, destVideo, srcVideo, destMeta, srcMeta string) *FSFileWriter {
	same := 0
	if destVideo != srcVideo {
		same++
	}
	if destMeta != srcMeta {
		same++
	}

	logging.D(2, "Made FSFileWriter with parameters:\n\nSkip videos? %v\n\nOriginal Video: %s\nRenamed Video:  %s\n\nOriginal Metafile: %s\nRenamed Metafile:  %s\n\n%d file names will be changed...\n\n",
		skipVids, srcVideo, destVideo, srcMeta, destMeta, same)
	return &FSFileWriter{
		SkipVids:  skipVids,
		DestVideo: destVideo,
		SrcVideo:  srcVideo,
		DestMeta:  destMeta,
		SrcMeta:   srcMeta,
	}
}

// WriteResults executes the final commands to write the transformed files
func (fs *FSFileWriter) WriteResults() error {
	fs.muFs.Lock()
	defer fs.muFs.Unlock()

	logging.D(1, "Rename function final commands:\n\nVideo: Replacing '%v' with '%v'\nMetafile: Replacing '%v' with '%v'", fs.SrcVideo, fs.DestVideo,
		fs.SrcMeta, fs.DestMeta)

	if !fs.SkipVids {
		if fs.SrcVideo != fs.DestVideo && fs.SrcVideo != "" && fs.DestVideo != "" {
			if err := os.Rename(fs.SrcVideo, fs.DestVideo); err != nil {
				return fmt.Errorf("failed to rename %s to %s. error: %v", fs.SrcVideo, fs.DestVideo, err)
			}
		} else {
			logging.D(2, "Video source and destination files are equal, not moving/renaming...")
		}
	}

	if fs.SrcMeta != fs.DestMeta && fs.SrcMeta != "" && fs.DestMeta != "" {
		if err := os.Rename(fs.SrcMeta, fs.DestMeta); err != nil {
			return fmt.Errorf("failed to rename %s to %s. error: %v", fs.SrcMeta, fs.DestMeta, err)
		}
	} else {
		logging.D(2, "Metadata source and destination files are equal, not moving/renaming...")
	}

	return nil
}

// MoveFile moves files to specified location
func (fs *FSFileWriter) MoveFile(noMeta bool) error {
	fs.muFs.Lock()
	defer fs.muFs.Unlock()

	if !config.IsSet(keys.MoveOnComplete) {
		return nil
	}

	if fs.DestVideo == "" && fs.DestMeta == "" {
		return fmt.Errorf("video and metafile source strings both empty")
	}

	dst := config.GetString(keys.MoveOnComplete)
	dst = filepath.Clean(dst)

	// Check destination directory
	check, err := os.Stat(dst)
	if err != nil {
		return fmt.Errorf("unable to stat destination folder '%s': %w", dst, err)
	}
	if !check.IsDir() {
		return fmt.Errorf("destination path must be a folder: '%s'", dst)
	}

	// Move/copy video and metadata file
	if fs.DestVideo != "" {
		destVBase := filepath.Base(fs.DestVideo)
		destVTarget := filepath.Join(dst, destVBase)
		if err := fs.moveOrCopyFile(fs.DestVideo, destVTarget); err != nil {
			return fmt.Errorf("failed to move video file: %w", err)
		}
	}

	if !noMeta {
		if fs.DestMeta != "" {
			destMBase := filepath.Base(fs.DestMeta)
			destMTarget := filepath.Join(dst, destMBase)
			if err := fs.moveOrCopyFile(fs.DestMeta, destMTarget); err != nil {
				return fmt.Errorf("failed to move metadata file: %w", err)
			}
		}
	}
	return nil
}

// copyFile copies a file to a target destination
func (fs *FSFileWriter) copyFile(src, dst string) error {
	src = filepath.Clean(src)
	dst = filepath.Clean(dst)

	if src == dst {
		return fmt.Errorf("entered source file '%s' and destination '%s' file as the same name and same path", src, dst)
	}

	logging.I("Copying:\n'%s'\nto\n'%s'...", src, dst)

	// Validate source file
	sourceInfo, err := os.Stat(src)
	if err != nil {
		return fmt.Errorf("failed to stat source file: %w", err)
	}
	if !sourceInfo.Mode().IsRegular() {
		return fmt.Errorf("source is not a regular file: %s", src)
	}
	if sourceInfo.Size() == 0 {
		return fmt.Errorf("source file is empty: %s", src)
	}

	// Check destination
	if destInfo, err := os.Stat(dst); err == nil {
		if os.SameFile(sourceInfo, destInfo) {
			return nil // Same file
		}
		return fmt.Errorf("aborting move, destination file '%s' is equal to source file '%s'", dst, src)
	} else if !os.IsNotExist(err) {
		return fmt.Errorf("error checking destination file: %w", err)
	}

	// Ensure destination directory exists
	if err := os.MkdirAll(filepath.Dir(dst), 0755); err != nil {
		return fmt.Errorf("failed to create destination directory: %w", err)
	}

	// Open source file
	sourceFile, err := os.Open(src)
	if err != nil {
		return fmt.Errorf("failed to open source file: %w", err)
	}
	defer sourceFile.Close()

	// Create destination file
	destFile, err := os.Create(dst)
	if err != nil {
		return fmt.Errorf("failed to create destination file, do you have adequate permissions on the destination folder?: %w", err)
	}
	defer func() {
		destFile.Close()
		if err != nil {
			os.Remove(dst) // Clean up on error
		}
	}()

	// Copy contents with buffer
	bufferedSource := bufio.NewReaderSize(sourceFile, 4*1024*1024) // 4MB: 1024 * 1024 is 1 MB
	bufferedDest := bufio.NewWriterSize(destFile, 4*1024*1024)
	defer bufferedDest.Flush()

	buf := make([]byte, 4*1024*1024)

	if _, err = io.CopyBuffer(bufferedDest, bufferedSource, buf); err != nil {
		return fmt.Errorf("failed to copy file contents: %w", err)
	}

	// Sync to ensure write is complete
	if err = destFile.Sync(); err != nil {
		return fmt.Errorf("failed to sync destination file: %w", err)
	}

	// Set same permissions as source
	if err = os.Chmod(dst, sourceInfo.Mode()); err != nil {
		logging.I("Failed to set file permissions, is destination folder remote? (%v)", err)
	}

	// Verify destination file
	check, err := destFile.Stat()
	if err != nil {
		return fmt.Errorf("error statting destination file: %w", err)
	}
	if check.Size() != sourceInfo.Size() {
		return fmt.Errorf("destination file size (%d) does not match source size (%d)",
			check.Size(), sourceInfo.Size())
	}
	return nil
}

// moveOrCopyFile attempts rename first, falls back to copy+delete for cross-device moves
func (fs *FSFileWriter) moveOrCopyFile(src, dst string) error {
	src = filepath.Clean(src)
	dst = filepath.Clean(dst)

	if src == dst {
		return nil // Same file, nothing to do
	}

	srcHash, err := fs.calculateFileHash(src)
	if err != nil {
		return fmt.Errorf("failed to calculate initial source hash: %w", err)
	}

	// Try rename (pure move) first
	err = os.Rename(src, dst)
	if err == nil {
		dstHash, verifyErr := fs.calculateFileHash(dst)
		if verifyErr != nil {
			return fmt.Errorf("move verification failed: %w", verifyErr)
		}
		if !bytes.Equal(srcHash, dstHash) {
			return fmt.Errorf("hash mismatch after move")
		}
		return nil
	}

	logging.S(0, "Moved file from '%s' to '%s'", src, dst)

	// If cross-device error, fall back to copy+delete
	if strings.Contains(err.Error(), "invalid cross-device link") {
		logging.D(1, "Falling back to copy for moving '%s' to '%s'", src, dst)

		// Copy the file
		if err := fs.copyFile(src, dst); err != nil {
			os.Remove(dst)
			return fmt.Errorf("failed to copy file: %w", err)
		}

		// Verify copy with hash comparison
		dstHash, verifyErr := fs.calculateFileHash(dst)
		if verifyErr != nil {
			os.Remove(dst)
			return fmt.Errorf("copy verification failed: %w", verifyErr)
		}
		if !bytes.Equal(srcHash, dstHash) {
			os.Remove(dst)
			return fmt.Errorf("hash mismatch after copy")
		}

		// Remove source after successful copy and verification
		if err := os.Remove(src); err != nil {
			logging.E(0, "Failed to remove source file after verified copy: %v", err)
			// Operation successful, do not return error, just log the error
		}
		return nil
	}
	return fmt.Errorf("failed to move file: %w", err)
}

// DeleteJSON safely removes JSON metadata files once file operations are complete
func (fs *FSFileWriter) DeleteMetafile(file string) (error, bool) {

	if !config.IsSet(keys.MetaPurgeEnum) {
		return fmt.Errorf("meta purge enum not set"), false
	}

	e, ok := config.Get(keys.MetaPurgeEnum).(enums.PurgeMetafiles)
	if !ok {
		return fmt.Errorf("wrong type for purge metafile enum. Got %T", e), false
	}

	ext := filepath.Ext(file)
	ext = strings.ToLower(ext)

	switch e {
	case enums.PURGEMETA_ALL:
		// Continue
	case enums.PURGEMETA_JSON:
		if ext != consts.MExtJSON {
			logging.D(3, "Skipping deletion of metafile '%s' as extension does not match user selection")
			return nil, false
		}

	case enums.PURGEMETA_NFO:
		if ext != consts.MExtNFO {
			logging.D(3, "Skipping deletion of metafile '%s' as extension does not match user selection")
			return nil, false
		}

	case enums.PURGEMETA_NONE:
		return fmt.Errorf("user selected to skip purging metadata, this should be inaccessible. Exiting function"), false
	default:
		return fmt.Errorf("support not added for this metafile purge enum yet, exiting function"), false
	}

	fileInfo, err := os.Stat(file)
	if err != nil {
		return err, false
	}

	if fileInfo.IsDir() {
		return fmt.Errorf("metafile '%s' is a directory, not a file", file), false
	}

	if !fileInfo.Mode().IsRegular() {
		return fmt.Errorf("metafile '%s' is not a regular file", file), false
	}

	if err := os.Remove(file); err != nil {
		return fmt.Errorf("unable to delete meta file: %w", err), false
	}

	logging.S(0, "Successfully deleted metafile. Bye bye '%s'!", file)

	return nil, true
}
package utils

import (
	"bufio"
	"crypto/sha256"
	"fmt"
	"io"
	"os"
)

// calculateFileHash computes SHA-256 hash of a file
func (fs *FSFileWriter) calculateFileHash(filepath string) ([]byte, error) {
	file, err := os.Open(filepath)
	if err != nil {
		return nil, fmt.Errorf("failed to open file for hashing: %w", err)
	}
	defer file.Close()

	hash := sha256.New()
	buf := make([]byte, 4*1024*1024) // 4MB buffer
	reader := bufio.NewReaderSize(file, 4*1024*1024)

	for {
		n, err := reader.Read(buf)
		if n > 0 {
			if _, err := hash.Write(buf[:n]); err != nil {
				return nil, fmt.Errorf("error writing to hash: %w", err)
			}
		}
		if err == io.EOF {
			break
		}
		if err != nil {
			return nil, fmt.Errorf("error reading file for hash: %w", err)
		}
	}

	return hash.Sum(nil), nil
}
package utils

import (
	"fmt"
	consts "metarr/internal/domain/constants"
	keys "metarr/internal/domain/keys"
	"path/filepath"
	"runtime"
	"sync"

	"github.com/spf13/viper"
)

var (
	Level int = -1 // Pre initialization
	mu    sync.Mutex
)

func E(l int, format string, args ...interface{}) string {

	mu.Lock()
	defer mu.Unlock()
	var msg string

	pc, file, line, _ := runtime.Caller(1)
	file = filepath.Base(file)
	funcName := filepath.Base(runtime.FuncForPC(pc).Name())
	tag := fmt.Sprintf("["+consts.ColorBlue+"Function:"+consts.ColorReset+" %s - "+consts.ColorBlue+"File:"+consts.ColorReset+" %s : "+consts.ColorBlue+"Line:"+consts.ColorReset+" %d] ", funcName, file, line)

	if Level < 0 {
		Level = viper.GetInt(keys.DebugLevel)
	}
	if l <= viper.GetInt(keys.DebugLevel) {

		if len(args) != 0 && args != nil {
			msg = fmt.Sprintf(consts.RedError+format+" "+tag+"\n", args...)
		} else {
			msg = fmt.Sprintf(consts.RedError + format + " " + tag + "\n")
		}
		fmt.Print(msg)
		writeLog(msg, l)
	}

	return msg
}

func S(l int, format string, args ...interface{}) string {

	mu.Lock()
	defer mu.Unlock()
	var msg string

	if Level < 0 {
		Level = viper.GetInt(keys.DebugLevel)
	}
	if l <= viper.GetInt(keys.DebugLevel) {

		if len(args) != 0 && args != nil {
			msg = fmt.Sprintf(consts.GreenSuccess+format+" \n", args...)
		} else {
			msg = fmt.Sprintf(consts.GreenSuccess + format + " \n")
		}
		fmt.Print(msg)
		writeLog(msg, l)
	}
	return msg
}

func D(l int, format string, args ...interface{}) string {

	mu.Lock()
	defer mu.Unlock()
	var msg string

	pc, file, line, _ := runtime.Caller(1)
	file = filepath.Base(file)
	funcName := filepath.Base(runtime.FuncForPC(pc).Name())
	tag := fmt.Sprintf("["+consts.ColorBlue+"Function:"+consts.ColorReset+" %s - "+consts.ColorBlue+"File:"+consts.ColorReset+" %s : "+consts.ColorBlue+"Line:"+consts.ColorReset+" %d] ", funcName, file, line)

	if Level < 0 {
		Level = viper.GetInt(keys.DebugLevel)
	}
	if l <= viper.GetInt(keys.DebugLevel) && l != 0 { // Debug messages don't appear by default

		if len(args) != 0 && args != nil {
			msg = fmt.Sprintf(consts.YellowDebug+format+" "+tag+"\n", args...)
		} else {
			msg = fmt.Sprintf(consts.YellowDebug + format + " " + tag + "\n")
		}
		fmt.Print(msg)
		writeLog(msg, l)
	}
	return msg
}

func I(format string, args ...interface{}) string {

	mu.Lock()
	defer mu.Unlock()
	var msg string

	if len(args) != 0 && args != nil {
		msg = fmt.Sprintf(consts.BlueInfo+format+"\n", args...)
	} else {
		msg = fmt.Sprintf(consts.BlueInfo + format + "\n")
	}
	fmt.Print(msg)
	writeLog(msg, 0)

	return msg
}

func P(format string, args ...interface{}) string {

	mu.Lock()
	defer mu.Unlock()
	var msg string

	if len(args) != 0 && args != nil {
		msg = fmt.Sprintf(format+"\n", args...)
	} else {
		msg = fmt.Sprintf(format + "\n")
	}
	fmt.Print(msg)
	writeLog(msg, 0)

	return msg
}
package utils

import (
	"log"
	"metarr/internal/domain/regex"
	"path/filepath"
	"strings"
	"time"

	"gopkg.in/natefinch/lumberjack.v2"
)

var (
	ErrorArray []error
	Loggable   bool = false
	Logger     *log.Logger

	// Matches ANSI escape codes
	ansiEscape = regex.AnsiEscapeCompile()
)

// SetupLogging creates and/or opens the log file
func SetupLogging(targetDir string) error {

	logFile := &lumberjack.Logger{
		Filename:   filepath.Join(targetDir, "/metarr.log"), // Log file path
		MaxSize:    1,                                       // Max size in MB before rotation
		MaxBackups: 3,                                       // Number of backups to retain
		Compress:   true,                                    // Gzip compression
	}

	// Assign lumberjack logger to standard log output
	Logger = log.New(logFile, "", log.LstdFlags)
	Loggable = true

	Logger.Printf(":\n=========== %v ===========\n\n", time.Now().Format(time.RFC1123Z))
	return nil
}

// Write writes error information to the log file
func writeLog(msg string, level int) {
	// Do not add mutex
	if Loggable && level < 2 {
		if !strings.HasPrefix(msg, "\n") {
			msg += "\n"
		}

		if ansiEscape == nil {
			ansiEscape = regex.AnsiEscapeCompile()
		}

		Logger.Print(ansiEscape.ReplaceAllString(msg, ""))
	}
}

// WriteArray writes an array of error information to the log file
func writeLogArray(msgs []string) {
	if Loggable {

		if ansiEscape == nil {
			ansiEscape = regex.AnsiEscapeCompile()
		}
		out := strings.Join(msgs, ", ")

		Logger.Print(ansiEscape.ReplaceAllString(out, ""))
	}
}
package print

import (
	"fmt"
	consts "metarr/internal/domain/constants"
	"metarr/internal/models"
	logging "metarr/internal/utils/logging"
	"reflect"
	"strings"
	"sync"
)

var muPrint sync.Mutex

// CreateModelPrintout prints out the values stored in a struct.
// taskName allows you to enter your own identifier for this task.
func CreateModelPrintout(model any, filename, taskName string, args ...interface{}) {
	muPrint.Lock()
	defer muPrint.Unlock()

	var b strings.Builder
	b.Grow(2048)

	// Helper function to add sections
	addSection := func(title string, content string) {
		b.WriteString(consts.ColorYellow + "\n" + title + ":\n" + consts.ColorReset)
		b.WriteString(content)
	}

	// Header
	b.WriteString("\n\n================= ")
	b.WriteString(consts.ColorCyan + "Printing metadata fields for: " + consts.ColorReset)
	b.WriteString("'" + consts.ColorReset + filename + "'")
	b.WriteString(" =================\n")

	if taskName != "" {
		str := fmt.Sprintf("'"+taskName+"'", args...)
		b.WriteString("\n" + consts.ColorGreen + "Printing model at point of task " + consts.ColorReset + str + "\n")
	}

	// Add fields from the struct
	addSection("File Information", printStructFields(model))

	if m, ok := model.(*models.FileData); ok {

		addSection("Credits", printStructFields(m.MCredits))
		addSection("Titles and descriptions", printStructFields(m.MTitleDesc))
		addSection("Dates and timestamps", printStructFields(m.MDates))
		addSection("Webpage data", printStructFields(m.MWebData))
		addSection("Show data", printStructFields(m.MShowData))
		addSection("Other data", printStructFields(m.MOther))

	} else if n, ok := model.(*models.NFOData); ok {
		// Credits section
		b.WriteString(consts.ColorYellow + "\nCredits:\n" + consts.ColorReset)

		// Handle each slice type separately
		for _, actor := range n.Actors {
			b.WriteString(printStructFields(actor.Name))
		}
		for _, director := range n.Directors {
			b.WriteString(printStructFields(director))
		}
		for _, producer := range n.Producers {
			b.WriteString(printStructFields(producer))
		}
		for _, publisher := range n.Publishers {
			b.WriteString(printStructFields(publisher))
		}
		for _, studio := range n.Studios {
			b.WriteString(printStructFields(studio))
		}
		for _, writer := range n.Writers {
			b.WriteString(printStructFields(writer))
		}

		addSection("Titles and descriptions", printStructFields(n.Title)+
			printStructFields(n.Description)+
			printStructFields(n.Plot))

		addSection("Webpage data", printStructFields(n.WebpageInfo))

		addSection("Show data", printStructFields(n.ShowInfo.Show)+
			printStructFields(n.ShowInfo.EpisodeID)+
			printStructFields(n.ShowInfo.EpisodeTitle)+
			printStructFields(n.ShowInfo.SeasonNumber))
	}

	// Footer
	b.WriteString("\n\n================= ")
	b.WriteString(consts.ColorYellow + "End metadata fields for: " + consts.ColorReset)
	b.WriteString("'" + filename + "'")
	b.WriteString(" =================\n\n")

	logging.P(b.String())
}

// Function to print the fields of a struct using reflection
func printStructFields(s interface{}) string {
	val := reflect.ValueOf(s)

	// Dereference pointer
	if val.Kind() == reflect.Ptr {
		val = val.Elem()
	}

	if val.Kind() != reflect.Struct {
		return fmt.Sprintf("Expected a struct, got %s\n", val.Kind())
	}

	typ := val.Type()

	var b strings.Builder
	b.Grow(val.NumField() * 1024)

	for i := 0; i < val.NumField(); i++ {
		field := typ.Field(i)      // Get field metadata
		fieldValue := val.Field(i) // Get field value

		// Skip zero or empty fields
		if fieldValue.IsZero() {
			b.WriteString(field.Name + consts.ColorRed + " [empty]\n" + consts.ColorReset)
			continue
		}

		fieldName := field.Name
		fieldValueStr := fmt.Sprintf("%v", fieldValue.Interface()) // Convert the value to a string

		// Append the field name and value in key-value format
		b.WriteString(fmt.Sprintf("%s: %s\n", fieldName, fieldValueStr))
	}

	return b.String()
}

// Print out the fetched fields
func PrintGrabbedFields(fieldType string, p *map[string]string) {

	printMap := *p

	muPrint.Lock()
	defer muPrint.Unlock()

	fmt.Println()
	logging.I("Found and stored %s metadata fields from metafile:", fieldType)
	fmt.Println()

	for printKey, printVal := range printMap {
		if printKey != "" && printVal != "" {
			fmt.Printf(consts.ColorGreen + "Key: " + consts.ColorReset + printKey + consts.ColorYellow + "\nValue: " + consts.ColorReset + printVal + "\n")
		}
	}
	fmt.Println()
}
package utils

import (
	"bufio"
	"context"
	"fmt"
	logging "metarr/internal/utils/logging"
	"os"
	"strings"
)

var (
	userInputChan = make(chan string) // Channel for user input
	decisionMade  bool
)

// InitUserInputReader initializes a user input reading function in a goroutine
func InitUserInputReader() {
	go func() {
		reader := bufio.NewReader(os.Stdin)
		for {
			input, _ := reader.ReadString('\n')
			userInputChan <- strings.TrimSpace(input)
		}
	}()
}

// PromptMetaReplace displays a prompt message and waits for valid user input.
// The option can be used to tell the program to overwrite all in the queue,
// preserve all in the queue, or move through value by value
func PromptMetaReplace(promptMsg string, ow, ps bool) (string, error) {

	logging.D(3, "Entering PromptUser dialogue...")
	ctx := context.Background()

	if decisionMade {
		// If overwriteAll, return "Y" without waiting
		if ow {

			logging.D(3, "Overwrite all is set...")
			return "Y", nil
		} else if ps {

			logging.D(3, "Preserve all is set...")
			return "N", nil
		}
	}

	fmt.Println()
	logging.I(promptMsg)

	// Wait for user input
	select {
	case response := <-userInputChan:
		if response == "Y" {
			ow = true
		}
		decisionMade = true
		return response, nil

	case <-ctx.Done():
		logging.I("Operation canceled during input.")
		return "", fmt.Errorf("operation canceled")
	}
}
package utils

import "strings"

// validateExtension checks if the output extension is valid
func ValidateExtension(ext string) string {
	ext = strings.TrimSpace(ext)

	// Handle empty or invalid cases
	if ext == "" || ext == "." {
		return ""
	}

	// Ensure proper dot prefix
	if !strings.HasPrefix(ext, ".") {
		ext = "." + ext
	}

	// Verify the extension is not just a lone dot
	if len(ext) <= 1 {
		return ""
	}

	return ext
}
package main

import (
	"context"
	"fmt"
	"log"
	"metarr/internal/config"
	keys "metarr/internal/domain/keys"
	"metarr/internal/processing"
	fsRead "metarr/internal/utils/fs/read"
	logging "metarr/internal/utils/logging"
	prompt "metarr/internal/utils/prompt"
	"os"
	"os/signal"
	"path/filepath"
	"runtime/pprof"
	"runtime/trace"
	"sync"
	"syscall"
	"time"
)

var startTime time.Time

func init() {
	startTime = time.Now()
	logging.I("metarr started at: %v", startTime.Format("2006-01-02 15:04:05.00 MST"))
}

func main() {

	var (
		err       error
		directory string
	)

	// TESTING FUNCTIONS
	if config.GetBool(keys.Benchmarking) {
		// CPU profile
		cpuFile, err := os.Create("cpu.prof")
		if err != nil {
			log.Fatal("could not create CPU profile: ", err)
		}
		defer cpuFile.Close() // Don't forget to close the file
		if err := pprof.StartCPUProfile(cpuFile); err != nil {
			log.Fatal("could not start CPU profile: ", err)
		}
		defer pprof.StopCPUProfile()

		// Memory profile
		memFile, err := os.Create("mem.prof")
		if err != nil {
			log.Fatal("could not create memory profile: ", err)
		}
		defer memFile.Close()
		defer func() {
			if config.GetBool(keys.Benchmarking) {
				if err := pprof.WriteHeapProfile(memFile); err != nil {
					log.Fatal("could not write memory profile: ", err)
				}
			}
		}()

		// Trace
		traceFile, err := os.Create("trace.out")
		if err != nil {
			log.Fatal("could not create trace file: ", err)
		}
		defer traceFile.Close()
		if err := trace.Start(traceFile); err != nil {
			log.Fatal("could not start trace: ", err)
		}
		defer trace.Stop()
	}
	// END OF TESTING FUNCTIONS: MEM TEST WRITE AT BOTTOM

	if err := config.Execute(); err != nil {
		fmt.Fprintln(os.Stderr, err)
		fmt.Println()
		os.Exit(1)
	}

	if !config.GetBool("execute") {
		fmt.Println()
		logging.I(`(Separate fields supporting multiple entries by commas with no spaces e.g. "title:example,date:20240101")`)
		fmt.Println()
		return // Exit early if not meant to execute
	}

	// Handle cleanup on interrupt or termination signals
	ctx, cancel := context.WithCancel(context.Background())
	config.Set(keys.Context, ctx)
	defer cancel()

	var (
		inputVideoDir,
		inputVideo string

		openVideo *os.File
	)
	if config.IsSet(keys.VideoDir) {

		inputVideoDir = config.GetString(keys.VideoDir)
		openVideo, err = os.Open(inputVideoDir)
		if err != nil {
			logging.E(0, "Error: %v", err)
			os.Exit(1)
		}
		defer openVideo.Close()
		directory = inputVideoDir

	} else if config.IsSet(keys.VideoFile) {

		inputVideo = config.GetString(keys.VideoFile)
		openVideo, err = os.Open(inputVideo)
		if err != nil {
			logging.E(0, "Error: %v", err)
			os.Exit(1)
		}
		defer openVideo.Close()
		directory = filepath.Dir(inputVideo)
	}
	config.Set(keys.OpenVideo, openVideo)

	var (
		inputMetaDir,
		inputMeta string

		openJson *os.File
	)
	if config.IsSet(keys.JsonDir) {

		inputMetaDir = config.GetString(keys.JsonDir)
		openJson, err = os.Open(inputMetaDir)
		if err != nil {
			logging.E(0, "Error: %v", err)
			os.Exit(1)
		}
		defer openJson.Close()
		if directory == "" {
			directory = inputMetaDir
		}

	} else if config.IsSet(keys.JsonFile) {

		inputMeta = config.GetString(keys.JsonFile)
		openJson, err = os.Open(inputMeta)
		if err != nil {
			logging.E(0, "Error: %v", err)
			os.Exit(1)
		}
		defer openJson.Close()
		if directory == "" {
			directory = filepath.Dir(inputMeta)
		}
	}
	config.Set(keys.OpenJson, openJson)

	// Setup logging
	if directory != "" {
		err = logging.SetupLogging(directory)
		if err != nil {
			fmt.Printf("\n\nNotice: Log file was not created\nReason: %s\n\n", err)
		}
	} else {
		logging.I("Directory and file strings were entered empty. Exiting...")
		os.Exit(1)
	}

	if err := fsRead.InitFetchFilesVars(); err != nil {
		logging.E(0, "Failed to initialize variables to fetch files. Exiting...")
		os.Exit(1)
	}

	// Program control
	var wg sync.WaitGroup
	config.Set(keys.WaitGroup, &wg)

	cleanupChan := make(chan os.Signal, 1)
	signal.Notify(cleanupChan, syscall.SIGINT, syscall.SIGTERM)
	prompt.InitUserInputReader()

	// Proceed to process files (videos, metadata files, etc...)
	processing.ProcessFiles(ctx, cancel, &wg, cleanupChan, openVideo, openJson)

	endTime := time.Now()
	logging.I("metarr finished at: %v", endTime.Format("2006-01-02 15:04:05.00 MST"))
	logging.I("Time elapsed: %.2f seconds", endTime.Sub(startTime).Seconds())
}
