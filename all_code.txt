package config

import (
	keys "Metarr/internal/domain/keys"

	"github.com/spf13/viper"
)

// initFilesDirs initializes user flag settings for input files and directories
func initFilesDirs() {
	// Videos
	rootCmd.PersistentFlags().StringP(keys.VideoDir, "v", ".", "Video directory")
	viper.BindPFlag(keys.VideoDir, rootCmd.PersistentFlags().Lookup(keys.VideoDir))

	rootCmd.PersistentFlags().StringP(keys.VideoFile, "V", ".", "Video file")
	viper.BindPFlag(keys.VideoFile, rootCmd.PersistentFlags().Lookup(keys.VideoFile))

	// JSON
	rootCmd.PersistentFlags().StringP(keys.JsonDir, "j", ".", "JSON directory")
	viper.BindPFlag(keys.JsonDir, rootCmd.PersistentFlags().Lookup(keys.JsonDir))

	rootCmd.PersistentFlags().StringP(keys.JsonFile, "J", ".", "JSON file")
	viper.BindPFlag(keys.JsonFile, rootCmd.PersistentFlags().Lookup(keys.JsonFile))
}

// initResourceRelated initializes user flag settings for parameters related to system hardware
func initResourceRelated() {
	// Concurrency limit
	rootCmd.PersistentFlags().IntP(keys.Concurrency, "l", 5, "Max concurrency limit")
	viper.BindPFlag(keys.Concurrency, rootCmd.PersistentFlags().Lookup(keys.Concurrency))

	// CPU usage
	rootCmd.PersistentFlags().Float64P(keys.MaxCPU, "c", 100.0, "Max CPU usage")
	viper.BindPFlag(keys.MaxCPU, rootCmd.PersistentFlags().Lookup(keys.MaxCPU))

	// Min memory
	rootCmd.PersistentFlags().Uint64P(keys.MinMem, "m", 0, "Minimum RAM to start process")
	viper.BindPFlag(keys.MinMem, rootCmd.PersistentFlags().Lookup(keys.MinMem))

	// Hardware accelerated transcoding
	rootCmd.PersistentFlags().StringP(keys.GPU, "g", "none", "GPU acceleration type (nvidia, amd, intel, none)")
	viper.BindPFlag(keys.GPU, rootCmd.PersistentFlags().Lookup(keys.GPU))
}

// initAllFileTransformers initializes user flag settings for transformations applying to all files
func initAllFileTransformers() {
	// Prefix file with metafield
	rootCmd.PersistentFlags().StringSlice(keys.MFilenamePfx, nil, "Adds a specified metatag's value onto the start of the filename")
	viper.BindPFlag(keys.MFilenamePfx, rootCmd.PersistentFlags().Lookup(keys.MFilenamePfx))

	// Prefix files with date tag
	rootCmd.PersistentFlags().String(keys.InputFileDatePfx, "", "Looks for dates in metadata to prefix the video with. (date:format [e.g. Ymd for yyyy-mm-dd])")
	viper.BindPFlag(keys.InputFileDatePfx, rootCmd.PersistentFlags().Lookup(keys.InputFileDatePfx))

	// Rename convention
	rootCmd.PersistentFlags().StringP(keys.RenameStyle, "r", "skip", "Rename flag (spaces, underscores, or skip)")
	viper.BindPFlag(keys.RenameStyle, rootCmd.PersistentFlags().Lookup(keys.RenameStyle))

	// Replace filename suffix
	rootCmd.PersistentFlags().StringSliceVar(&filenameReplaceSuffixInput, keys.InputFilenameReplaceSfx, nil, "Replaces a specified suffix on filenames. (suffix:replacement)")
	viper.BindPFlag(keys.InputFilenameReplaceSfx, rootCmd.PersistentFlags().Lookup(keys.InputFilenameReplaceSfx))

	// Backup files by renaming original files
	rootCmd.PersistentFlags().BoolP(keys.NoFileOverwrite, "n", false, "Renames the original files to avoid overwriting")
	viper.BindPFlag(keys.NoFileOverwrite, rootCmd.PersistentFlags().Lookup(keys.NoFileOverwrite))

	// Output directory (can be external)
	rootCmd.PersistentFlags().StringP(keys.MoveOnComplete, "o", "", "Move files to given directory on program completion")
	viper.BindPFlag(keys.MoveOnComplete, rootCmd.PersistentFlags().Lookup(keys.MoveOnComplete))
}

// initMetaTransformers initializes user flag settings for manipulation of metadata
func initMetaTransformers() {
	// Metadata replacement & new additions
	rootCmd.PersistentFlags().StringSliceVar(&metaReplaceSuffixInput, "meta-replace-suffix", nil, "Trim suffixes from metadata fields (metatag:fieldsuffix:replacement)")
	rootCmd.PersistentFlags().StringSliceVar(&metaReplacePrefixInput, "meta-replace-prefix", nil, "Trim prefixes from metadata fields (metatag:fieldprefix:replacement)")
	rootCmd.PersistentFlags().StringSliceVar(&metaNewFieldInput, "meta-add-field", nil, "Add new fields into metadata files (metatag:value)")

	// Prefix or append description fields with dates
	rootCmd.PersistentFlags().Bool(keys.MDescDatePfx, false, "Adds the date to the start of the description field.")
	viper.BindPFlag(keys.MDescDatePfx, rootCmd.PersistentFlags().Lookup(keys.MDescDatePfx))

	rootCmd.PersistentFlags().Bool(keys.MDescDateSfx, false, "Adds the date to the end of the description field.")
	viper.BindPFlag(keys.MDescDateSfx, rootCmd.PersistentFlags().Lookup(keys.MDescDateSfx))

	// Overwrite or preserve metafields
	rootCmd.PersistentFlags().Bool(keys.MOverwrite, false, "When adding new metadata fields, automatically overwrite existing fields with your new values")
	viper.BindPFlag(keys.MOverwrite, rootCmd.PersistentFlags().Lookup(keys.MOverwrite))

	rootCmd.PersistentFlags().Bool(keys.MPreserve, false, "When adding new metadata fields, skip already existent fields")
	viper.BindPFlag(keys.MPreserve, rootCmd.PersistentFlags().Lookup(keys.MPreserve))
}

// initVideoTransformers initializes user flag settings for transformation of video files
func initVideoTransformers() {
	// Output extension type
	rootCmd.PersistentFlags().String(keys.OutputFiletype, "", "File extension to output files as (mp4 works best for most media servers)")
	viper.BindPFlag(keys.OutputFiletype, rootCmd.PersistentFlags().Lookup(keys.OutputFiletype))

}

// initFiltering initializes user flag settings for filtering files to work with
func initFiltering() {
	// Video file extensions to convert
	rootCmd.PersistentFlags().StringSliceP(keys.InputVideoExts, "e", []string{"all"}, "File extensions to convert (all, mkv, mp4, webm)")
	viper.BindPFlag(keys.InputVideoExts, rootCmd.PersistentFlags().Lookup(keys.InputVideoExts))

	// Meta file extensions to convert
	rootCmd.PersistentFlags().StringSlice(keys.InputMetaExts, []string{"all"}, "File extensions to convert (all, json, nfo)")
	viper.BindPFlag(keys.InputMetaExts, rootCmd.PersistentFlags().Lookup(keys.InputMetaExts))

	// Only convert files with prefix
	rootCmd.PersistentFlags().StringSliceP(keys.FilePrefixes, "p", []string{""}, "Filters files by prefixes")
	viper.BindPFlag(keys.FilePrefixes, rootCmd.PersistentFlags().Lookup(keys.FilePrefixes))
}

// initProgramFunctions initializes user flag settings for miscellaneous program features such as debug level
func initProgramFunctions() {
	// Debugging level
	rootCmd.PersistentFlags().Uint16P(keys.DebugLevel, "d", 0, "Level of debugging (0 - 3)")
	viper.BindPFlag(keys.DebugLevel, rootCmd.PersistentFlags().Lookup(keys.DebugLevel))

	// Skip videos, only alter metafiles
	rootCmd.PersistentFlags().Bool(keys.SkipVideos, false, "Skips compiling/transcoding the videos and just edits the file names/JSON file fields")
	viper.BindPFlag(keys.SkipVideos, rootCmd.PersistentFlags().Lookup(keys.SkipVideos))

	// Preset configurations for sites
	rootCmd.PersistentFlags().String(keys.InputPreset, "", "Use a preset configuration (e.g. censoredtv)")
	viper.BindPFlag(keys.InputPreset, rootCmd.PersistentFlags().Lookup(keys.InputPreset))

	// Output benchmarking files
	rootCmd.PersistentFlags().Bool(keys.Benchmarking, false, "Benchmarks the program")
	viper.BindPFlag(keys.Benchmarking, rootCmd.PersistentFlags().Lookup(keys.Benchmarking))
}
package config

import (
	consts "Metarr/internal/domain/constants"
	enums "Metarr/internal/domain/enums"
	keys "Metarr/internal/domain/keys"
	logging "Metarr/internal/utils/logging"
	"fmt"
	"os"
	"strings"

	"github.com/shirou/gopsutil/mem"
	"github.com/spf13/cobra"
	"github.com/spf13/viper"
)

var rootCmd = &cobra.Command{
	Use:   "metarr",
	Short: "Metarr is a video and metatagging tool",
	RunE: func(cmd *cobra.Command, args []string) error {
		if cmd.Flags().Lookup("help").Changed {
			return nil // Stop further execution if help is invoked
		}
		viper.Set("execute", true)
		return execute()
	},
}

// init sets the initial Viper settings
func init() {

	// Files and directories
	initFilesDirs()

	// System resource related
	initResourceRelated()

	// Filtering
	initFiltering()

	// All file transformations
	initAllFileTransformers()

	// Filename transformations
	initVideoTransformers()

	// Metadata and metafile manipulation
	initMetaTransformers()

	// Special functions
	initProgramFunctions()
}

// Execute is the primary initializer of Viper
func Execute() error {

	fmt.Println()

	err := rootCmd.Execute()
	if err != nil {
		logging.PrintE(0, "Failed to execute cobra")
		return err

	}
	return nil
}

// execute more thoroughly handles settings created in the Viper init
func execute() error {

	// Parse GPU settings and set commands
	verifyHWAcceleration()

	// Concurrency
	verifyConcurrencyLimit()

	// Resource usage limits (CPU and memory)
	verifyResourceLimits()

	// File extension settings
	verifyInputFiletypes()

	// File prefix filter settings
	verifyFilePrefixes()

	// Debugging level
	verifyDebugLevel()

	// Filetype to output as
	verifyOutputFiletype()

	// Meta overwrite and preserve flags
	verifyMetaOverwritePreserve()

	// Ensure no video and metadata location conflicts
	if err := checkFileDirs(); err != nil {
		return err
	}

	// Get presets
	switch viper.GetString(keys.InputPreset) {
	case "censoredtv":
		logging.PrintI("Setting preset settings for videos retrieved from Censored.tv")
		censoredTvPreset()
	default:
		// Do nothing
	}

	if err := initTextReplace(); err != nil {
		return err
	}

	if err := initDateReplaceFormat(); err != nil {
		return err
	}

	return nil
}

// checkFileDirConflicts ensures no conflicts in the file and directories entered by the user
func checkFileDirs() error {

	jsonFileSet := viper.IsSet(keys.JsonFile)
	jsonDirSet := viper.IsSet(keys.JsonDir)
	videoFileSet := viper.IsSet(keys.VideoFile)
	videoDirSet := viper.IsSet(keys.VideoDir)

	if jsonFileSet {
		if _, err := os.Stat(viper.GetString(keys.JsonFile)); err != nil {
			return fmt.Errorf("file '%s' does not exist", viper.GetString(keys.JsonFile))
		}
		// Check if it's actually a directory
		if fileInfo, _ := os.Stat(viper.GetString(keys.JsonFile)); fileInfo.IsDir() {
			return fmt.Errorf("entered directory '%s' as a file", viper.GetString(keys.JsonFile))
		}
	}

	if jsonDirSet {
		if _, err := os.Stat(viper.GetString(keys.JsonDir)); err != nil {
			return fmt.Errorf("directory '%s' does not exist", viper.GetString(keys.JsonDir))
		}
		// Check if it's actually a file
		if fileInfo, _ := os.Stat(viper.GetString(keys.JsonDir)); !fileInfo.IsDir() {
			return fmt.Errorf("entered file '%s' as a directory", viper.GetString(keys.JsonDir))
		}
	}

	if videoFileSet {
		if _, err := os.Stat(viper.GetString(keys.VideoFile)); err != nil {
			return fmt.Errorf("file '%s' does not exist", viper.GetString(keys.VideoFile))
		}
		// Check if it's actually a directory
		if fileInfo, _ := os.Stat(viper.GetString(keys.VideoFile)); fileInfo.IsDir() {
			return fmt.Errorf("entered directory '%s' as a file", viper.GetString(keys.VideoFile))
		}
	}

	if videoDirSet {
		if _, err := os.Stat(viper.GetString(keys.VideoDir)); err != nil {
			return fmt.Errorf("directory '%s' does not exist", viper.GetString(keys.VideoDir))
		}
		// Check if it's actually a file
		if fileInfo, _ := os.Stat(viper.GetString(keys.VideoDir)); !fileInfo.IsDir() {
			return fmt.Errorf("entered file '%s' as a directory", viper.GetString(keys.VideoDir))
		}
	}

	if jsonFileSet && jsonDirSet {
		return fmt.Errorf("cannot set both the JSON file and the JSON directory")
	}
	if jsonFileSet && videoDirSet {
		return fmt.Errorf("cannot set singular metadata file for whole video directory")
	}
	if videoFileSet && videoDirSet {
		return fmt.Errorf("cannot set singular video file AND video directory")
	}

	if videoFileSet {
		viper.Set(keys.SingleFile, true)
	}
	return nil
}

// verifyFilePrefixes checks and sets the file prefix filters
func verifyFilePrefixes() {

	argsInputPrefixes := viper.GetStringSlice(keys.FilePrefixes)
	filePrefixes := make([]string, 0, len(argsInputPrefixes))

	for _, arg := range argsInputPrefixes {
		if arg != "" {
			filePrefixes = append(filePrefixes, arg)
		}
	}
	if len(filePrefixes) > 0 {
		viper.Set(keys.FilePrefixes, filePrefixes)
	}
}

// verifyMetaOverwritePreserve checks if the entered meta overwrite and preserve flags are valid
func verifyMetaOverwritePreserve() {
	if GetBool(keys.MOverwrite) && GetBool(keys.MPreserve) {
		logging.PrintE(0, "Cannot enter both meta preserve AND meta overwrite, exiting...")
		os.Exit(1)
	}
}

// verifyDebugLevel checks and sets the debugging level to use
func verifyDebugLevel() {
	debugLevel := viper.GetUint16(keys.DebugLevel)
	if debugLevel > 3 {
		debugLevel = 3
	} else if debugLevel == 0 {
		logging.PrintI("Debugging level: %v", debugLevel)
	}
	viper.Set(keys.DebugLevel, debugLevel)
}

// verifyInputFiletypes checks that the inputted filetypes are accepted
func verifyInputFiletypes() {
	argsVInputExts := viper.GetStringSlice(keys.InputVideoExts)
	inputVExts := make([]enums.ConvertFromFiletype, 0, len(argsVInputExts))

	for _, data := range argsVInputExts {
		switch data {
		case "mkv":
			inputVExts = append(inputVExts, enums.VID_EXTS_MKV)
		case "mp4":
			inputVExts = append(inputVExts, enums.VID_EXTS_MP4)
		case "webm":
			inputVExts = append(inputVExts, enums.VID_EXTS_WEBM)
		default:
			inputVExts = append(inputVExts, enums.VID_EXTS_ALL)
		}
	}
	if len(inputVExts) == 0 {
		inputVExts = append(inputVExts, enums.VID_EXTS_ALL)
	}
	logging.PrintD(2, "Received video input extension filter: %v", inputVExts)
	viper.Set(keys.InputVExtsEnum, inputVExts)

	argsMInputExts := viper.GetStringSlice(keys.InputMetaExts)
	inputMExts := make([]enums.MetaFiletypeFilter, 0, len(argsMInputExts))

	for _, data := range argsMInputExts {
		switch data {
		case "json":
			inputMExts = append(inputMExts, enums.META_EXTS_JSON)
		case "nfo":
			inputMExts = append(inputMExts, enums.META_EXTS_NFO)
		default:
			inputMExts = append(inputMExts, enums.META_EXTS_ALL)
		}
	}
	if len(inputMExts) == 0 {
		inputMExts = append(inputMExts, enums.META_EXTS_ALL)
	}
	logging.PrintD(2, "Received meta input extension filter: %v", inputMExts)
	viper.Set(keys.InputMExtsEnum, inputMExts)
}

// verifyHWAcceleration checks and sets HW acceleration to use
func verifyHWAcceleration() {
	switch viper.GetString(keys.GPU) {
	case "nvidia":
		viper.Set(keys.GPUEnum, enums.GPU_NVIDIA)
		logging.Print("GPU acceleration selected by user: %v", keys.GPU)
	case "amd":
		viper.Set(keys.GPUEnum, enums.GPU_AMD)
		logging.Print("GPU acceleration selected by user: %v", keys.GPU)
	case "intel":
		viper.Set(keys.GPUEnum, enums.GPU_INTEL)
		logging.Print("GPU acceleration selected by user: %v", keys.GPU)
	default:
		viper.Set(keys.GPUEnum, enums.GPU_NO_HW_ACCEL)
	}
}

// verifyConcurrencyLimit checks and ensures correct concurrency limit input
func verifyConcurrencyLimit() {
	maxConcurrentProcesses := viper.GetInt(keys.Concurrency)

	switch {
	case maxConcurrentProcesses < 1:
		maxConcurrentProcesses = 1
		logging.PrintE(2, "Max concurrency set too low, set to minimum value: %d", maxConcurrentProcesses)
	default:
		logging.PrintI("Max concurrency: %d", maxConcurrentProcesses)
	}
	viper.Set(keys.Concurrency, maxConcurrentProcesses)
}

// verifyCPUUsage verifies the value used to limit the CPU needed to spawn a new routine
func verifyResourceLimits() {
	MinMemUsage := viper.GetUint64(keys.MinMem)
	MinMemUsage *= 1024 * 1024 // Convert input to MB

	currentAvailableMem, err := mem.VirtualMemory()
	if err != nil {
		logging.PrintE(0, "Could not get system memory, using default max RAM requirements", err)
		currentAvailableMem.Available = 1024
	}
	if MinMemUsage > currentAvailableMem.Available {
		MinMemUsage = currentAvailableMem.Available
	}

	if MinMemUsage > 0 {
		logging.PrintI("Min RAM to spawn process: %v", MinMemUsage)
	}
	viper.Set(keys.MinMemMB, MinMemUsage)

	maxCPUUsage := viper.GetFloat64(keys.MaxCPU)
	switch {
	case maxCPUUsage > 100.0:
		maxCPUUsage = 100.0
		logging.PrintE(2, "Max CPU usage entered too high, setting to default max: %.2f%%", maxCPUUsage)

	case maxCPUUsage < 1.0:
		maxCPUUsage = 10.0
		logging.PrintE(0, "Max CPU usage entered too low, setting to default low: %.2f%%", maxCPUUsage)
	}
	if maxCPUUsage != 100.0 {
		logging.PrintI("Max CPU usage: %.2f%%", maxCPUUsage)
	}
	viper.Set(keys.MaxCPU, maxCPUUsage)
}

// Verify the output filetype is valid for FFmpeg
func verifyOutputFiletype() {
	o := GetString(keys.OutputFiletype)
	o = strings.TrimSpace(o)

	if !strings.HasPrefix(o, ".") {
		o = "." + o
		Set(keys.OutputFiletype, o)
	}

	valid := false
	for _, ext := range consts.AllVidExtensions {
		if o != ext {
			continue
		}
		valid = true
		break
	}

	switch valid {
	case true:
		logging.PrintI("Outputting files as %s", o)
	default:
		Set(keys.OutputFiletype, "")
	}
}
package config

import (
	keys "Metarr/internal/domain/keys"
	"Metarr/internal/models"
	"strings"
)

// AutoPreset is a switch point for selecting from a range of presets.
// These presets apply common fixes for video/meta files for specific
// video sources.
//
// For example, censored.tv downloads through yt-dlp are often affixed
// with "_1" when filenames are restricted. And titles are often
// affixed with " (1)"
func AutoPreset(url string) {
	if strings.Contains(url, "censored.tv") {
		censoredTvPreset()
	}
}

// censoredTvPreset for censored.tv:
//
// Removes (1) from title fields
// Removes -1 from id and display_id (probably inconsequential)
// Removes the _1 suffix from restricted filenames
func censoredTvPreset() {

	var (
		metaReplaceSuffix []*models.MetaReplaceSuffix
		sfx               *models.MetaReplaceSuffix
	)

	sfx = &models.MetaReplaceSuffix{
		Field:       "title",
		Suffix:      " (1)",
		Replacement: "",
	}
	metaReplaceSuffix = append(metaReplaceSuffix, sfx)

	sfx = &models.MetaReplaceSuffix{
		Field:       "fulltitle",
		Suffix:      " (1)",
		Replacement: "",
	}
	metaReplaceSuffix = append(metaReplaceSuffix, sfx)

	sfx = &models.MetaReplaceSuffix{
		Field:       "id",
		Suffix:      "-1",
		Replacement: "",
	}
	metaReplaceSuffix = append(metaReplaceSuffix, sfx)

	sfx = &models.MetaReplaceSuffix{
		Field:       "display_id",
		Suffix:      "-1",
		Replacement: "",
	}
	metaReplaceSuffix = append(metaReplaceSuffix, sfx)

	Set(keys.MReplaceSfx, metaReplaceSuffix)

	var filenameReplaceSuffix []*models.FilenameReplaceSuffix

	trimEnd := &models.FilenameReplaceSuffix{
		Suffix:      "_1",
		Replacement: "",
	}
	filenameReplaceSuffix = append(filenameReplaceSuffix, trimEnd)
	Set(keys.FilenameReplaceSfx, filenameReplaceSuffix)
}
package config

import (
	enums "Metarr/internal/domain/enums"
	keys "Metarr/internal/domain/keys"
	"Metarr/internal/models"
	logging "Metarr/internal/utils/logging"
	"fmt"
	"strings"
)

var (
	metaReplaceSuffixInput,
	metaReplacePrefixInput,
	metaNewFieldInput,
	filenameReplaceSuffixInput []string
)

// initTextReplace initializes text replacement functions
func initTextReplace() error {

	// Parse rename flag
	setRenameFlag()

	// Add new field
	if err := validateNewMetafields(); err != nil {
		return err
	}

	// Replace metafield value suffixes
	if err := validateMetaSuffixes(); err != nil {
		return err
	}

	// Replace metafield value prefixes
	if err := validateMetaPrefixes(); err != nil {
		return err
	}

	// Replace filename suffixes
	if err := validateFilenameSuffixReplace(); err != nil {
		return err
	}

	return nil
}

// validateFilenameSuffixReplace checks if the input format for filename suffix replacement is valid
func validateFilenameSuffixReplace() error {
	filenameReplaceSuffix := make([]*models.FilenameReplaceSuffix, 0, len(filenameReplaceSuffixInput))

	for _, pair := range filenameReplaceSuffixInput {
		parts := strings.SplitN(pair, ":", 2)
		if len(parts) < 2 {
			return fmt.Errorf("invalid use of filename-replace-suffix, values must be written as (suffix:replacement)")
		}
		filenameReplaceSuffix = append(filenameReplaceSuffix, &models.FilenameReplaceSuffix{
			Suffix:      parts[0],
			Replacement: parts[1],
		})
	}
	if len(filenameReplaceSuffix) > 0 {
		logging.PrintI("Meta replace suffixes: %v", filenameReplaceSuffix)
		Set(keys.FilenameReplaceSfx, filenameReplaceSuffix)
	}
	return nil
}

// validateMetaPrefixes checks if the input format for meta prefix alterations is valid
func validateMetaPrefixes() error {
	metaReplacePrefix := make([]*models.MetaReplacePrefix, 0, len(metaReplacePrefixInput))

	for _, tuple := range metaReplacePrefixInput {
		parts := strings.SplitN(tuple, ":", 3)
		if len(parts) < 3 {
			return fmt.Errorf("invalid use of meta-replace-prefix, values must be written as (metatag:fieldprefix:replacement)")
		}
		metaReplacePrefix = append(metaReplacePrefix, &models.MetaReplacePrefix{
			Field:       parts[0],
			Prefix:      parts[1],
			Replacement: parts[2],
		})
	}
	if len(metaReplacePrefix) > 0 {
		logging.PrintI("Meta replace prefixes: %v", metaReplacePrefix)
		Set(keys.MReplacePfx, metaReplacePrefix)
	}
	return nil
}

// validateMetaSuffixes checks if the input format for meta suffix alterations is valid
func validateMetaSuffixes() error {
	metaReplaceSuffix := make([]*models.MetaReplaceSuffix, 0, len(metaReplaceSuffixInput))

	for _, tuple := range metaReplaceSuffixInput {
		parts := strings.SplitN(tuple, ":", 3)
		if len(parts) < 3 {
			return fmt.Errorf("invalid use of meta-replace-suffix, values must be written as (metatag:fieldsuffix:replacement)")
		}
		metaReplaceSuffix = append(metaReplaceSuffix, &models.MetaReplaceSuffix{
			Field:       parts[0],
			Suffix:      parts[1],
			Replacement: parts[2],
		})
	}
	if len(metaReplaceSuffix) > 0 {
		logging.PrintI("Meta replace suffixes: %v\n", metaReplaceSuffix)
		Set(keys.MReplaceSfx, metaReplaceSuffix)
	}
	return nil
}

// validateNewMetafields checks if the input format for metatag and field additions is valid
func validateNewMetafields() error {
	metaNewField := make([]*models.MetaNewField, 0, len(metaNewFieldInput))

	for _, value := range metaNewFieldInput {
		parts := strings.SplitN(value, ":", 2)
		if len(parts) < 2 {
			return fmt.Errorf("invalid use of metadata addition, values must be written as (metatag:field)")
		}
		// Append each parsed field-value pair to the metaNewField array
		metaNewField = append(metaNewField, &models.MetaNewField{
			Field: parts[0],
			Value: parts[1],
		})
	}
	if len(metaNewField) > 0 {
		logging.PrintI("Meta new fields to add: %v", metaNewField)
		Set(keys.MNewField, metaNewField)
	}
	return nil
}

// setRenameFlag sets the rename style to apply
func setRenameFlag() {

	var renameFlag enums.ReplaceToStyle
	argRenameFlag := GetString(keys.RenameStyle)

	switch argRenameFlag {
	case "spaces", "space":
		renameFlag = enums.RENAMING_SPACES
		logging.Print("Rename style selected: %v", argRenameFlag)

	case "underscores", "underscore":
		renameFlag = enums.RENAMING_UNDERSCORES
		logging.Print("Rename style selected: %v", argRenameFlag)
	default:
		logging.PrintD(1, "'Spaces' or 'underscores' not selected for renaming style, skipping these modifications.")
		renameFlag = enums.RENAMING_SKIP
	}
	Set(keys.Rename, renameFlag)
}

// initDateReplaceFormat initializes the user's preferred format for dates
func initDateReplaceFormat() error {

	var formatEnum enums.FilenameDateFormat
	dateFmt := GetString(keys.InputFileDatePfx)

	if dateFmt == "" {
		formatEnum = enums.FILEDATE_SKIP
	} else if len(dateFmt) != 3 {
		return fmt.Errorf("invalid date format entered, please enter three characters (where 'Y' is yyyy and 'y' is yy)")
	} else {
		switch dateFmt {
		case "Ymd":
			formatEnum = enums.FILEDATE_YYYY_MM_DD
		case "ymd":
			formatEnum = enums.FILEDATE_YY_MM_DD
		case "Ydm":
			formatEnum = enums.FILEDATE_YYYY_DD_MM
		case "ydm":
			formatEnum = enums.FILEDATE_YY_DD_MM
		case "dmY":
			formatEnum = enums.FILEDATE_DD_MM_YYYY
		case "dmy":
			formatEnum = enums.FILEDATE_DD_MM_YY
		case "mdY":
			formatEnum = enums.FILEDATE_MM_DD_YYYY
		case "mdy":
			formatEnum = enums.FILEDATE_MM_DD_YY
		case "":
			formatEnum = enums.FILEDATE_SKIP
		default:
			return fmt.Errorf("invalid date format entered, please enter three characters (where capital Y is yyyy and y is yy)")
		}
	}
	Set(keys.FileDateFmt, formatEnum)
	logging.PrintD(1, "Set file date format to %v", formatEnum)
	return nil
}
package config

import (
	"github.com/spf13/viper"
)

// Set sets the value for the key in the override register. Set is case-insensitive for a key. Will be used instead of values obtained via flags, config file, ENV, default, or key/value store.
func Set(key string, value any) {

	viper.Set(key, value)
}

// Get can retrieve any value given the key to use. Get is case-insensitive for a key. Get has the behavior of returning the value associated with the first place from where it is set. Viper will check in the following order: override, flag, env, config file, key/value store, default
// Get returns an interface. For a specific value use one of the Get____ methods.
func Get(key string) any {
	return viper.Get(key)
}

// GetBool returns the value associated with the key as a boolean.
func GetBool(key string) bool {
	return viper.GetBool(key)
}

// GetInt returns the value associated with the key as an integer.
func GetInt(key string) int {
	return viper.GetInt(key)
}

// GetUint64 returns the value associated with the key as an unsigned integer.
func GetUint64(key string) uint64 {
	return viper.GetUint64(key)
}

// GetFloat64 returns the value associated with the key as a float64.
func GetFloat64(key string) float64 {
	return viper.GetFloat64(key)
}

// GetString returns the value associated with the key as a string.
func GetString(key string) string {
	return viper.GetString(key)
}

// GetStringSlice returns the value associated with the key as a slice of strings.
func GetStringSlice(key string) []string {
	return viper.GetStringSlice(key)
}

// IsSet checks to see if the key has been set in any of the data locations.
// IsSet is case-insensitive for a key.
func IsSet(key string) bool {
	return viper.IsSet(key)
}
package domain

// Colors
const (
	ColorReset  = "\033[0m"
	ColorRed    = "\033[91m"
	ColorGreen  = "\033[92m"
	ColorYellow = "\033[93m"
	ColorBlue   = "\033[34m"
	ColorPurple = "\033[35m"
	ColorCyan   = "\033[96m"
	ColorWhite  = "\033[37m"
)

const (
	RedError     string = ColorRed + "[ERROR] " + ColorReset
	GreenSuccess string = ColorGreen + "[SUCCESS] " + ColorReset
	YellowDebug  string = ColorYellow + "[DEBUG] " + ColorReset
	BlueInfo     string = ColorCyan + "[Info] " + ColorReset
)
package domain

// File prefix and suffix
const (
	OldTag  = "_metarrbackup"
	TempTag = "tmp_"
)

var (
	AllVidExtensions = [...]string{".3gp", ".avi", ".f4v", ".flv", ".m4v",
		".mkv", ".mov", ".mp4", ".mpeg", ".mpg",
		".ogm", ".ogv", ".ts", ".vob", ".webm",
		".wmv"}
)

var (
	AllMetaExtensions = [...]string{".json", ".nfo"}
)

// Webpage tags
var (
	// Ensure lengths match
	WebDateTags        = [...]string{"release-date", "upload-date", "date", "date-text", "text-date"}
	WebDescriptionTags = [...]string{"description", "longdescription", "long-description", "summary", "synopsis",
		"check-for-urls"}

	// Credits tags, and nested elements
	WebCreditsTags      = [...]string{"creator", "uploader", "uploaded-by", "uploaded_by", "channel-name", "claim-preview__title"}
	WebCreditsSelectors = map[string]string{
		"claim-preview__title":               "truncated-text",
		`script[type="application/ld+json"]`: "author.name",
	}

	WebTitleTags = [...]string{"video-title", "video-name"}
)
package domain

var (
	ContractionsSpaced = map[string]string{
		"ain t":     "aint",
		"can t":     "cant",
		"don t":     "dont",
		"didn t":    "didnt",
		"hasn t":    "hasnt",
		"haven t":   "havent",
		"won t":     "wont",
		"wouldn t":  "wouldnt",
		"shouldn t": "shouldnt",
		"couldn t":  "couldnt",
		"wasn t":    "wasnt",
		"weren t":   "werent",
		"let s":     "lets",
		"hadn t":    "hadnt",
		"who s":     "whos",
		"what s":    "whats",
		"when s":    "whens",
		"where s":   "wheres",
		"why s":     "whys",
		"how s":     "hows",
		"there s":   "theres",
		"that s":    "thats",
		"it d":      "itd",
		"she d":     "shed",
		"she s":     "shes",
		"he d":      "hed",
		"he s":      "hes",
		"it ll":     "itll",
		"should ve": "shouldve",
		"could ve":  "couldve",
		"would ve":  "wouldve",
	}

	ContractionsUnderscored = map[string]string{
		"ain_t":     "aint",
		"can_t":     "cant",
		"don_t":     "dont",
		"didn_t":    "didnt",
		"hasn_t":    "hasnt",
		"haven_t":   "havent",
		"won_t":     "wont",
		"wouldn_t":  "wouldnt",
		"shouldn_t": "shouldnt",
		"couldn_t":  "couldnt",
		"wasn_t":    "wasnt",
		"weren_t":   "werent",
		"let_s":     "lets",
		"hadn_t":    "hadnt",
		"who_s":     "whos",
		"what_s":    "whats",
		"when_s":    "whens",
		"where_s":   "wheres",
		"why_s":     "whys",
		"how_s":     "hows",
		"there_s":   "theres",
		"that_s":    "thats",
		"it_d":      "itd",
		"she_d":     "shed",
		"she_s":     "shes",
		"he_d":      "hed",
		"he_s":      "hes",
		"it_ll":     "itll",
		"should_ve": "shouldve",
		"could_ve":  "couldve",
		"would_ve":  "wouldve",
	}
)
package domain

// AV codec copy
var (
	AVCodecCopy = [...]string{"-codec", "copy"}
)

// Audio flags
var (
	AudioCodecCopy = [...]string{"-c:a", "copy"}
	AudioToAAC     = [...]string{"-c:a", "aac"}
	AudioBitrate   = [...]string{"-b:a", "256k"}
)

// Video flags
var (
	VideoCodecCopy      = [...]string{"-c:v", "copy"}
	VideoToH264Balanced = [...]string{"-c:v", "libx264", "-crf", "23", "-profile:v", "main"}
	PixelFmtYuv420p     = [...]string{"-pix_fmt", "yuv420p"}
	KeyframeBalanced    = [...]string{"-g", "50", "-keyint_min", "30"}
)

// GPU hardware flags
var (
	NvidiaAccel = [...]string{"-hwaccel", "nvdec"}
	AMDAccel    = [...]string{"-hwaccel", "vulkan"}
	IntelAccel  = [...]string{"-hwaccel", "qsv"}
)
package domain

const (
	JActor     = "actor"
	JAuthor    = "author"
	JArtist    = "artist"
	JChannel   = "channel"
	JComposer  = "composer"
	JCreator   = "creator"
	JDirector  = "director"
	JPerformer = "performer"
	JProducer  = "producer"
	JPublisher = "publisher"
	JStudio    = "studio"
	JUploader  = "uploader"
	JWriter    = "writer"
)

const (
	JComment          = "comment"
	JDescription      = "description"
	JFallbackTitle    = "title"
	JLongDescription  = "longdescription"
	JLong_Description = "long_description"
	JSubtitle         = "subtitle"
	JSummary          = "summary"
	JSynopsis         = "synopsis"
	JTitle            = "fulltitle"
)

const (
	JCreationTime        = "creation_time"
	JDate                = "date"
	JFormattedDate       = "formatted_date"
	JOriginallyAvailable = "originally_available_at"
	JReleaseDate         = "release_date"
	JUploadDate          = "upload_date"
	JYear                = "year"
	JReleaseYear         = "release_year"
)

const (
	JDomain        = "domain"
	JReferer       = "referer"
	JURL           = "url"
	JWebpageDomain = "webpage_url_domain"
	JWebpageURL    = "webpage_url"
)
package domain

// Log file keys
const (
	LogFinished = "FINISHED: "
	LogError    = "ERROR: "
	LogFailure  = "FAILED: "
	LogSuccess  = "Success: "
	LogInfo     = "Info: "
	LogWarning  = "Warning: "
	LogBasic    = ""
)
package domain

const (
	// Core Descriptive Metadata
	NTitle         = "title"
	NOriginalTitle = "originaltitle"
	NSortTitle     = "sorttitle"
	NTagline       = "tagline"
	NDescription   = "description"
	NPlot          = "plot"
	NOutline       = "outline"
	NShowTitle     = "showtitle"
	NSubtitle      = "subtitle"

	// Cast and Crew Metadata
	NActors   = "actor"
	NDirector = "director"
	NWriter   = "writer"
	NComposer = "composer"
	NProducer = "producer"

	// Genre, Category, and Rating Metadata
	NGenre      = "genre"
	NMood       = "mood"
	NMPAA       = "mpaa"
	NVotes      = "votes"
	NRatingsURL = "ratingurl"

	// Date and Release Metadata
	NAired        = "aired"
	NPremiereDate = "premiered"
	NYear         = "year"

	// Episodic Metadata
	NSeason       = "season"
	NEpisode      = "episode"
	NEpisodeTitle = "episodetitle"

	// Technical Information
	NCountry   = "country"
	NLanguage  = "language"
	NRated     = "rated"
	NEncodedBy = "encodedby"
	NRuntime   = "runtime"
	NRating    = "rating"

	// Production Metadata
	NProductionCompany = "productioncompany"
	NStudio            = "studio"
	NCoverArtist       = "coverartist"
	NPublisher         = "publisher"
	NCompilation       = "compilation"

	// Artwork, Media Assets, and Related Links
	NThumb    = "thumb"
	NFanart   = "fanart"
	NTrailer  = "trailer"
	NCoverArt = "cover_art"

	// Sorting and Alternate Display Titles
	NShowSortTitle = "showsorttitle"

	// Miscellaneous
	NComment = "comment"
	NTop250  = "top250"
	NTrack   = "track"
	NAlbum   = "album"
	NLicence = "license"
	NRights  = "rights"
	NURL     = "url"
)
package domain

// User selection of filetypes to convert from
type ConvertFromFiletype int

const (
	VID_EXTS_ALL ConvertFromFiletype = iota
	VID_EXTS_MKV
	VID_EXTS_MP4
	VID_EXTS_WEBM
)

// MetaFiletypeFilter filters the metadata files to read from
type MetaFiletypeFilter int

const (
	META_EXTS_ALL MetaFiletypeFilter = iota
	META_EXTS_JSON
	META_EXTS_NFO
)

// User system graphics hardware for transcoding
type SysGPU int

const (
	GPU_NVIDIA SysGPU = iota
	GPU_AMD
	GPU_INTEL
	GPU_NO_HW_ACCEL
)

// Naming syle
type ReplaceToStyle int

const (
	RENAMING_SPACES ReplaceToStyle = iota
	RENAMING_UNDERSCORES
	RENAMING_SKIP
)

// Date formats
type FilenameDateFormat int

const (
	FILEDATE_YYYY_MM_DD FilenameDateFormat = iota
	FILEDATE_YY_MM_DD
	FILEDATE_YYYY_DD_MM
	FILEDATE_YY_DD_MM
	FILEDATE_DD_MM_YYYY
	FILEDATE_DD_MM_YY
	FILEDATE_MM_DD_YYYY
	FILEDATE_MM_DD_YY
	FILEDATE_SKIP
)

// Web tags
type MetaFiletypeFound int

const (
	METAFILE_JSON MetaFiletypeFound = iota
	METAFILE_NFO
	WEBCLASS_XML
)

// Viper variable types
type ViperVarTypes int

const (
	VIPER_ANY ViperVarTypes = iota
	VIPER_BOOL
	VIPER_INT
	VIPER_STRING
	VIPER_STRING_SLICE
)

// Web tags
type WebClassTags int

const (
	WEBCLASS_DATE WebClassTags = iota
	WEBCLASS_TITLE
	WEBCLASS_DESCRIPTION
	WEBCLASS_CREDITS
	WEBCLASS_WEBINFO
)

// Presets
type SitePresets int

const (
	PRESET_CENSOREDTV SitePresets = iota
)
package domain

// Terminal keys
const (
	VideoDir                string = "video-dir"
	VideoFile               string = "video-file"
	JsonDir                 string = "json-dir"
	JsonFile                string = "json-file"
	InputVideoExts          string = "input-video-exts"
	InputMetaExts           string = "input-meta-exts"
	FilePrefixes            string = "prefix"
	Concurrency             string = "concurrency"
	MaxCPU                  string = "max-cpu"
	MinMem                  string = "min-mem"
	MinMemMB                string = "min-mem-mb"
	GPU                     string = "gpu"
	GetLatest               string = "get-latest"
	MFilenamePfx            string = "metadata-filename-prefix"
	InputFilenameReplaceSfx string = "filename-replace-suffix"
	InputFileDatePfx        string = "filename-date-tag"
	RenameStyle             string = "input-rename-style"
	MReplaceSfx             string = "meta-trim-suffix"
	MReplacePfx             string = "meta-trim-prefix"
	MNewField               string = "meta-add-field"
	MOverwrite              string = "meta-overwrite"
	MPreserve               string = "meta-preserve"
	DebugLevel              string = "debug-level"
	SkipVideos              string = "skip-videos"
	NoFileOverwrite         string = "no-file-overwrite"
	MDescDatePfx            string = "desc-date-prefix"
	MDescDateSfx            string = "desc-date-suffix"
	InputPreset             string = "preset"
	MoveOnComplete          string = "output-directory"
	OutputFiletype          string = "ext"
	Benchmarking            string = "benchmark"
)

// Primary program
const (
	Context    string = "Context"
	WaitGroup  string = "WaitGroup"
	SingleFile string = "SingleFile"
)

// Files and directories
const (
	OpenVideo string = "openVideo"
	OpenJson  string = "openJson"
	VideoMap  string = "videoMap"
	MetaMap   string = "metaMap"
)

// Filter for files
const (
	InputVExtsEnum string = "inputVideoExtsEnum"
	InputMExtsEnum string = "inputMetaExtsEnum"
)

// Performance
const (
	GPUEnum string = "gpuEnum"
)

// Filename edits
const (
	Rename             string = "Rename"
	FileDateFmt        string = "filenameDateTag"
	FilenameReplaceSfx string = "filenameReplaceSfx"
)

// Contains the fields which accept multiple entries as string arrays
var MultiEntryFields = []string{
	InputVideoExts,
	InputMetaExts,
	FilePrefixes,
	MReplaceSfx,
	MReplacePfx,
	MNewField,
	FilenameReplaceSfx,
}
package metadata

import (
	"Metarr/internal/models"
	logging "Metarr/internal/utils/logging"
	"fmt"
	"strconv"
	"strings"

	"github.com/araddon/dateparse"
)

// ParseAndFormatDate parses and formats the inputted date string
func ParseStringDate(dateString string) (string, error) {

	t, err := dateparse.ParseAny(dateString)
	if err != nil {
		return "", fmt.Errorf("unable to parse date: %s", dateString)
	}

	return t.Format("2006-01-02"), nil
}

// ParseAndFormatDate parses and formats the inputted date string
func ParseNumDate(dateNum string) (string, error) {

	t, err := dateparse.ParseAny(dateNum)
	if err != nil {
		return "", fmt.Errorf("unable to parse date '%s' to word date", dateNum)
	}
	time := t.Format("01022006")
	if time == "" {
		time = t.Format("010206")
	}

	var day, month, year, dateStr string

	if len(time) < 6 {
		return dateNum, fmt.Errorf("unable to parse date, date '%s' is too short", time)
	}

	if len(time) >= 8 {
		day = time[2:4]
		month = time[:2]
		year = time[4:8]
	} else if len(time) >= 6 {
		day = time[2:4]
		month = time[:2]
		year = time[4:6]
	}

	month = monthStringSwitch(month)
	day = dayStringSwitch(day)

	dateStr = month + " " + day + ", " + year
	logging.PrintS(1, "Made string form date: '%s'", dateStr)

	return dateStr, nil
}

// Convert a numerical month to a word
func monthStringSwitch(month string) string {
	var monthStr string
	switch month {
	case "01":
		monthStr = "Jan"
	case "02":
		monthStr = "Feb"
	case "03":
		monthStr = "Mar"
	case "04":
		monthStr = "Apr"
	case "05":
		monthStr = "May"
	case "06":
		monthStr = "Jun"
	case "07":
		monthStr = "Jul"
	case "08":
		monthStr = "Aug"
	case "09":
		monthStr = "Sep"
	case "10":
		monthStr = "Oct"
	case "11":
		monthStr = "Nov"
	case "12":
		monthStr = "Dec"
	default:
		logging.PrintE(0, "Failed to make month string from month number '%s'", month)
		monthStr = "Jan"
	}
	return monthStr
}

// Affix a numerical day with the appropriate suffix (e.g. '1st', '2nd', '3rd')
func dayStringSwitch(day string) string {
	if thCheck, err := strconv.Atoi(day); err != nil {
		logging.PrintE(0, "Failed to convert date string to number")
		return day
	} else if thCheck > 10 && thCheck < 20 {
		return day + "th"
	}
	switch {
	case strings.HasSuffix(day, "1"):
		return day + "st"
	case strings.HasSuffix(day, "2"):
		return day + "nd"
	case strings.HasSuffix(day, "3"):
		return day + "rd"
	default:
		return day + "th"
	}
}

// YyyyMmDd converts inputted date strings into the user's defined format
func YyyyMmDd(fieldValue string) (string, bool) {

	var t string = ""
	if tIdx := strings.Index(fieldValue, "T"); tIdx != -1 {
		t = fieldValue[tIdx:]
	}

	fieldValue = strings.ReplaceAll(fieldValue, "-", "")

	if len(fieldValue) >= 8 {
		formatted := fmt.Sprintf("%s-%s-%s%s", fieldValue[:4], fieldValue[4:6], fieldValue[6:8], t)
		logging.PrintS(2, "Made date %s", formatted)
		return formatted, true

	} else if len(fieldValue) >= 6 {
		formatted := fmt.Sprintf("%s-%s-%s%s", fieldValue[:2], fieldValue[2:4], fieldValue[4:6], t)
		logging.PrintS(2, "Made date %s", formatted)
		return formatted, true
	}
	logging.PrintD(3, "Returning empty or short date element (%s) without formatting", fieldValue)
	return fieldValue, false
}

// FormatAllDates formats timestamps into a hyphenated form
func FormatAllDates(fd *models.FileData) string {

	var (
		result string
		ok     bool
	)

	d := fd.MDates

	if !ok && d.Originally_Available_At != "" {
		logging.PrintD(2, "Attempting to format originally available date: %v", d.Originally_Available_At)
		result, ok = YyyyMmDd(d.Originally_Available_At)
	}
	if !ok && d.ReleaseDate != "" {
		logging.PrintD(2, "Attempting to format release date: %v", d.ReleaseDate)
		result, ok = YyyyMmDd(d.ReleaseDate)
	}
	if !ok && d.Date != "" {
		logging.PrintD(2, "Attempting to format date: %v", d.Date)
		result, ok = YyyyMmDd(d.Date)
	}
	if !ok && d.UploadDate != "" {
		logging.PrintD(2, "Attempting to format upload date: %v", d.UploadDate)
		result, ok = YyyyMmDd(d.UploadDate)
	}
	if !ok && d.Creation_Time != "" {
		logging.PrintD(3, "Attempting to format creation time: %v", d.Creation_Time)
		result, ok = YyyyMmDd(d.Creation_Time)
	}
	if !ok {
		logging.PrintE(0, "Failed to format dates")
		return ""
	} else {
		logging.PrintD(2, "Exiting with formatted date: %v", result)

		d.FormattedDate = result

		logging.PrintD(2, "Got formatted date '%s' and entering parse to string function...", result)

		var err error
		d.StringDate, err = ParseNumDate(d.FormattedDate)
		if err != nil {
			logging.PrintE(0, err.Error())
		}

		return result
	}
}
package metadata

import (
	consts "Metarr/internal/domain/constants"
	enums "Metarr/internal/domain/enums"
	"Metarr/internal/models"
	browser "Metarr/internal/utils/browser"
	logging "Metarr/internal/utils/logging"
)

// fillCredits fills in the metadator for credits (e.g. actor, director, uploader)
func fillCredits(fd *models.FileData, data map[string]interface{}) (map[string]interface{}, bool) {

	c := fd.MCredits
	w := fd.MWebData

	fieldMap := map[string]*string{
		// Order by importance
		consts.JCreator:   &c.Creator,
		consts.JPerformer: &c.Performer,
		consts.JAuthor:    &c.Author,
		consts.JArtist:    &c.Artist, // May be alias for "author" in some systems
		consts.JChannel:   &c.Channel,
		consts.JDirector:  &c.Director,
		consts.JActor:     &c.Actor,
		consts.JStudio:    &c.Studio,
		consts.JProducer:  &c.Producer,
		consts.JWriter:    &c.Writer,
		consts.JUploader:  &c.Uploader,
		consts.JPublisher: &c.Publisher,
		consts.JComposer:  &c.Composer,
	}

	dataFilled := unpackJSON("credits", fieldMap, data)

	// Check if filled
	for key, val := range fieldMap {
		if val == nil {
			logging.PrintE(0, "Value is null")
			continue
		}
		if *val == "" {
			logging.PrintD(2, "Value for '%s' is empty, attempting to fill by inference...", key)
			*val = fillEmptyCredits(c)
			logging.PrintD(2, "Set value to '%s'", *val)
			if *val != "" {
				dataFilled = true
			}
		} else if *val != "" {
			dataFilled = true
		}
	}

	switch {
	case dataFilled:
		rtn, err := fd.JSONFileRW.WriteMetadata(fieldMap)
		switch {
		case err != nil:
			logging.PrintE(0, "Failed to write into JSON file '%s': %v", fd.JSONFilePath, err)
			return data, true
		case rtn != nil:
			data = rtn
			return data, true
		}

	case w.WebpageURL == "":
		logging.PrintI("Page URL not found in metadata, so cannot scrape for missing credits in '%s'", fd.JSONFilePath)
		return data, false
	}

	credits := browser.ScrapeMeta(w, enums.WEBCLASS_CREDITS)
	if credits != "" {
		for _, value := range fieldMap {
			if *value == "" {
				*value = credits
			}
		}

		rtn, err := fd.JSONFileRW.WriteMetadata(fieldMap)
		switch {
		case err != nil:
			logging.PrintE(0, "Failed to write new metadata (%s) into JSON file '%s': %v", credits, fd.JSONFilePath, err)
			return data, true
		case rtn != nil:
			data = rtn
			return data, true
		}

	}
	return data, false
}

// fillEmptyCredits attempts to fill empty fields by inference
func fillEmptyCredits(c *models.MetadataCredits) string {

	// Order by importance
	switch {
	case c.Creator != "":
		return c.Creator

	case c.Author != "":
		return c.Author

	case c.Publisher != "":
		return c.Publisher

	case c.Producer != "":
		return c.Producer

	case c.Actor != "":
		return c.Actor

	case c.Channel != "":
		return c.Channel

	case c.Performer != "":
		return c.Performer

	case c.Uploader != "":
		return c.Uploader

	case c.Artist != "":
		return c.Artist

	case c.Director != "":
		return c.Director

	case c.Studio != "":
		return c.Studio

	case c.Writer != "":
		return c.Writer

	case c.Composer != "":
		return c.Composer
	default:
		return ""
	}
}
package metadata

import (
	consts "Metarr/internal/domain/constants"
	enums "Metarr/internal/domain/enums"
	helpers "Metarr/internal/metadata/process/helpers"
	"Metarr/internal/models"
	browser "Metarr/internal/utils/browser"
	logging "Metarr/internal/utils/logging"
	print "Metarr/internal/utils/print"
	"strings"
)

// fillTimestamps grabs timestamp metadata from JSON
func fillTimestamps(fd *models.FileData, data map[string]interface{}) (map[string]interface{}, bool) {
	var (
		err             error
		gotRelevantDate bool
	)

	t := fd.MDates
	w := fd.MWebData

	fieldMap := map[string]*string{ // Order by importance
		consts.JReleaseDate:         &t.ReleaseDate,
		consts.JOriginallyAvailable: &t.Originally_Available_At,
		consts.JDate:                &t.Date,
		consts.JUploadDate:          &t.UploadDate,
		consts.JReleaseYear:         &t.Year,
		consts.JYear:                &t.Year,
		consts.JCreationTime:        &t.Creation_Time,
	}

	if ok := unpackJSON("date", fieldMap, data); !ok {
		logging.PrintE(1, "Failed to unpack date JSON, no dates currently exist in file?")
	}

	printMap := make(map[string]string, len(fieldMap))

	for key, value := range data {
		if strVal, ok := value.(string); ok {
			if _, exists := fieldMap[key]; exists {

				if len(strVal) >= 6 {
					if formatted, ok := helpers.YyyyMmDd(strVal); ok {
						*fieldMap[key] = formatted
						printMap[key] = formatted
						gotRelevantDate = true
						continue

					} else {
						*fieldMap[key] = strVal
						printMap[key] = strVal
						gotRelevantDate = true
						continue
					}
				} else {
					*fieldMap[key] = strVal
					printMap[key] = strVal
					gotRelevantDate = true
					continue
				}
			}
		}
		continue
	}

	if fillEmptyTimestamps(t) {
		gotRelevantDate = true
	}

	switch {
	case gotRelevantDate:

		logging.PrintD(3, "Got a relevant date, proceeding...")
		print.PrintGrabbedFields("time and date", &printMap)
		if t.FormattedDate == "" {
			helpers.FormatAllDates(fd)
		} else {
			t.StringDate, err = helpers.ParseNumDate(t.FormattedDate)
			if err != nil {
				logging.PrintE(0, err.Error())
			}
		}

		rtn, err := fd.JSONFileRW.WriteMetadata(fieldMap)
		if err != nil {
			logging.PrintE(0, "Failed to write into JSON file '%s': %v", fd.JSONFilePath, err)
			return data, true
		} else if rtn != nil {
			data = rtn
			return data, true
		}

	case w.WebpageURL == "":

		logging.PrintI("Page URL not found in metadata, so cannot scrape for missing date in '%s'", fd.JSONFilePath)
		print.PrintGrabbedFields("time and date", &printMap)
		return data, false
	}

	scrapedDate := browser.ScrapeMeta(w, enums.WEBCLASS_DATE)
	logging.PrintD(1, "Scraped date: %s", scrapedDate)

	logging.PrintD(3, "Passed web scrape attempt for date.")

	var date string
	if scrapedDate != "" {
		date, err = helpers.ParseStringDate(scrapedDate)
		if err != nil || date == "" {
			logging.PrintE(0, "Failed to parse date '%s': %v", scrapedDate, err)
			return data, false
		} else {
			if t.ReleaseDate == "" {
				t.ReleaseDate = date
			}
			if t.Date == "" {
				t.Date = date
			}
			if t.Creation_Time == "" {
				t.Creation_Time = date + "T00:00:00Z"
			}
			if t.UploadDate == "" {
				t.UploadDate = date
			}
			if t.Originally_Available_At == "" {
				t.Originally_Available_At = date
			}
			if t.FormattedDate == "" {
				t.FormattedDate = date
			}
			if len(date) >= 4 {
				t.Year = date[:4]
			}

			printMap[consts.JReleaseDate] = t.ReleaseDate
			printMap[consts.JDate] = t.Date
			printMap[consts.JYear] = t.Year

			print.PrintGrabbedFields("time and date", &printMap)

			if t.FormattedDate == "" {
				helpers.FormatAllDates(fd)
			}
			rtn, err := fd.JSONFileRW.WriteMetadata(fieldMap)
			switch {
			case err != nil:
				logging.PrintE(0, "Failed to write new metadata (%s) into JSON file '%s': %v", date, fd.JSONFilePath, err)
				return data, true
			case rtn != nil:
				data = rtn
				return data, true
			}
		}
	}
	return data, false
}

// fillEmptyTimestamps attempts to infer missing timestamps
func fillEmptyTimestamps(t *models.MetadataDates) bool {

	gotRelevantDate := false

	// Infer from originally available date
	if t.Originally_Available_At != "" && len(t.Originally_Available_At) >= 6 {
		gotRelevantDate = true
		if t.Creation_Time == "" {
			if formatted, ok := helpers.YyyyMmDd(t.Originally_Available_At); ok {
				if !strings.ContainsRune(formatted, 'T') {
					t.Creation_Time = formatted + "T00:00:00Z"
					t.FormattedDate = formatted
				} else {
					t.Creation_Time = formatted
					t.FormattedDate, _, _ = strings.Cut(formatted, "T")
				}
			} else {
				if formatted, ok := helpers.YyyyMmDd(t.Originally_Available_At); ok {
					if !strings.ContainsRune(formatted, 'T') {
						t.Creation_Time = formatted + "T00:00:00Z"
						t.FormattedDate = formatted
					} else {
						t.Creation_Time = formatted
						t.FormattedDate, _, _ = strings.Cut(formatted, "T")
					}
				} else {
					t.Creation_Time = t.Originally_Available_At + "T00:00:00Z"
				}
			}
		}
	}
	// Infer from release date
	if t.ReleaseDate != "" && len(t.ReleaseDate) >= 6 {
		gotRelevantDate = true
		if t.Creation_Time == "" {
			if formatted, ok := helpers.YyyyMmDd(t.ReleaseDate); ok {
				t.Creation_Time = formatted + "T00:00:00Z"
				if t.FormattedDate == "" {
					t.FormattedDate = formatted
				}
			} else {
				t.Creation_Time = t.ReleaseDate + "T00:00:00Z"
			}
		}
		if t.Originally_Available_At == "" {
			if formatted, ok := helpers.YyyyMmDd(t.ReleaseDate); ok {
				t.Originally_Available_At = formatted
				if t.FormattedDate == "" {
					t.FormattedDate = formatted
				}
			} else {
				t.Originally_Available_At = t.ReleaseDate
			}
		}
	}
	// Infer from date
	if t.Date != "" && len(t.Date) >= 6 {
		gotRelevantDate = true
		if formatted, ok := helpers.YyyyMmDd(t.ReleaseDate); ok {
			t.Creation_Time = formatted + "T00:00:00Z"
			if t.FormattedDate == "" {
				t.FormattedDate = formatted
			}
		} else {
			t.Creation_Time = t.Date + "T00:00:00Z"
		}
		if t.Originally_Available_At == "" {
			if formatted, ok := helpers.YyyyMmDd(t.ReleaseDate); ok {
				t.Originally_Available_At = formatted
				if t.FormattedDate == "" {
					t.FormattedDate = formatted
				}
			} else {
				t.Originally_Available_At = t.Date
			}
		}
	}

	// Infer from upload date
	if t.UploadDate != "" && len(t.UploadDate) >= 6 {
		if formatted, ok := helpers.YyyyMmDd(t.UploadDate); ok {
			t.Creation_Time = formatted + "T00:00:00Z"
			if t.FormattedDate == "" {
				t.FormattedDate = formatted
			}
		} else {
			t.Creation_Time = t.UploadDate + "T00:00:00Z"
		}
		if t.Originally_Available_At == "" {
			t.Originally_Available_At = t.UploadDate
		}
	}
	// Fill empty date
	if t.Date == "" {
		switch {
		case t.ReleaseDate != "":
			t.Date = t.ReleaseDate
			t.Originally_Available_At = t.ReleaseDate

		case t.UploadDate != "":
			t.Date = t.UploadDate
			t.Originally_Available_At = t.UploadDate

		case t.FormattedDate != "":
			t.Date = t.FormattedDate
		}
	}
	// Fill empty year
	if t.Year == "" {
		switch {
		case t.Date != "" && len(t.Date) >= 4:
			t.Year = t.Date[:4]

		case t.UploadDate != "" && len(t.UploadDate) >= 4:
			t.Year = t.UploadDate[:4]

		case t.FormattedDate != "" && len(t.FormattedDate) >= 4:
			t.Year = t.FormattedDate[:4]
		}
	}
	if len(t.Year) > 4 {
		t.Year = t.Year[:4]
	}
	return gotRelevantDate
}
package metadata

import (
	"Metarr/internal/config"
	consts "Metarr/internal/domain/constants"
	enums "Metarr/internal/domain/enums"
	keys "Metarr/internal/domain/keys"
	"Metarr/internal/models"
	browser "Metarr/internal/utils/browser"
	logging "Metarr/internal/utils/logging"
	"strings"
)

// fillDescriptions grabs description data from JSON
func fillDescriptions(fd *models.FileData, data map[string]interface{}) (map[string]interface{}, bool) {

	d := fd.MTitleDesc
	w := fd.MWebData
	t := fd.MDates

	fieldMap := map[string]*string{ // Order by importance
		consts.JLongDescription:  &d.LongDescription,
		consts.JLong_Description: &d.Long_Description,
		consts.JDescription:      &d.Description,
		consts.JSynopsis:         &d.Synopsis,
		consts.JSummary:          &d.Summary,
		consts.JComment:          &d.Comment,
	}
	filled := unpackJSON("descriptions", fieldMap, data)

	datePfx := config.GetBool(keys.MDescDatePfx)
	dateSfx := config.GetBool(keys.MDescDateSfx)

	if (datePfx || dateSfx) && t.StringDate != "" {

		for _, value := range fieldMap {
			if value != nil {
				switch {
				case datePfx:
					if !strings.HasPrefix(*value, t.StringDate) {
						*value = t.StringDate + "\n\n" + *value // Prefix string date
					}
					continue
				case dateSfx:
					if !strings.HasSuffix(*value, t.StringDate) {
						*value = *value + "\n\n" + t.StringDate // Suffix string date
					}
					continue
				default:
					logging.PrintD(1, "Unknown issue appending date to description. Condition should be impossible? (reached: %s)", *value)
					continue
				}
			}
		}
	}

	// Attempt to fill empty description fields by inference
	for _, value := range fieldMap {
		if ok := fillEmptyDescriptions(value, d); ok {
			filled = true
		}
	}

	// Check if any values are present
	if !filled {
		for _, val := range fieldMap {
			if val != nil {
				if *val == "" {
					continue
				} else {
					filled = true
				}
			}
		}
	}

	switch {
	case filled:
		rtn, err := fd.JSONFileRW.WriteMetadata(fieldMap)
		switch {
		case err != nil:
			logging.PrintE(0, "Failed to write into JSON file '%s': %v", fd.JSONFilePath, err)
			return data, true
		case rtn != nil:
			data = rtn
			return data, true
		}

	case w.WebpageURL == "":
		logging.PrintI("Page URL not found in data, so cannot scrape for missing description in '%s'", fd.JSONFilePath)
		return data, false
	}

	description := browser.ScrapeMeta(w, enums.WEBCLASS_DESCRIPTION)

	// Infer remaining fields from description
	if description != "" {
		for _, value := range fieldMap {
			if *value == "" {
				*value = description
			}
		}

		// Insert new scraped fields into file
		rtn, err := fd.JSONFileRW.WriteMetadata(fieldMap)
		if err != nil {
			logging.PrintE(0, "Failed to insert new data (%s) into JSON file '%s': %v", description, fd.JSONFilePath, err)
		} else if rtn != nil {
			data = rtn
		}
		return data, true
	} else {
		return data, false
	}
}

// fillEmptyDescriptions fills empty description fields by inference
func fillEmptyDescriptions(want *string, d *models.MetadataTitlesDescs) bool {

	filled := false
	if want == nil {
		logging.PrintE(0, "Sent in string null, returning...")
		return false
	}
	if *want == "" {
		switch {
		case d.LongDescription != "":
			*want = d.LongDescription
			filled = true

		case d.Long_Description != "":
			*want = d.Long_Description
			filled = true

		case d.Description != "":
			*want = d.Description
			filled = true

		case d.Synopsis != "":
			*want = d.Synopsis
			filled = true

		case d.Summary != "":
			*want = d.Summary
			filled = true

		case d.Comment != "":
			*want = d.Comment
			filled = true
		}
	}
	return filled
}
package metadata

import (
	"Metarr/internal/models"
	logging "Metarr/internal/utils/logging"
	print "Metarr/internal/utils/print"
)

// Primary function to fill out meta fields before writing
func FillMetaFields(fd *models.FileData, data map[string]interface{}) (map[string]interface{}, bool) {

	var (
		ok   bool
		meta map[string]interface{}
	)
	allFilled := true

	if !FillWebpageDetails(fd, data) {
		logging.PrintI("No URL metadata found")
		allFilled = false
	}

	if !fillTitles(fd, data) {
		logging.PrintI("No title metadata found")
		allFilled = false
	}

	if meta, ok = fillCredits(fd, data); !ok {
		logging.PrintI("No credits metadata found")
		allFilled = false
	} else if meta != nil {
		data = meta
	}

	if meta, ok = fillTimestamps(fd, data); !ok {
		logging.PrintI("No date metadata found")
		allFilled = false
	} else {
		if meta != nil {
			data = meta
		}
	}

	if meta, ok = fillDescriptions(fd, data); !ok {
		logging.PrintI("No description metadata found")
		allFilled = false
	} else if meta != nil {
		data = meta
	}
	return data, allFilled
}

// unpackJSON decodes JSON for metafields
func unpackJSON(fieldType string, fieldMap map[string]*string, metadata map[string]interface{}) bool {

	dataFilled := false
	printMap := make(map[string]string, len(fieldMap))

	// Iterate through the decoded JSON to match fields against
	// the passed in map of fields to fill
	for key, value := range metadata {
		if strVal, ok := value.(string); ok {
			if field, exists := fieldMap[key]; exists && field != nil && *field == "" {

				*field = strVal
				dataFilled = true

				if printMap[key] == "" {
					printMap[key] = strVal
				}
			}
		}
	}
	print.PrintGrabbedFields(fieldType, &printMap)

	return dataFilled
}
package metadata

import (
	consts "Metarr/internal/domain/constants"
	enums "Metarr/internal/domain/enums"
	"Metarr/internal/models"
	browser "Metarr/internal/utils/browser"
	print "Metarr/internal/utils/print"
)

// fillTitles grabs the fulltitle ("title")
func fillTitles(fd *models.FileData, data map[string]interface{}) bool {

	t := fd.MTitleDesc
	w := fd.MWebData

	printMap := make(map[string]string, len(data))

	for key, value := range data {
		if val, ok := value.(string); ok && val != "" {
			switch {
			case key == consts.JTitle:
				t.Title = val
				printMap[key] = val

			case key == consts.JFallbackTitle:
				t.FallbackTitle = val
				printMap[key] = val

			case key == consts.JSubtitle:
				t.Subtitle = val
				printMap[key] = val
			}
		}
	}
	if t.Title == "" && t.FallbackTitle != "" {
		t.Title = t.FallbackTitle
	}
	if t.Title == "" {
		title := browser.ScrapeMeta(w, enums.WEBCLASS_TITLE)
		if title != "" {
			t.Title = title
		}
	}
	print.PrintGrabbedFields("title", &printMap)

	return t.Title != ""
}
package metadata

import (
	consts "Metarr/internal/domain/constants"
	"Metarr/internal/models"
	logging "Metarr/internal/utils/logging"
	print "Metarr/internal/utils/print"
)

// Grabs details necessary to scrape the web for missing metafields
func FillWebpageDetails(fd *models.FileData, data map[string]interface{}) bool {

	var isFilled bool

	w := fd.MWebData

	priorityMap := [5]string{consts.JWebpageURL,
		consts.JURL,
		consts.JReferer,
		consts.JWebpageDomain,
		consts.JDomain}

	printMap := make(map[string]string, len(priorityMap))

	for _, wanted := range priorityMap {
		for key, value := range data {

			if val, ok := value.(string); ok && val != "" {
				if key == wanted {
					switch {
					case key == consts.JWebpageURL:

						logging.PrintD(3, "Got URL: %s", val)

						if w.WebpageURL == "" {
							w.WebpageURL = val
						}
						printMap[key] = val
						w.TryURLs = append(w.TryURLs, val)

						isFilled = true

					case key == consts.JURL:

						logging.PrintD(3, "Got URL: %s", val)

						if w.VideoURL == "" {
							w.VideoURL = val
						}
						printMap[key] = val
						w.TryURLs = append(w.TryURLs, val)

						isFilled = true

					case key == consts.JReferer:

						logging.PrintD(3, "Got URL: %s", val)

						if w.Referer == "" {
							w.Referer = val
						}
						printMap[key] = val
						w.TryURLs = append(w.TryURLs, val)

						isFilled = true

					case key == consts.JWebpageDomain, key == consts.JDomain:

						logging.PrintD(3, "Got URL: %s", val)

						if w.Domain == "" {
							w.Domain = val
						}
						printMap[key] = val

						isFilled = true
					}
				}
			}
		}
	}

	logging.PrintD(2, "Stored URLs for scraping missing fields: %v", w.TryURLs)

	print.PrintGrabbedFields("web details", &printMap)

	return isFilled
}
package metadata

import (
	consts "Metarr/internal/domain/constants"
	"Metarr/internal/models"
	logging "Metarr/internal/utils/logging"
)

// fillNFODescriptions attempts to fill in title info from NFO
func fillNFOCredits(fd *models.FileData) bool {

	c := fd.MCredits
	n := fd.NFOData

	fieldMap := map[string]*string{
		consts.NActors:            &c.Actor,
		consts.NDirector:          &c.Director,
		consts.NProductionCompany: &c.Publisher,
		consts.NStudio:            &c.Studio,
		consts.NWriter:            &c.Writer,
		consts.NProducer:          &c.Producer,
	}

	// Post-unmarshal clean
	cleanEmptyFields(fieldMap)

	if n.Actors != nil {
		for _, actor := range n.Actors {
			c.Actors = append(c.Actors, actor.Name)
		}
		fillSingleCredits(c.Actors, &c.Actor)
	}
	if n.Directors != nil {
		c.Directors = append(c.Directors, n.Directors...)
		fillSingleCredits(c.Directors, &c.Director)
	}
	if n.Producers != nil {
		c.Producers = append(c.Producers, n.Producers...)
		fillSingleCredits(c.Producers, &c.Producer)
	}
	if n.Writers != nil {
		c.Writers = append(c.Writers, n.Writers...)
		fillSingleCredits(c.Writers, &c.Writer)
	}
	if n.Publishers != nil {
		c.Publishers = append(c.Publishers, n.Publishers...)
		fillSingleCredits(c.Publishers, &c.Publisher)
	}
	if n.Studios != nil {
		c.Studios = append(c.Studios, n.Studios...)
		fillSingleCredits(c.Studios, &c.Studio)
	}

	return true
}

// fillSingleCredits fills empty singular credits fields from
// filled arrays
func fillSingleCredits(entries []string, target *string) {

	if target == nil || *target != "" {
		logging.PrintD(1, "Target string is nil or not empty, skipping...")
	}

	var out string

	for i, entry := range entries {
		if entry != "" {
			out += entry
			if i != len(entries)-1 {
				out += ", "
			}
		}
	}

	*target = out
}

func unpackCredits(fd *models.FileData, creditsData map[string]interface{}) bool {
	c := fd.MCredits
	filled := false

	// Recursive helper to search for "role" within nested maps
	var findRoles func(data map[string]interface{})
	findRoles = func(data map[string]interface{}) {
		// Check each key-value pair within the actor data
		for k, v := range data {
			if k == "role" {
				if role, ok := v.(string); ok {
					logging.PrintD(3, "Adding role '%s' to actors", role)
					c.Actors = append(c.Actors, role)
					filled = true
				}
			} else if nested, ok := v.(map[string]interface{}); ok {
				// Recursive call for further nested maps
				findRoles(nested)
			} else if nestedList, ok := v.([]interface{}); ok {
				// Handle lists of nested elements
				for _, item := range nestedList {
					if nestedMap, ok := item.(map[string]interface{}); ok {
						findRoles(nestedMap)
					}
				}
			}
		}
	}

	// Access the "cast" data to find "actor" entries
	if castData, ok := creditsData["cast"].(map[string]interface{}); ok {
		if actorsData, ok := castData["actor"].([]interface{}); ok {
			for _, actorData := range actorsData {
				if actorMap, ok := actorData.(map[string]interface{}); ok {
					if name, ok := actorMap["name"].(string); ok {
						logging.PrintD(3, "Adding actor name '%s'", name)
						c.Actors = append(c.Actors, name)
						filled = true
					}
					if role, ok := actorMap["role"].(string); ok {
						logging.PrintD(3, "Adding actor role '%s'", role)
						filled = true
					}
				}
			}
		} else {
			logging.PrintD(1, "'actor' key is present but not a valid structure")
		}
	} else {
		logging.PrintD(1, "'cast' key is missing or not a map")
	}

	return filled
}
package metadata

import (
	consts "Metarr/internal/domain/constants"
	enums "Metarr/internal/domain/enums"
	helpers "Metarr/internal/metadata/process/helpers"
	"Metarr/internal/models"
	browser "Metarr/internal/utils/browser"
	logging "Metarr/internal/utils/logging"
	print "Metarr/internal/utils/print"
)

func fillNFOTimestamps(fd *models.FileData) bool {

	t := fd.MDates
	w := fd.MWebData
	n := fd.NFOData

	fieldMap := map[string]*string{
		consts.NAired:        &t.Date,
		consts.NPremiereDate: &t.ReleaseDate,
		consts.NYear:         &t.Year,
	}

	cleanEmptyFields(fieldMap)

	gotRelevantDate := false
	printMap := make(map[string]string, len(fieldMap))

	if n.Premiered != "" {
		if rtn, ok := helpers.YyyyMmDd(n.Premiered); ok && rtn != "" {
			if t.FormattedDate == "" {
				t.FormattedDate = rtn
			}
		}
		printMap[consts.NPremiereDate] = n.Premiered
		gotRelevantDate = true
	}
	if n.ReleaseDate != "" {
		if rtn, ok := helpers.YyyyMmDd(n.ReleaseDate); ok && rtn != "" {
			if t.FormattedDate == "" {
				t.FormattedDate = rtn
			}
		}
		printMap[consts.NAired] = n.Premiered
		gotRelevantDate = true
	}
	if n.Year != "" {
		t.Year = n.Year
		printMap[consts.NYear] = n.Year
	}

	if t.FormattedDate != "" {
		if t.Date == "" {
			t.Date = t.FormattedDate
		}
		if t.ReleaseDate == "" {
			t.ReleaseDate = t.FormattedDate
		}
		if t.Creation_Time == "" {
			t.Creation_Time = t.FormattedDate + "T00:00:00Z"
		}
		gotRelevantDate = true
	}

	switch {
	case gotRelevantDate:

		var err error

		logging.PrintD(3, "Got a relevant date, proceeding...")
		print.PrintGrabbedFields("time and date", &printMap)
		if t.FormattedDate == "" {
			helpers.FormatAllDates(fd)
		} else {
			t.StringDate, err = helpers.ParseNumDate(t.FormattedDate)
			if err != nil {
				logging.PrintE(0, err.Error())
			}
		}

	case w.WebpageURL == "":

		logging.PrintI("Page URL not found in metadata, so cannot scrape for missing date in '%s'", fd.JSONFilePath)
		print.PrintGrabbedFields("time and date", &printMap)
		return false
	}

	scrapedDate := browser.ScrapeMeta(w, enums.WEBCLASS_DATE)
	logging.PrintD(1, "Scraped date: %s", scrapedDate)

	logging.PrintD(3, "Passed web scrape attempt for date.")

	var (
		date string
		err  error
	)
	if scrapedDate != "" {
		date, err = helpers.ParseStringDate(scrapedDate)
		if err != nil || date == "" {
			logging.PrintE(0, "Failed to parse date '%s': %v", scrapedDate, err)
			return false
		} else {
			if t.ReleaseDate == "" {
				t.ReleaseDate = date
			}
			if t.Date == "" {
				t.Date = date
			}
			if t.Creation_Time == "" {
				t.Creation_Time = date + "T00:00:00Z"
			}
			if t.UploadDate == "" {
				t.UploadDate = date
			}
			if t.Originally_Available_At == "" {
				t.Originally_Available_At = date
			}
			if t.FormattedDate == "" {
				t.FormattedDate = date
			}
			if len(date) >= 4 {
				t.Year = date[:4]
			}

			printMap[consts.NPremiereDate] = t.ReleaseDate
			printMap[consts.NAired] = t.Date
			printMap[consts.NYear] = t.Year

			print.PrintGrabbedFields("time and date", &printMap)

			if t.FormattedDate == "" {
				helpers.FormatAllDates(fd)
			}
		}
	}

	return true
}
package metadata

import (
	consts "Metarr/internal/domain/constants"
	"Metarr/internal/models"
	print "Metarr/internal/utils/print"
)

// fillNFODescriptions attempts to fill in title info from NFO
func fillNFODescriptions(fd *models.FileData) bool {

	d := fd.MTitleDesc
	n := fd.NFOData

	fieldMap := map[string]*string{
		consts.NDescription: &d.Description,
		consts.NPlot:        &d.LongDescription,
	}

	// Post-unmarshal clean
	cleanEmptyFields(fieldMap)

	if n.Description != "" {
		if d.Description == "" {
			d.Description = n.Description
		}
		if d.LongDescription == "" {
			d.LongDescription = n.Description
		}
	}
	if n.Plot != "" {
		if d.Description == "" {
			d.Description = n.Plot
		}
		if d.LongDescription == "" {
			d.LongDescription = n.Plot
		}
	}

	if d.Synopsis == "" {
		switch {
		case n.Plot != "":
			d.Synopsis = n.Plot
		case n.Description != "":
			d.Summary = n.Description
		case d.LongDescription != "":
			d.Synopsis = d.LongDescription
		case d.Description != "":
			d.Synopsis = d.Description
		}
	}
	if d.Summary == "" {
		switch {
		case n.Plot != "":
			d.Summary = n.Plot
		case n.Description != "":
			d.Summary = n.Description
		case d.LongDescription != "":
			d.Summary = d.LongDescription
		case d.Description != "":
			d.Summary = d.Description
		}
	}
	if d.Comment == "" {
		switch {
		case n.Plot != "":
			d.Comment = n.Plot
		case n.Description != "":
			d.Comment = n.Description
		case d.LongDescription != "":
			d.Comment = d.LongDescription
		case d.Description != "":
			d.Comment = d.Description
		}
	}

	print.CreateModelPrintout(fd, fd.NFOFilePath, "Parsing NFO descriptions")
	return true
}
package metadata

import (
	"Metarr/internal/models"
	logging "Metarr/internal/utils/logging"
	print "Metarr/internal/utils/print"
	"strings"
)

// FillNFO is the primary entrypoint for filling NFO metadata
// from an open file's read content
func FillNFO(fd *models.FileData) bool {

	var filled bool

	if ok := fillNFOTimestamps(fd); ok {
		filled = true
	}

	if ok := fillNFOTitles(fd); ok {
		filled = true
	}

	if ok := fillNFODescriptions(fd); ok {
		filled = true
	}

	if ok := fillNFOCredits(fd); ok {
		filled = true
	}

	if ok := fillNFOWebData(fd); ok {
		filled = true
	}

	print.CreateModelPrintout(fd, fd.NFOBaseName, "Fill metadata from NFO for file '%s'", fd.NFOFilePath)

	return filled
}

// Clean up empty fields from fieldmap
func cleanEmptyFields(fieldMap map[string]*string) {
	for _, value := range fieldMap {
		if strings.TrimSpace(*value) == "" {
			*value = ""
		}
	}
}

// nestedLoop parses content recursively and returns a nested map
func nestedLoop(content string) map[string]interface{} {
	nested := make(map[string]interface{})

	logging.PrintD(2, "Parsing content in nestedLoop: %s", content)

	for len(content) > 0 {
		if strings.HasPrefix(content, "<?xml") || strings.HasPrefix(content, "<?") {
			endIdx := strings.Index(content, "?>")
			if endIdx == -1 {
				logging.PrintE(0, "Malformed XML declaration in content: %s", content)
				break
			}
			content = content[endIdx+2:]
			logging.PrintD(2, "Skipping XML declaration, remaining content: %s", content)
			continue
		}

		// Find the opening tag
		openIdx := strings.Index(content, "<")
		if openIdx == -1 {
			break // No more tags
		}

		openIdxClose := strings.Index(content, ">")
		if openIdxClose == -1 {
			logging.PrintE(0, "No valid tag close bracket for entry beginning %s", content[openIdx:])
			break // No closing tag bracket
		}

		// Get the tag name and check if it is self-closing
		tag := content[openIdx+1 : openIdxClose]
		isSelfClosing := strings.HasSuffix(tag, "/")
		tag = strings.TrimSuffix(tag, "/") // Remove trailing / if present

		if isSelfClosing {
			// Self-closing tag; skip over and move to the next
			content = content[openIdxClose+1:]
			logging.PrintD(2, "Skipping self-closing tag: %s", tag)
			continue
		}

		// Look for the corresponding closing tag
		closeTag := "</" + tag + ">"
		closeIdx := strings.Index(content, closeTag)
		if closeIdx == -1 {
			// No closing tag; skip this tag and continue
			content = content[openIdxClose+1:]
			logging.PrintD(2, "Skipping tag without end tag: %s", tag)
			continue
		}

		// Extract the inner content between tags
		innerContent := content[openIdxClose+1 : closeIdx]
		logging.PrintD(2, "Found inner content for tag '%s': %s", tag, innerContent)

		// Recursive call if innerContent contains nested tags
		if strings.Contains(innerContent, "<") && strings.Contains(innerContent, ">") {
			logging.PrintD(2, "Recursively parsing nested content for tag '%s'", tag)
			nested[tag] = nestedLoop(innerContent)
		} else {
			logging.PrintD(2, "Assigning inner content to tag '%s': %s", tag, innerContent)
			nested[tag] = innerContent
		}

		// Move past the processed tag
		content = content[closeIdx+len(closeTag):]
		logging.PrintD(2, "Remaining content after parsing tag '%s': %s", tag, content)
	}

	logging.PrintD(2, "Final parsed structure from nestedLoop: %v", nested)
	return nested
}

// unpackNFO unpacks an NFO map back to the model
func unpackNFO(fd *models.FileData, data map[string]interface{}, fieldMap map[string]*string) {
	logging.PrintD(3, "Unpacking NFO map...")

	// Access the top-level "movie" key
	movieData, ok := data["movie"].(map[string]interface{})
	if !ok {
		logging.PrintE(0, "Missing 'movie' key in data, unable to unpack")
		return
	}

	for field, fieldVal := range fieldMap {
		if fieldVal == nil {
			logging.PrintE(0, "Field value is null, continuing...")
			continue
		}

		// Look for the field in the movie data
		val, exists := movieData[field]
		if !exists {
			continue // Field does not exist in this map
		}

		switch v := val.(type) {
		case string:
			logging.PrintD(3, "Setting field '%s' to '%s'", field, v)
			*fieldVal = v
		case map[string]interface{}:
			switch field {

			case "title":
				logging.PrintD(3, "Unpacking nested 'title' map...")
				unpackTitle(fd, v)
			case "cast":
				logging.PrintD(3, "Unpacking nested 'cast' map...")
				unpackCredits(fd, v)
			}
		default:
			logging.PrintD(1, "Unknown field type for '%s', skipping...", field)
		}
	}
}
package metadata

import (
	consts "Metarr/internal/domain/constants"
	"Metarr/internal/models"
	logging "Metarr/internal/utils/logging"
)

// fillNFOTitles attempts to fill in title info from NFO
func fillNFOTitles(fd *models.FileData) bool {

	t := fd.MTitleDesc
	n := fd.NFOData

	fieldMap := map[string]*string{
		consts.NTitle:         &t.Title,
		consts.NOriginalTitle: &t.FallbackTitle,
		consts.NTagline:       &t.Subtitle,
	}

	// Post-unmarshal clean
	cleanEmptyFields(fieldMap)

	logging.PrintI("Grab NFO metadata: %v", t)

	if n.Title.Main != "" {
		if t.Title == "" {
			t.Title = n.Title.Main
		}
	}
	if n.Title.Original != "" {
		if t.FallbackTitle == "" {
			t.FallbackTitle = n.Title.Original
		}
		if t.Title == "" {
			t.Title = n.Title.Original
		}
	}
	if n.Title.Sub != "" {
		if t.Subtitle == "" {
			t.Subtitle = n.Title.Sub
		}
	}
	if n.Title.PlainText != "" {
		if t.Title == "" {
			t.Title = n.Title.PlainText
		}
	}
	return true
}

// unpackTitle unpacks common nested title elements to the model
func unpackTitle(fd *models.FileData, titleData map[string]interface{}) bool {
	t := fd.MTitleDesc
	filled := false

	for key, value := range titleData {
		switch key {
		case "main":
			if strVal, ok := value.(string); ok {
				logging.PrintD(3, "Setting main title to '%s'", strVal)
				t.Title = strVal
				filled = true
			}
		case "sub":
			if strVal, ok := value.(string); ok {
				logging.PrintD(3, "Setting subtitle to '%s'", strVal)
				t.Subtitle = strVal
				filled = true
			}
		default:
			logging.PrintD(1, "Unknown nested title element '%s', skipping...", key)
		}
	}
	return filled
}
package metadata

import (
	consts "Metarr/internal/domain/constants"
	"Metarr/internal/models"
	print "Metarr/internal/utils/print"
)

// fillNFODescriptions attempts to fill in title info from NFO
func fillNFOWebData(fd *models.FileData) bool {

	w := fd.MWebData
	nw := fd.NFOData.WebpageInfo

	fieldMap := map[string]*string{
		consts.NURL: &w.WebpageURL,
	}

	// Post-unmarshal clean
	cleanEmptyFields(fieldMap)

	if nw.URL != "" {
		if w.WebpageURL == "" {
			w.WebpageURL = nw.URL
		}
	}

	print.CreateModelPrintout(fd, fd.NFOFilePath, "Parsing NFO descriptions")
	return true
}
package metadata

import (
	"Metarr/internal/config"
	enums "Metarr/internal/domain/enums"
	keys "Metarr/internal/domain/keys"
	helpers "Metarr/internal/metadata/process/helpers"
	process "Metarr/internal/metadata/process/json"
	tags "Metarr/internal/metadata/tags"
	writer "Metarr/internal/metadata/writer"
	"Metarr/internal/models"
	"Metarr/internal/transformations"
	logging "Metarr/internal/utils/logging"
	"fmt"
	"os"
	"sync"
)

var (
	mu sync.Mutex
)

// ProcessJSONFile reads a single JSON file and fills in the metadata
func ProcessJSONFile(fd *models.FileData) (*models.FileData, error) {
	if fd == nil {
		return nil, fmt.Errorf("model passed in null")
	}

	logging.PrintD(2, "Beginning JSON file processing...")

	// Function mutex
	mu.Lock()
	defer mu.Unlock()

	filePath := fd.JSONFilePath

	// Open the file
	file, err := os.OpenFile(filePath, os.O_RDWR, 0644)
	if err != nil {
		logging.ErrorArray = append(logging.ErrorArray, err)
		return nil, fmt.Errorf("failed to open file: %w", err)
	}
	defer file.Close()

	// Grab and store metadata reader/writer
	jsonRW := writer.NewJSONFileRW(file)
	if jsonRW != nil {
		fd.JSONFileRW = jsonRW
	}

	data, err := fd.JSONFileRW.DecodeMetadata(file)
	if err != nil {
		return nil, err
	}

	logging.PrintD(3, "%v", data)

	process.FillWebpageDetails(fd, data)
	logging.PrintI("URLs grabbed: %s", fd.MWebData.TryURLs)

	transformations.TryTransPresets(fd.MWebData.TryURLs)

	// Make metadata adjustments per user selection
	edited, err := fd.JSONFileRW.MakeMetaEdits(data, file, fd.MWebData)
	if err != nil {
		return nil, err
	}
	if edited {
		logging.PrintD(2, "Refreshing JSON metadata after edits were made...")
		data, err = fd.JSONFileRW.RefreshMetadata()
		if err != nil {
			return nil, err
		}
	}

	var ok bool
	if data, ok = process.FillMetaFields(fd, data); !ok {
		logging.PrintD(2, "Some metafields were unfilled")
	}

	if fd.MDates.FormattedDate == "" {
		helpers.FormatAllDates(fd)
	}

	// Make date tag
	logging.PrintD(3, "About to make date tag for: %v", file.Name())
	if config.Get(keys.FileDateFmt).(enums.FilenameDateFormat) != enums.FILEDATE_SKIP {
		fd.FilenameDateTag, err = tags.MakeDateTag(data, file.Name())
		if err != nil {
			logging.PrintE(0, "Failed to make date tag: %v", err)
		}
	}

	// Add new filename tag for files
	if config.IsSet(keys.MFilenamePfx) {
		logging.PrintD(3, "About to make prefix tag for: %v", file.Name())
		fd.FilenameMetaPrefix = tags.MakeFilenameTag(data, file)
	}

	return fd, nil
}
package metadata

import (
	nfo "Metarr/internal/metadata/process/nfo"
	writer "Metarr/internal/metadata/writer"
	"Metarr/internal/models"
	logging "Metarr/internal/utils/logging"
	"fmt"
	"os"
)

// ProcessNFOFiles processes NFO files and sends data into the metadata model
func ProcessNFOFiles(fd *models.FileData) (*models.FileData, error) {
	if fd == nil {
		return nil, fmt.Errorf("model passed in null")
	}

	logging.PrintD(2, "Beginning NFO file processing...")

	// Open the file
	file, err := os.OpenFile(fd.NFOFilePath, os.O_RDWR, 0644)
	if err != nil {
		logging.ErrorArray = append(logging.ErrorArray, err)
		return nil, fmt.Errorf("failed to open file: %w", err)
	}
	defer file.Close()

	nfoRW := writer.NewNFOFileRW(file)
	if nfoRW != nil {
		// Store NFO RW in model
		fd.NFOFileRW = nfoRW
	}

	data, err := nfoRW.DecodeMetadata(file)
	if err != nil || data == nil {
		logging.PrintE(0, "Failed to decode metadata from file: %v", err)
	} else {
		// Store NFO data in model
		fd.NFOData = data
	}

	edited, err := nfoRW.MakeMetaEdits(nfoRW.Meta, file, fd.MWebData)
	if err != nil {
		logging.PrintE(0, "Encountered issue making meta edits: %v", err)
	}
	if edited {
		logging.PrintD(2, "Refreshing NFO metadata after edits were made...")
		data, err := fd.NFOFileRW.RefreshMetadata()
		if err != nil {
			return nil, err
		} else {
			fd.NFOData = data
		}
	}

	// Fill to file metadata
	if ok := nfo.FillNFO(fd); !ok {
		logging.PrintE(0, "No metadata filled from NFO file...")
	}
	return fd, nil
}
package metadata

import (
	"Metarr/internal/config"
	consts "Metarr/internal/domain/constants"
	enums "Metarr/internal/domain/enums"
	keys "Metarr/internal/domain/keys"
	logging "Metarr/internal/utils/logging"
	"fmt"
	"path/filepath"
	"strconv"
	"strings"
)

// MakeDateTag attempts to create the date tag for files using metafile data
func MakeDateTag(metadata map[string]interface{}, fileName string) (string, error) {
	dateFmt, ok := config.Get(keys.FileDateFmt).(enums.FilenameDateFormat)
	if !ok {
		return "", fmt.Errorf("invalid date format configuration")
	}

	date, found := extractDateFromMetadata(metadata)
	if !found {
		logging.PrintE(0, "No dates found in JSON file")
		return "", nil
	}

	year, month, day, err := parseDateComponents(date, dateFmt)
	if err != nil {
		return "", fmt.Errorf("failed to parse date components: %w", err)
	}

	dateStr, err := formatDateString(year, month, day, dateFmt)
	if dateStr == "" || err != nil {
		logging.PrintE(0, "Failed to create date string")
		return "[]", nil
	}

	dateTag := "[" + dateStr + "]"
	if checkTagExists(dateTag, filepath.Base(fileName)) {
		logging.PrintD(2, "Tag '%s' already detected in name, skipping...", dateTag)
		return "[]", nil
	}

	logging.PrintS(0, "Made date tag '%s' from file '%v'", dateTag, filepath.Base(fileName))
	return dateTag, nil
}

// extractDateFromMetadata attempts to find a date in the metadata using predefined fields
func extractDateFromMetadata(metadata map[string]interface{}) (string, bool) {
	preferredDateFields := []string{
		consts.JReleaseDate,
		"releasedate",
		"released_on",
		consts.JOriginallyAvailable,
		"originally_available",
		"originallyavailable",
		consts.JDate,
		consts.JUploadDate,
		"uploaddate",
		"uploaded_on",
		consts.JCreationTime, // Last resort, may give false positives
		"created_at",
	}

	for _, field := range preferredDateFields {
		if value, found := metadata[field]; found {
			if strVal, ok := value.(string); ok && strVal != "" && len(strVal) > 4 {
				if date, _, found := strings.Cut(strVal, "T"); found {
					return date, true
				}
				return strVal, true
			}
		}
	}
	return "", false
}

// parseDateComponents extracts and validates year, month, and day from the date string
func parseDateComponents(date string, dateFmt enums.FilenameDateFormat) (year, month, day string, err error) {
	date = strings.ReplaceAll(date, "-", "")
	date = strings.TrimSpace(date)

	year, month, day, err = getYearMonthDay(date, dateFmt)
	if err != nil {
		return "", "", "", err
	}

	return validateDateComponents(year, month, day)
}

// formatDateString formats the date as a hyphenated string
func formatDateString(year, month, day string, dateFmt enums.FilenameDateFormat) (string, error) {
	var parts [3]string

	switch dateFmt {
	case enums.FILEDATE_YYYY_MM_DD, enums.FILEDATE_YY_MM_DD:
		parts = [3]string{year, month, day}
	case enums.FILEDATE_YYYY_DD_MM, enums.FILEDATE_YY_DD_MM:
		parts = [3]string{year, day, month}
	case enums.FILEDATE_DD_MM_YYYY, enums.FILEDATE_DD_MM_YY:
		parts = [3]string{day, month, year}
	case enums.FILEDATE_MM_DD_YYYY, enums.FILEDATE_MM_DD_YY:
		parts = [3]string{month, day, year}
	}

	result := joinNonEmpty(parts)
	if result == "" {
		return "", fmt.Errorf("no valid date components found")
	}
	return result, nil
}

// joinNonEmpty joins non-empty strings from an array with hyphens
func joinNonEmpty(parts [3]string) string {
	nonEmpty := make([]string, 0, len(parts))
	for _, p := range parts {
		if p != "" {
			nonEmpty = append(nonEmpty, p)
		}
	}
	if len(nonEmpty) == 0 {
		return ""
	}
	return strings.Join(nonEmpty, "-")
}

// getYear returns the year digits from the date string
func getYearMonthDay(d string, dateFmt enums.FilenameDateFormat) (year, month, day string, err error) {
	d = strings.ReplaceAll(d, "-", "")
	d = strings.TrimSpace(d)

	if len(d) >= 8 {
		switch dateFmt {
		case enums.FILEDATE_DD_MM_YY, enums.FILEDATE_MM_DD_YY, enums.FILEDATE_YY_DD_MM, enums.FILEDATE_YY_MM_DD:
			year = d[2:4]
		default:
			year = d[:4]
		}
		month = d[4:6]
		day = d[6:8]

		return year, month, day, nil
	}
	if len(d) >= 6 {
		year = d[:2]
		month = d[2:4]
		day = d[4:6]

		return year, month, day, nil
	}
	if len(d) == 4 { // Guess year or month-day

		i, err := strconv.Atoi(d[:2])
		if err != nil {
			return "", "", "", fmt.Errorf("invalid date string '%s' threw error: %w", d, err)
		}
		j, err := strconv.Atoi(d[2:4])
		if err != nil {
			return "", "", "", fmt.Errorf("invalid date string '%s' threw error: %w", d, err)
		}

		if (i == 20 || i == 19) && j > 12 { // First guess year
			logging.PrintI("Guessing date string '%s' as year", d)
			switch dateFmt {
			case enums.FILEDATE_DD_MM_YY, enums.FILEDATE_MM_DD_YY, enums.FILEDATE_YY_DD_MM, enums.FILEDATE_YY_MM_DD:
				return d[2:4], "", "", nil
			default:
				return d[:4], "", "", nil
			}
		} else { // Second guess, month-date
			if ddmm, mmdd := maybeDayMonth(i, j); ddmm || mmdd {
				if ddmm {
					logging.PrintI("Guessing date string '%s' as day-month")
					day = d[:2]
					month = d[2:4]

				} else if mmdd {
					logging.PrintI("Guessing date string '%s' as month-day")
					day = d[2:4]
					month = d[:2]
				}
				return "", month, day, nil
			} else if i == 20 || i == 19 { // Final guess year
				logging.PrintI("Guessing date string '%s' as year after failed day-month check", d)
				switch dateFmt {
				case enums.FILEDATE_DD_MM_YY, enums.FILEDATE_MM_DD_YY, enums.FILEDATE_YY_DD_MM, enums.FILEDATE_YY_MM_DD:
					return d[2:4], "", "", nil
				default:
					return d[:4], "", "", nil
				}
			}
		}
	}

	return "", "", "", fmt.Errorf("failed to parse year, month, and day from '%s'", d)
}

// validateDateComponents attempts to fix faulty date arrangements
func validateDateComponents(year, month, day string) (string, string, string, error) {

	if isValidMonth(month) && isValidDay(day, month, year) {
		return year, month, day, nil
	}

	// Attempt swapping day and month
	if isValidMonth(day) && isValidDay(month, day, year) {
		return year, day, month, nil
	}

	// Fail check:
	return "", "", "", fmt.Errorf("invalid date components: year=%s, month=%s, day=%s", year, month, day)
}

// isValidMonth checks if the month inputted is a valid month
func isValidMonth(month string) bool {
	m, err := strconv.Atoi(month)
	if err != nil {
		return false
	}
	return m >= 1 && m <= 12
}

// isValidDay checks if the day inputted is a valid day
func isValidDay(day, month, year string) bool {
	d, err := strconv.Atoi(day)
	if err != nil {
		return false
	}

	m, err := strconv.Atoi(month)
	if err != nil {
		return false
	}

	y, err := strconv.Atoi(year)
	if err != nil {
		return false
	}

	if d < 1 || d > 31 {
		return false
	}

	// Months with 30 days
	if m == 4 || m == 6 || m == 9 || m == 11 {
		return d <= 30
	}

	// February
	if m == 2 {
		// Leap year check
		isLeap := y%4 == 0 && (y%100 != 0 || y%400 == 0)
		if isLeap {
			return d <= 29
		}
		return d <= 28
	}

	return true
}

// maybeDayMonth guesses if the input is a DD-MM or MM-DD format
func maybeDayMonth(i, j int) (ddmm, mmdd bool) {
	if i == 0 || i >= 31 || j == 0 || j >= 31 {
		return false, false
	}

	switch {
	case i <= 31 && j <= 12:
		return ddmm, false
	case j <= 31 && i <= 12:
		return false, mmdd
	default:
		return false, false
	}
}
package metadata

import (
	"Metarr/internal/config"
	keys "Metarr/internal/domain/keys"
	logging "Metarr/internal/utils/logging"
	"fmt"
	"os"
	"path/filepath"
	"regexp"
	"strings"
)

// makeFilenameTag creates the metatag string to prefix filenames with
func MakeFilenameTag(metadata map[string]interface{}, file *os.File) string {
	logging.PrintD(3, "Entering makeFilenameTag with data@ %v", metadata)

	tagArray := config.GetStringSlice(keys.MFilenamePfx)
	tag := "["

	for field, value := range metadata {
		for i, data := range tagArray {

			if field == data {
				tag += fmt.Sprintf(value.(string))
				logging.PrintD(3, "Added metafield %v data %v to prefix tag (Tag so far: %s)", field, data, tag)

				if i != len(tagArray)-1 {
					tag += "_"
				}
			}
		}
	}
	tag += "]"
	tag = strings.TrimSpace(tag)
	tag = strings.ToValidUTF8(tag, "")

	invalidChars := regexp.MustCompile(`[<>:"/\\|?*\x00-\x1F]`)
	tag = invalidChars.ReplaceAllString(tag, "")

	logging.PrintD(1, "Made metatag '%s' from file '%s'", tag, file.Name())

	if tag != "[]" {
		if checkTagExists(tag, filepath.Base(file.Name())) {
			logging.PrintD(2, "Tag '%s' already detected in name, skipping...", tag)
			tag = "[]"
		}
	}
	return tag
}

// checkTagExists checks if the constructed tag already exists in the filename
func checkTagExists(tag, filename string) bool {
	logging.PrintD(3, "Checking if tag '%s' exists in filename '%s'", tag, filename)

	return strings.Contains(filename, tag)
}
package metadata

import (
	"Metarr/internal/config"
	consts "Metarr/internal/domain/constants"
	enums "Metarr/internal/domain/enums"
	keys "Metarr/internal/domain/keys"
	"Metarr/internal/models"
	backup "Metarr/internal/utils/fs/backup"
	logging "Metarr/internal/utils/logging"
	"fmt"
	"os"
	"os/exec"
	"path/filepath"
	"strings"
	"sync"
)

var (
	muWriteCommand sync.Mutex
)

// CommandBuilder handles FFmpeg command construction
type CommandBuilder struct {
	inputFile   string
	outputFile  string
	formatFlags []string
	gpuAccel    []string
	metadataMap map[string]string
}

// NewCommandBuilder creates a new FFmpeg command builder
func NewCommandBuilder(m *models.FileData, outputFile string) *CommandBuilder {
	return &CommandBuilder{
		inputFile:   m.OriginalVideoPath,
		outputFile:  outputFile,
		metadataMap: make(map[string]string),
	}
}

// buildCommand constructs the complete FFmpeg command
func buildCommand(m *models.FileData, outputFile string) ([]string, error) {

	builder := NewCommandBuilder(m, outputFile)

	builder.setGPUAcceleration()
	builder.addAllMetadata(m)
	builder.setFormatFlags()

	// Return the fully appended argument string
	return builder.buildFinalCommand()
}

// WriteMetadata writes metadata to a single video file
func WriteMetadata(m *models.FileData) error {

	var tempOutputFilePath string

	originalVPath := m.OriginalVideoPath
	dir := m.VideoDirectory
	originalExt := filepath.Ext(originalVPath)
	outputExt := config.GetString(keys.OutputFiletype)

	fmt.Printf("\nWriting metadata for file: %s\n", originalVPath)
	// Make temp output path with .mp4 extension
	fileBase := strings.TrimSuffix(filepath.Base(originalVPath), filepath.Ext(originalVPath))

	if outputExt == "" {
		outputExt = filepath.Ext(m.FinalVideoPath)
		config.Set(keys.OutputFiletype, outputExt)
	}

	switch {
	case outputExt != "":
		tempOutputFilePath = filepath.Join(dir, consts.TempTag+fileBase+originalExt+outputExt)
	default:
		tempOutputFilePath = filepath.Join(dir, consts.TempTag+fileBase+originalExt+originalExt)
	}

	m.TempOutputFilePath = tempOutputFilePath // Add to video file data struct

	defer func() {
		if _, err := os.Stat(tempOutputFilePath); err == nil {
			os.Remove(tempOutputFilePath)
		}
	}()

	muWriteCommand.Lock()
	args, err := buildCommand(m, tempOutputFilePath)
	if err != nil {
		muWriteCommand.Unlock()
		return err
	}

	command := exec.Command("ffmpeg", args...)
	muWriteCommand.Unlock()

	logging.PrintI("%sConstructed FFmpeg command for%s '%s':\n\n%v\n", consts.ColorCyan, consts.ColorReset, m.OriginalVideoPath, command.String())

	command.Stdout = os.Stdout
	command.Stderr = os.Stderr

	origPath := originalVPath
	m.FinalVideoBaseName = strings.TrimSuffix(filepath.Base(origPath), filepath.Ext(origPath))

	switch {
	case outputExt != "":
		m.FinalVideoPath = filepath.Join(m.VideoDirectory, m.FinalVideoBaseName) + outputExt
	default:
		m.FinalVideoPath = filepath.Join(m.VideoDirectory, m.FinalVideoBaseName) + originalExt
	}

	fmt.Printf("\n\nVideo file path data:\n\nOriginal Video Path: %s\nMetadata File Path: %s\nFinal Video Path: %s\n\nTemp Output Path: %s\n\n", originalVPath,
		m.JSONFilePath,
		m.FinalVideoPath,
		m.TempOutputFilePath)

	// Run the ffmpeg command
	logging.Print("%s!!! Starting FFmpeg command for '%s'...\n%s", consts.ColorCyan, m.FinalVideoBaseName, consts.ColorReset)
	if err := command.Run(); err != nil {
		logging.ErrorArray = append(logging.ErrorArray, err)
		return fmt.Errorf("failed to run ffmpeg command: %w", err)
	}

	// Rename temporary file to overwrite the original video file:
	// First check overwrite rules
	if config.GetBool(keys.NoFileOverwrite) && originalVPath == m.FinalVideoPath {
		if err := backup.RenameToBackup(originalVPath); err != nil {
			return fmt.Errorf("failed to rename original file and preserve file is on, aborting: %w", err)
		}
	}
	err = os.Rename(tempOutputFilePath, m.FinalVideoPath)
	if err != nil {
		return fmt.Errorf("failed to overwrite original file: %w", err)
	}

	fmt.Printf("Successfully renamed video from %s to %s\n", tempOutputFilePath, m.FinalVideoPath)

	if filepath.Ext(originalVPath) != filepath.Ext(m.FinalVideoPath) {

		logging.PrintI("Original file not type %s, removing '%s'", outputExt, originalVPath)

		if config.GetBool(keys.NoFileOverwrite) {
			if _, err := os.Stat(originalVPath); os.IsNotExist(err) {
				logging.PrintI("File does not exist, safe to proceed overwriting: %s", originalVPath)
			} else {
				if err := backup.RenameToBackup(originalVPath); err != nil {
					return fmt.Errorf("failed to rename original file and preserve file is on, aborting: %w", err)
				}
			}
			err = os.Remove(originalVPath)
			if err != nil {
				logging.ErrorArray = append(logging.ErrorArray, err)
				return fmt.Errorf("failed to remove original file (%s). Error: %v", originalVPath, err)
			}
		}
	}

	logging.PrintS(0, "Successfully processed video:\n\nOriginal file: %s\nNew file: %s\n\nTitle: %s", originalVPath,
		m.FinalVideoPath,
		m.MTitleDesc.Title)

	return nil
}

// setGPUAcceleration sets appropriate GPU acceleration flags
func (b *CommandBuilder) setGPUAcceleration() {
	gpuFlag, ok := config.Get(keys.GPUEnum).(enums.SysGPU)
	if ok {
		switch gpuFlag {
		case enums.GPU_NVIDIA:
			b.gpuAccel = consts.NvidiaAccel[:]
		case enums.GPU_AMD:
			b.gpuAccel = consts.AMDAccel[:]
		case enums.GPU_INTEL:
			b.gpuAccel = consts.IntelAccel[:]
		}
	}
}

// addAllMetadata combines all metadata into a single map
func (b *CommandBuilder) addAllMetadata(m *models.FileData) {

	b.addTitlesDescs(m.MTitleDesc)
	b.addCredits(m.MCredits)
	b.addDates(m.MDates)
	b.addShowInfo(m.MShowData)
	b.addOtherMetadata(m.MOther)
}

// setFormatFlags adds commands specific for the extension input and output
func (b *CommandBuilder) setFormatFlags() {

	inExt := filepath.Ext(b.inputFile)
	outExt := config.GetString(keys.OutputFiletype)

	if outExt == "" {
		outExt = inExt
	}

	logging.PrintI("Input extension set '%s' and output extension '%s'. File: %s", inExt, outExt, b.inputFile)

	// Return early with straight copy if no extension change
	if strings.TrimPrefix(inExt, ".") == strings.TrimPrefix(outExt, ".") {
		b.formatFlags = consts.AVCodecCopy[:]
		return
	}

	flags := make([]string, 0)

	// Set flags based on output format requirements
	switch outExt {
	case ".mp4":
		flags = append(flags, "-f", outExt)
		flags = append(flags, consts.VideoToH264Balanced[:]...)
		flags = append(flags, consts.PixelFmtYuv420p[:]...)
		flags = append(flags, consts.AudioToAAC[:]...)
		flags = append(flags, consts.AudioBitrate[:]...)

	case ".mkv":
		flags = append(flags, "-f", outExt)
		// MKV is flexible, copy AV codec for supported formats
		if inExt == ".mp4" || inExt == ".m4v" {
			flags = append(flags, consts.VideoCodecCopy[:]...)
		} else {
			flags = append(flags, consts.VideoToH264Balanced[:]...)
		}
		flags = append(flags, consts.AudioToAAC[:]...)
		flags = append(flags, consts.AudioBitrate[:]...)

	case ".webm":
		flags = append(flags, "-f", outExt)
		flags = append(flags, consts.VideoToH264Balanced[:]...)
		flags = append(flags, consts.PixelFmtYuv420p[:]...)
		flags = append(flags, consts.KeyframeBalanced[:]...)
		flags = append(flags, consts.AudioToAAC[:]...)
		flags = append(flags, consts.AudioBitrate[:]...)

	default:
		// Safe defaults for any other output format
		flags = append(flags, "-f", outExt)
		flags = append(flags, consts.VideoToH264Balanced[:]...)
		flags = append(flags, consts.PixelFmtYuv420p[:]...)
		flags = append(flags, consts.AudioToAAC[:]...)
		flags = append(flags, consts.AudioBitrate[:]...)
	}

	b.formatFlags = flags
}

// buildFinalCommand assembles the final FFmpeg command
func (b *CommandBuilder) buildFinalCommand() ([]string, error) {

	// MAP LENGTH LOGIC [KEEP CLOSE EYE ON THIS IF COMMANDS CHANGE]:
	//
	// GPU acceleration flags
	// "-y", "i", input file, output file (+4)
	// Length of metadata map, then * 2 to prefix "-metadata" to each entry
	// Length of format flags
	// Output file (+1)
	args := make([]string, 0, len(b.gpuAccel)+4+len(b.metadataMap)*2+len(b.formatFlags)+1)

	// Add GPU acceleration if present
	args = append(args, b.gpuAccel...)

	// Add input file
	args = append(args, "-y", "-i", b.inputFile)

	// Add all -metadata commands
	for key, value := range b.metadataMap {
		args = append(args, "-metadata", fmt.Sprintf("%s=%s", key, fieldFormatter(value)))
	}

	// Add format flags
	args = append(args, b.formatFlags...)

	// Add output file
	args = append(args, b.outputFile)

	return args, nil
}

// fieldFormatter cleans field values
func fieldFormatter(value string) string {
	return strings.TrimSpace(value)
}
package metadata

import (
	"Metarr/internal/models"
	"strings"
)

// addCredits adds all credit-related metadata
func (b *CommandBuilder) addTitlesDescs(t *models.MetadataTitlesDescs) {

	if t.Title == "" && t.FallbackTitle != "" {
		t.Title = t.FallbackTitle
	}
	if t.LongDescription == "" && t.Long_Description != "" {
		t.LongDescription = t.Long_Description
	}

	fields := map[string]string{
		"title":           t.Title,
		"subtitle":        t.Subtitle,
		"description":     t.Description,
		"longdescription": t.LongDescription,
		"summary":         t.Summary,
		"synopsis":        t.Synopsis,
	}

	for field, value := range fields {
		if value != "" {
			b.metadataMap[field] = value
		}
	}
}

// addCredits adds all credit-related metadata
func (b *CommandBuilder) addCredits(c *models.MetadataCredits) {

	// Single value credits
	fields := map[string]string{
		"actor":     c.Actor,
		"author":    c.Author,
		"artist":    c.Artist,
		"creator":   c.Creator,
		"studio":    c.Studio,
		"publisher": c.Publisher,
		"producer":  c.Producer,
		"performer": c.Performer,
		"composer":  c.Composer,
		"director":  c.Director,
		"writer":    c.Writer,
	}

	for field, value := range fields {
		if value != "" {
			b.metadataMap[field] = value
		}
	}

	// Array credits
	b.addArrayMetadata("actor", c.Actors)
	b.addArrayMetadata("composer", c.Composers)
	b.addArrayMetadata("artist", c.Artists)
	b.addArrayMetadata("studio", c.Studios)
	b.addArrayMetadata("performer", c.Performers)
	b.addArrayMetadata("producer", c.Producers)
	b.addArrayMetadata("publisher", c.Publishers)
	b.addArrayMetadata("director", c.Directors)
	b.addArrayMetadata("writer", c.Writers)
}

// addCredits adds all date-related metadata
func (b *CommandBuilder) addDates(d *models.MetadataDates) {

	fields := map[string]string{
		"creation_time":           d.Creation_Time,
		"date":                    d.Date,
		"originally_available_at": d.Originally_Available_At,
		"release_date":            d.ReleaseDate,
		"upload_date":             d.UploadDate,
		"year":                    d.Year,
	}

	for field, value := range fields {
		if value != "" {
			b.metadataMap[field] = value
		}
	}
}

// addShowInfo adds all show info related metadata
func (b *CommandBuilder) addShowInfo(s *models.MetadataShowData) {

	fields := map[string]string{
		"episode_id":    s.Episode_ID,
		"episode_sort":  s.Episode_Sort,
		"season_number": s.Season_Number,
		"season_title":  s.Season_Title,
		"show":          s.Show,
	}

	for field, value := range fields {
		if value != "" {
			b.metadataMap[field] = value
		}
	}
}

// addOtherMetadata adds other related metadata
func (b *CommandBuilder) addOtherMetadata(o *models.MetadataOtherData) {

	fields := map[string]string{
		"genre":    o.Genre,
		"hd_video": o.HD_Video,
		"language": o.Language,
	}

	for field, value := range fields {
		if value != "" {
			b.metadataMap[field] = value
		}
	}
}

// addArrayMetadata combines array values with existing metadata
func (b *CommandBuilder) addArrayMetadata(key string, values []string) {
	if len(values) == 0 {
		return
	}

	existing, exists := b.metadataMap[key]
	newValue := strings.Join(values, "; ")

	if exists && existing != "" {
		b.metadataMap[key] = existing + "; " + newValue
	} else {
		b.metadataMap[key] = newValue
	}
}
package metadata

import (
	"Metarr/internal/config"
	keys "Metarr/internal/domain/keys"
	"Metarr/internal/models"
	backup "Metarr/internal/utils/fs/backup"
	logging "Metarr/internal/utils/logging"
	prompt "Metarr/internal/utils/prompt"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"os"
	"strings"
	"sync"
)

type JSONFileRW struct {
	mu   sync.RWMutex
	Meta map[string]interface{}
	File *os.File
}

// NewJSONFileRW creates a new instance of the JSON file reader/writer
func NewJSONFileRW(file *os.File) *JSONFileRW {
	logging.PrintD(3, "Retrieving new meta writer/rewriter for file '%s'...", file.Name())
	return &JSONFileRW{
		File: file,
	}
}

// DecodeMetadata parses and stores XML metadata into a map and returns it
func (rw *JSONFileRW) DecodeMetadata(file *os.File) (map[string]interface{}, error) {

	rw.mu.Lock()
	defer rw.mu.Unlock()

	if _, err := file.Seek(0, io.SeekStart); err != nil {
		return nil, fmt.Errorf("failed to seek file: %w", err)
	}

	// Create a decoder to read directly from file
	decoder := json.NewDecoder(file)

	// Decode to map
	input := make(map[string]interface{})
	if err := decoder.Decode(&input); err != nil {
		return nil, fmt.Errorf("failed to decode JSON: %w", err)
	}

	switch {
	case len(input) <= 0, input == nil:
		logging.PrintD(3, "Metadata not stored, is blank: %v", input)
	default:
		rw.Meta = input
		logging.PrintD(3, "Decoded and stored metadata: %v", rw.Meta)
	}

	return rw.Meta, nil
}

// RefreshMetadata reloads the metadata map from the file after updates
func (rw *JSONFileRW) RefreshMetadata() (map[string]interface{}, error) {

	rw.mu.RLock()
	defer rw.mu.RUnlock()

	if _, err := rw.File.Seek(0, io.SeekStart); err != nil {
		return nil, fmt.Errorf("failed to seek file: %w", err)
	}

	// Decode metadata
	decoder := json.NewDecoder(rw.File)

	if err := decoder.Decode(&rw.Meta); err != nil {
		return nil, fmt.Errorf("failed to decode JSON: %w", err)
	}

	logging.PrintD(3, "Decoded metadata: %v", rw.Meta)

	return rw.Meta, nil
}

// WriteMetadata inserts metadata into the JSON file from a map
func (rw *JSONFileRW) WriteMetadata(fieldMap map[string]*string) (map[string]interface{}, error) {

	rw.mu.Lock()
	defer rw.mu.Unlock()

	logging.PrintD(3, "Entering WriteMetadata for file '%s'", rw.File.Name())
	noFileOW := config.GetBool(keys.NoFileOverwrite)
	metaOW := config.GetBool(keys.MOverwrite)

	if noFileOW {
		err := backup.BackupFile(rw.File)
		if err != nil {
			return rw.Meta, fmt.Errorf("failed to create a backup of file '%s'", rw.File.Name())
		}
	}
	// Refresh metadata without lock
	if err := rw.refreshMetadataInternal(rw.File); err != nil {
		return rw.Meta, err
	}

	// Update metadata with new fields
	updated := false
	for field, value := range fieldMap {
		if value != nil && *value != "" {
			currentVal, exists := rw.Meta[field]
			if !exists {
				logging.PrintD(3, "Adding new field '%s' with value '%s'", field, *value)
				rw.Meta[field] = *value
				updated = true
			} else if currentStrVal, ok := currentVal.(string); !ok || currentStrVal != *value || metaOW {
				logging.PrintD(3, "Updating field '%s' from '%v' to '%s'", field, currentVal, *value)
				rw.Meta[field] = *value
				updated = true
			} else {
				logging.PrintD(3, "Skipping field '%s' - value unchanged and overwrite not forced", field)
			}
		}
	}

	// Return if no updates
	if !updated {
		logging.PrintD(2, "No fields were updated")
		return rw.Meta, nil
	}
	// Format the updated metadata for writing to file
	updatedContent, err := json.MarshalIndent(rw.Meta, "", "  ")
	if err != nil {
		return rw.Meta, fmt.Errorf("failed to marshal updated JSON: %w", err)
	}

	if err = rw.writeMetadataToFile(rw.File, updatedContent); err != nil {
		return rw.Meta, err
	}

	logging.PrintD(3, "Successfully updated JSON file with new metadata")
	return rw.Meta, nil
}

// MakeMetaEdits applies a series of transformations and writes the final result to the file
func (rw *JSONFileRW) MakeMetaEdits(data map[string]interface{}, file *os.File, wd *models.MetadataWebData) (bool, error) {

	var (
		edited, ok bool
		pfx        []*models.MetaReplacePrefix
		sfx        []*models.MetaReplaceSuffix
		new        []*models.MetaNewField
	)

	if config.IsSet(keys.MReplacePfx) {
		pfx, ok = config.Get(keys.MReplacePfx).([]*models.MetaReplacePrefix)
		if !ok {
			logging.PrintE(0, "Could not retrieve prefixes, wrong type: '%T'", pfx)
		}
	}
	if config.IsSet(keys.MReplaceSfx) {
		sfx, ok = config.Get(keys.MReplaceSfx).([]*models.MetaReplaceSuffix)
		if !ok {
			logging.PrintE(0, "Could not retrieve suffixes, wrong type: '%T'", pfx)
		}
	}
	if config.IsSet(keys.MNewField) {
		new, ok = config.Get(keys.MNewField).([]*models.MetaNewField)
		if !ok {
			logging.PrintE(0, "Could not retrieve new fields, wrong type: '%T'", pfx)
		}
	}

	if len(pfx) > 0 {
		newPrefix, err := rw.replaceMetaPrefix(data)
		if err != nil {
			logging.PrintE(0, err.Error())
		}
		if newPrefix {
			edited = true
		}
	}
	logging.PrintD(3, "After meta prefix replace: %v", data)

	if len(sfx) > 0 {
		newSuffix, err := rw.replaceMetaSuffix(data)
		if err != nil {
			logging.PrintE(0, err.Error())
		}
		if newSuffix {
			edited = true
		}
	}
	logging.PrintD(3, "After meta suffix replace: %v", data)

	if len(new) > 0 {
		newField, err := rw.addNewMetaField(data)
		if err != nil {
			logging.PrintE(0, err.Error())
		}
		if newField {
			edited = true
		}
	}

	logging.PrintD(3, "JSON after transformations: %v", data)

	// Marshal the updated JSON back to a byte slice
	updatedFileContent, err := json.MarshalIndent(data, "", "  ")
	if err != nil {
		return false, fmt.Errorf("failed to marshal updated JSON: %w", err)
	}

	if err = rw.writeMetadataToFile(file, updatedFileContent); err != nil {
		return false, fmt.Errorf("failed to write updated JSON to file: %w", err)
	}

	fmt.Println()
	logging.PrintS(0, "Successfully applied metadata edits to: %v", file.Name())

	return edited, nil
}

// refreshMetadataInternal is a private metadata refresh function
func (rw *JSONFileRW) refreshMetadataInternal(file *os.File) error {

	if _, err := file.Seek(0, io.SeekStart); err != nil {
		return fmt.Errorf("failed to seek file: %w", err)
	}

	if len(rw.Meta) <= 0 || rw.Meta == nil {
		return fmt.Errorf("JSONFileRW's stored metadata map is empty or null, did you forget to decode?")
	}

	decoder := json.NewDecoder(file)
	if err := decoder.Decode(&rw.Meta); err != nil {
		return fmt.Errorf("failed to decode JSON: %w", err)
	}

	logging.PrintD(3, "Decoded metadata: %v", rw.Meta)
	return nil
}

// writeMetadataToFile is a private metadata writing helper function
func (rw *JSONFileRW) writeMetadataToFile(file *os.File, content []byte) error {

	if err := file.Truncate(0); err != nil {
		return fmt.Errorf("failed to truncate file: %w", err)
	}

	if _, err := file.Seek(0, io.SeekStart); err != nil {
		return fmt.Errorf("failed to seek to beginning of file: %w", err)
	}

	if _, err := file.Write(content); err != nil {
		return fmt.Errorf("failed to write to file: %w", err)
	}

	return nil
}

// replaceMetaSuffix applies suffix replacement to the fields in the JSON data
func (rw *JSONFileRW) replaceMetaSuffix(data map[string]interface{}) (bool, error) {

	sfx, ok := config.Get(keys.MReplaceSfx).([]*models.MetaReplaceSuffix)
	if !ok {
		logging.PrintE(0, "Could not retrieve suffixes in private meta replace function, wrong type: '%T'", sfx)
	}

	logging.PrintD(3, "Entering replaceMetaSuffix with data: %v", data)

	if len(sfx) == 0 {
		return false, nil // No replacements to apply
	}

	newAddition := false
	for _, replace := range sfx {
		if replace.Field == "" || replace.Suffix == "" {
			continue
		}

		if value, found := data[replace.Field]; found {

			if strValue, ok := value.(string); ok {

				logging.PrintD(2, "Identified input JSON field '%v', trimming off '%v'", value, replace.Suffix)

				if strings.HasSuffix(strValue, replace.Suffix) {
					newValue := strings.TrimSuffix(strValue, replace.Suffix) + replace.Replacement
					newValue = strings.TrimSpace(newValue)

					logging.PrintD(2, "Changing '%v' to new value '%v'", replace.Field, newValue)
					data[replace.Field] = newValue
					newAddition = true
				}
			}
		}
	}
	return newAddition, nil
}

// replaceMetaPrefix applies prefix replacement to the fields in the JSON data
func (rw *JSONFileRW) replaceMetaPrefix(data map[string]interface{}) (bool, error) {

	pfx, ok := config.Get(keys.MReplacePfx).([]*models.MetaReplacePrefix)
	if !ok {
		logging.PrintE(0, "Could not retrieve prefixes, wrong type: '%T'", pfx)
	}
	logging.PrintD(2, "Entering replaceMetaPrefix with data: %v", data)
	if len(pfx) == 0 {
		return false, nil // No replacements to apply
	}

	newAddition := false
	for _, replace := range pfx {
		if replace.Field == "" || replace.Prefix == "" {
			continue
		}

		if value, found := data[replace.Field]; found {
			if strValue, ok := value.(string); ok {

				if strings.HasPrefix(strValue, replace.Prefix) {
					newValue := strings.TrimPrefix(strValue, replace.Prefix) + replace.Replacement
					newValue = strings.TrimSpace(newValue)
					data[replace.Field] = newValue
					newAddition = true
				}
			}
		}
	}
	return newAddition, nil
}

// addNewField can insert a new field which does not yet exist into the metadata file
func (rw *JSONFileRW) addNewMetaField(data map[string]interface{}) (bool, error) {

	new, ok := config.Get(keys.MNewField).([]*models.MetaNewField)
	if !ok {
		logging.PrintE(0, "Could not retrieve new fields, wrong type: '%T'", new)
	}
	metaOW := config.GetBool(keys.MOverwrite)
	metaPS := config.GetBool(keys.MPreserve)

	if len(new) == 0 {
		logging.PrintD(2, "Key %s is not set in Viper", keys.MNewField)
		return false, nil
	}

	logging.PrintD(3, "Retrieved additions for new field data: %v", new)
	processedFields := make(map[string]bool, len(new))

	newAddition := false
	ctx := context.Background()
	for _, addition := range new {
		if addition.Field == "" || addition.Value == "" {
			continue
		}

		// If field doesn't exist at all, add it
		if _, exists := data[addition.Field]; !exists {
			data[addition.Field] = addition.Value
			processedFields[addition.Field] = true
			newAddition = true
			continue
		}
		if !metaOW {

			// Check for context cancellation before proceeding
			select {
			case <-ctx.Done():
				logging.PrintI("Operation canceled for field: %s", addition.Field)
				return false, fmt.Errorf("operation canceled")
			default:
				// Proceed
			}
			if _, alreadyProcessed := processedFields[addition.Field]; alreadyProcessed {
				continue
			}

			if existingValue, exists := data[addition.Field]; exists {

				if !metaOW && !metaPS {
					promptMsg := fmt.Sprintf("Field '%s' already exists with value '%v' in file '%v'. Overwrite? (y/n) to proceed, (Y/N) to apply to whole queue", addition.Field, existingValue, rw.File.Name())

					reply, err := prompt.PromptMetaReplace(promptMsg, metaOW, metaPS)
					if err != nil {
						logging.PrintE(0, err.Error())
					}
					switch reply {
					case "Y":
						logging.PrintD(2, "Received meta overwrite reply as 'Y' for %s in %s, falling through to 'y'", existingValue, rw.File.Name())
						config.Set(keys.MOverwrite, true)
						metaOW = true
						fallthrough
					case "y":
						logging.PrintD(2, "Received meta overwrite reply as 'y' for %s in %s", existingValue, rw.File.Name())
						addition.Field = strings.TrimSpace(addition.Field)
						logging.PrintD(3, "Adjusted field from '%s' to '%s'\n", data[addition.Field], addition.Field)

						data[addition.Field] = addition.Value
						processedFields[addition.Field] = true
						newAddition = true

					case "N":
						logging.PrintD(2, "Received meta overwrite reply as 'N' for %s in %s, falling through to 'n'", existingValue, rw.File.Name())
						config.Set(keys.MPreserve, true)
						metaPS = true
						fallthrough
					case "n":
						logging.PrintD(2, "Received meta overwrite reply as 'n' for %s in %s", existingValue, rw.File.Name())
						logging.Print("Skipping field '%s'\n", addition.Field)
						processedFields[addition.Field] = true
					}
				} else if metaOW { // FieldOverwrite is set

					data[addition.Field] = addition.Value
					processedFields[addition.Field] = true
					newAddition = true

				} else if metaPS { // FieldPreserve is set
					continue
				}
			}
		} else {
			// Add the field if it doesn't exist yet, or overwrite is true
			data[addition.Field] = addition.Value
			processedFields[addition.Field] = true
			newAddition = true
		}
	}
	return newAddition, nil
}
package metadata

import (
	"Metarr/internal/config"
	keys "Metarr/internal/domain/keys"
	"Metarr/internal/models"
	logging "Metarr/internal/utils/logging"
	prompt "Metarr/internal/utils/prompt"
	"bufio"
	"context"
	"encoding/xml"
	"fmt"
	"io"
	"os"
	"strings"
	"sync"
)

type NFOFileRW struct {
	mu    sync.RWMutex
	Model *models.NFOData
	Meta  string
	File  *os.File
}

// NewNFOFileRW creates a new instance of the NFO file reader/writer
func NewNFOFileRW(file *os.File) *NFOFileRW {
	logging.PrintD(3, "Retrieving new meta writer/rewriter for file '%s'...", file.Name())
	return &NFOFileRW{
		File: file,
	}
}

// DecodeMetadata decodes XML from a file into a map, stores, and returns it
func (rw *NFOFileRW) DecodeMetadata(file *os.File) (*models.NFOData, error) {
	rw.mu.Lock()
	defer rw.mu.Unlock()

	// Read the entire file content first
	content, err := io.ReadAll(file)
	if err != nil {
		return nil, fmt.Errorf("failed to read file: %w", err)
	}

	rtn := rw.ensureXMLStructure(string(content))
	if rtn != "" {
		content = []byte(rtn)
	}

	// Store the raw content
	rw.Meta = string(content)

	// Reset file pointer
	if _, err := file.Seek(0, io.SeekStart); err != nil {
		return nil, fmt.Errorf("failed to seek file: %w", err)
	}

	// Single decode for the model
	decoder := xml.NewDecoder(file)
	var input *models.NFOData
	if err := decoder.Decode(&input); err != nil {
		return nil, fmt.Errorf("failed to decode XML: %w", err)
	}

	rw.Model = input
	logging.PrintD(3, "Decoded metadata: %v", rw.Model)

	return rw.Model, nil
}

// RefreshMetadata reloads the metadata map from the file after updates
func (rw *NFOFileRW) RefreshMetadata() (*models.NFOData, error) {

	rw.mu.RLock()
	defer rw.mu.RUnlock()

	if _, err := rw.File.Seek(0, io.SeekStart); err != nil {
		return nil, fmt.Errorf("failed to seek file: %w", err)
	}

	// Decode metadata
	decoder := xml.NewDecoder(rw.File)

	if err := decoder.Decode(&rw.Model); err != nil {
		return nil, fmt.Errorf("failed to decode xml: %w", err)
	}

	logging.PrintD(3, "Decoded metadata: %v", rw.Model)

	return rw.Model, nil
}

// MakeMetaEdits applies a series of transformations and writes the final result to the file
func (rw *NFOFileRW) MakeMetaEdits(data string, file *os.File, wd *models.MetadataWebData) (bool, error) {
	// Ensure we have valid XML
	if !strings.Contains(data, "<movie>") {
		return false, fmt.Errorf("invalid XML: missing movie tag")
	}

	var (
		edited, ok bool
		pfx        []*models.MetaReplacePrefix
		sfx        []*models.MetaReplaceSuffix
		new        []*models.MetaNewField
	)

	if config.IsSet(keys.MReplacePfx) {
		pfx, ok = config.Get(keys.MReplacePfx).([]*models.MetaReplacePrefix)
		if !ok {
			return false, fmt.Errorf("invalid prefix configuration")
		}
	}
	if config.IsSet(keys.MReplaceSfx) {
		sfx, ok = config.Get(keys.MReplaceSfx).([]*models.MetaReplaceSuffix)
		if !ok {
			return false, fmt.Errorf("invalid suffix configuration")
		}
	}
	if config.IsSet(keys.MNewField) {
		new, ok = config.Get(keys.MNewField).([]*models.MetaNewField)
		if !ok {
			return false, fmt.Errorf("invalid new field configuration")
		}
	}

	// Track content at each step
	currentContent := data

	// Add new fields first
	if len(new) > 0 {
		modified, updated, err := rw.addMetaFields(currentContent)
		if err != nil {
			logging.PrintE(0, "New field addition error: %v", err)
		}
		if updated {
			currentContent = string(modified)
			edited = true
		}
		logging.PrintD(3, "After new field additions: %s", currentContent)
	}

	// Prefix replacements
	if len(pfx) > 0 {
		modified, updated, err := rw.replaceMetaPrefix(currentContent)
		if err != nil {
			logging.PrintE(0, "Prefix replacement error: %v", err)
		}
		if updated {
			currentContent = string(modified)
			edited = true
		}
	}

	// Suffix replacements
	if len(sfx) > 0 {
		modified, updated, err := rw.replaceMetaSuffix(currentContent)
		if err != nil {
			logging.PrintE(0, "Suffix replacement error: %v", err)
		}
		if updated {
			currentContent = string(modified)
			edited = true
		}
	}

	// Only write if changes were made
	if edited {
		if err := rw.writeMetadataToFile(file, []byte(currentContent)); err != nil {
			return false, fmt.Errorf("failed to refresh metadata: %w", err)
		}
	}

	return edited, nil
}

// Helper function to ensure XML structure
func (rw *NFOFileRW) ensureXMLStructure(content string) string {
	// Ensure XML declaration
	if !strings.HasPrefix(content, "<?xml") {
		content = `<?xml version="1.0" encoding="UTF-8"?>` + "\n" + content
	}

	// Ensure movie tag exists
	if !strings.Contains(content, "<movie>") {
		content = strings.TrimSpace(content)
		content = content + "\n<movie>\n</movie>"
	}

	return content
}

// refreshMetadataInternal is a private metadata refresh function
func (rw *NFOFileRW) refreshMetadataInternal(file *os.File) error {

	if _, err := file.Seek(0, io.SeekStart); err != nil {
		return fmt.Errorf("failed to seek file: %w", err)
	}

	if rw.Model == nil {
		return fmt.Errorf("NFOFileRW's stored metadata map is empty or null, did you forget to decode?")
	}

	decoder := xml.NewDecoder(file)
	if err := decoder.Decode(&rw.Model); err != nil {
		return fmt.Errorf("failed to decode xml: %w", err)
	}

	return nil
}

// writeMetadataToFile is a private metadata writing helper function
func (rw *NFOFileRW) writeMetadataToFile(file *os.File, content []byte) error {

	if err := file.Truncate(0); err != nil {
		return fmt.Errorf("truncate file: %w", err)
	}

	if _, err := file.Seek(0, io.SeekStart); err != nil {
		return fmt.Errorf("seek file: %w", err)
	}

	// Use buffered writer for efficiency
	writer := bufio.NewWriter(file)
	if _, err := writer.Write(content); err != nil {
		return fmt.Errorf("write content: %w", err)
	}

	if err := rw.refreshMetadataInternal(file); err != nil {
		return fmt.Errorf("failed to refresh metadata: %w", err)
	}

	return writer.Flush()
}

// replaceMetaSuffix applies suffix replacement to the fields in the xml data
func (rw *NFOFileRW) replaceMetaSuffix(data string) (string, bool, error) {
	sfx, ok := config.Get(keys.MReplaceSfx).([]*models.MetaReplaceSuffix)
	if !ok {
		logging.PrintE(0, "Could not retrieve suffixes, wrong type: '%T'", sfx)
	}

	logging.PrintD(3, "Entering replaceMetaSuffix with data: %v", string(data))

	if len(sfx) == 0 {
		return data, false, nil // No replacements to apply
	}

	newAddition := false
	for _, replace := range sfx {
		if replace.Field == "" || replace.Suffix == "" {
			continue
		}

		startTag := fmt.Sprintf("<%s>", replace.Field)
		endTag := fmt.Sprintf("</%s>", replace.Field)

		startIdx := strings.Index(data, startTag)
		endIdx := strings.Index(data, endTag)
		if startIdx == -1 || endIdx == -1 {
			continue // One or both tags missing
		}

		contentStart := startIdx + len(startTag)
		content := strings.TrimSpace(data[contentStart:endIdx])

		logging.PrintD(2, "Identified input xml field '%v', trimming off '%v'", content, replace.Suffix)

		if strings.HasSuffix(data, replace.Suffix) {
			newContent := strings.TrimSuffix(data, replace.Suffix) + replace.Replacement
			newContent = strings.TrimSpace(newContent)

			logging.PrintD(2, "Changing '%v' to new value '%v'", replace.Field, newContent)

			data = data[:contentStart] + newContent + data[endIdx:]
			newAddition = true
		}
	}
	return data, newAddition, nil
}

// replaceMetaPrefix applies Prefix replacement to the fields in the xml data
func (rw *NFOFileRW) replaceMetaPrefix(data string) (string, bool, error) {
	sfx, ok := config.Get(keys.MReplaceSfx).([]*models.MetaReplacePrefix)
	if !ok {
		logging.PrintE(0, "Could not retrieve prefixes, wrong type: '%T'", sfx)
	}

	logging.PrintD(3, "Entering replaceMetaPrefix with data: %v", data)

	if len(sfx) == 0 {
		return data, false, nil // No replacements to apply
	}

	newAddition := false
	for _, replace := range sfx {
		if replace.Field == "" || replace.Prefix == "" {
			continue
		}

		startTag := fmt.Sprintf("<%s>", replace.Field)
		endTag := fmt.Sprintf("</%s>", replace.Field)

		startIdx := strings.Index(data, startTag)
		endIdx := strings.Index(data, endTag)
		if startIdx == -1 || endIdx == -1 {
			continue // One or both tags missing
		}

		contentStart := startIdx + len(startTag)
		content := strings.TrimSpace(data[contentStart:endIdx])

		logging.PrintD(2, "Identified input xml field '%v', trimming off '%v'", content, replace.Prefix)

		if strings.HasPrefix(data, replace.Prefix) {
			newContent := strings.TrimPrefix(data, replace.Prefix) + replace.Replacement
			newContent = strings.TrimSpace(newContent)

			logging.PrintD(2, "Changing '%v' to new value '%v'", replace.Field, newContent)

			data = data[:contentStart] + newContent + data[endIdx:]
			newAddition = true
		}
	}
	return data, newAddition, nil
}

// addNewField can insert a new field which does not yet exist into the metadata file
func (rw *NFOFileRW) addMetaFields(data string) (string, bool, error) {
	new, ok := config.Get(keys.MNewField).([]*models.MetaNewField)
	if !ok {
		logging.PrintE(0, "Could not retrieve new fields, wrong type: '%T'", new)
	}
	metaOW := config.GetBool(keys.MOverwrite)
	metaPS := config.GetBool(keys.MPreserve)

	if len(new) == 0 {
		logging.PrintD(2, "Key %s is not set in Viper", keys.MNewField)
		return data, false, nil
	}

	logging.PrintD(3, "Retrieved additions for new field data: %v", new)

	newAddition := false
	ctx := context.Background()

	for _, addition := range new {
		if addition.Field == "" || addition.Value == "" {
			continue
		}

		// Special handling for actor fields
		if addition.Field == "actor" {
			// Check if actor already exists
			flatData := rw.flattenField(data)
			actorNameCheck := fmt.Sprintf("<name>%s</name>", rw.flattenField(addition.Value))

			if strings.Contains(flatData, actorNameCheck) {
				logging.PrintI("Actor '%s' is already inserted in the metadata, no need to add...", addition.Value)
			} else {
				if modified, ok := rw.addNewActorField(data, addition.Value); ok {
					data = modified
					newAddition = true
				}
			}
			continue
		}

		// Handle non-actor fields
		tagStart := fmt.Sprintf("<%s>", addition.Field)
		tagEnd := fmt.Sprintf("</%s>", addition.Field)

		startIdx := strings.Index(data, tagStart)
		if startIdx == -1 {
			// Field doesn't exist, add it
			if modified, ok := rw.addNewField(data, fmt.Sprintf("%s%s%s", tagStart, addition.Value, tagEnd)); ok {
				data = modified
				newAddition = true
			}
			continue
		}

		// Field exists, handle overwrite
		if !metaOW {
			startContent := startIdx + len(tagStart)
			endIdx := strings.Index(data, tagEnd)
			content := strings.TrimSpace(data[startContent:endIdx])

			// Check for context cancellation
			select {
			case <-ctx.Done():
				logging.PrintI("Operation canceled for field: %s", addition.Field)
				return data, false, fmt.Errorf("operation canceled")
			default:
				// Proceed
			}

			if !metaOW && !metaPS {
				promptMsg := fmt.Sprintf("Field '%s' already exists with value '%v' in file '%v'. Overwrite? (y/n) to proceed, (Y/N) to apply to whole queue",
					addition.Field, content, rw.File.Name())

				reply, err := prompt.PromptMetaReplace(promptMsg, metaOW, metaPS)
				if err != nil {
					logging.PrintE(0, err.Error())
				}

				switch reply {
				case "Y":
					config.Set(keys.MOverwrite, true)
					metaOW = true
					fallthrough
				case "y":
					data = data[:startContent] + addition.Value + data[endIdx:]
					newAddition = true
				case "N":
					config.Set(keys.MPreserve, true)
					metaPS = true
					fallthrough
				case "n":
					logging.PrintD(2, "Skipping field: %s", addition.Field)
				}
			} else if metaOW {
				data = data[:startContent] + addition.Value + data[endIdx:]
				newAddition = true
			}
		}
	}

	return data, newAddition, nil
}

// addNewField adds a new field into the NFO
func (rw *NFOFileRW) addNewField(data, addition string) (string, bool) {

	insertIdx := strings.Index(data, "<movie>")
	insertAfter := insertIdx + len("<movie>")

	if insertIdx != -1 {
		data = data[:insertAfter] + "\n" + addition + "\n" + data[insertAfter:]
	}
	return data, true
}

// addNewActorField adds a new actor into the file
func (rw *NFOFileRW) addNewActorField(data, name string) (string, bool) {
	castStart := strings.Index(data, "<cast>")
	castEnd := strings.Index(data, "</cast>")

	if castStart == -1 && castEnd == -1 {
		// No cast tag exists, create new structure
		movieStart := strings.Index(data, "<movie>")
		if movieStart == -1 {
			logging.PrintE(0, "Invalid XML structure: no movie tag found")
			return data, false
		}

		movieEnd := strings.Index(data, "</movie>")
		if movieEnd == -1 {
			logging.PrintE(0, "Invalid XML structure: no closing movie tag found")
			return data, false
		}

		// Create new cast section
		newCast := fmt.Sprintf("    <cast>\n        <actor>\n            <name>%s</name>\n        </actor>\n    </cast>", name)

		// Find the right spot to insert
		contentStart := movieStart + len("<movie>")
		if contentStart >= len(data) {
			logging.PrintE(0, "Invalid XML structure: movie tag at end of data")
			return data, false
		}

		return data[:contentStart] + "\n" + newCast + "\n" + data[contentStart:], true
	}

	// Cast exists, validate indices
	if castStart == -1 || castEnd == -1 || castStart >= len(data) || castEnd > len(data) {
		logging.PrintE(0, "Invalid XML structure: mismatched cast tags")
		return data, false
	}

	// Insert new actor
	newActor := fmt.Sprintf("    <actor>\n            <name>%s</name>\n        </actor>", name)

	if castEnd-castStart > 1 {
		// Cast has content, insert with proper spacing
		return data[:castEnd] + newActor + "\n    " + data[castEnd:], true
	} else {
		// Empty cast tag
		insertPoint := castStart + len("<cast>")
		return data[:insertPoint] + newActor + "\n    " + data[insertPoint:], true
	}
}

// flattenField flattens the metadata field for comparison
func (rw *NFOFileRW) flattenField(s string) string {

	rtn := strings.TrimSpace(s)
	rtn = strings.ReplaceAll(rtn, " ", "")
	rtn = strings.ReplaceAll(rtn, "\n", "")
	rtn = strings.ReplaceAll(rtn, "\r", "")
	rtn = strings.ReplaceAll(rtn, "\t", "")

	return rtn
}
package models

import (
	enums "Metarr/internal/domain/enums"
	"os"
)

func NewFileData() *FileData {
	return &FileData{
		MTitleDesc: &MetadataTitlesDescs{},
		MCredits:   &MetadataCredits{},
		MDates:     &MetadataDates{},
		MShowData:  &MetadataShowData{},
		MWebData:   &MetadataWebData{},
		MOther:     &MetadataOtherData{},
	}
}

type FileData struct {
	VideoDirectory        string   `json:"-" xml:"-"`
	OriginalVideoPath     string   `json:"-" xml:"-"`
	OriginalVideoBaseName string   `json:"-" xml:"-"`
	TempOutputFilePath    string   `json:"-" xml:"-"`
	FinalVideoPath        string   `json:"-" xml:"-"`
	FinalVideoBaseName    string   `json:"-" xml:"-"`
	FilenameMetaPrefix    string   `json:"-" xml:"-"`
	FilenameDateTag       string   `json:"-" xml:"-"`
	RenamedVideoPath      string   `json:"-"`
	RenamedMetaPath       string   `json:"-"`
	VideoFile             *os.File `json:"-" xml:"-"`
	// JSON paths
	JSONDirectory string `json:"-" xml:"-"`
	JSONFilePath  string `json:"-" xml:"-"`
	JSONBaseName  string `json:"-" xml:"-"`
	// NFO paths
	NFOBaseName  string `json:"-" xml:"-"`
	NFODirectory string `json:"-" xml:"-"`
	NFOFilePath  string `json:"-" xml:"-"`
	// Meta type
	MetaFileType enums.MetaFiletypeFound `json:"-" xml:"-"`
	// Metadata
	MCredits   *MetadataCredits     `json:"meta_credits" xml:"credits"`
	MTitleDesc *MetadataTitlesDescs `json:"meta_title_description" xml:"titles"`
	MDates     *MetadataDates       `json:"meta_dates" xml:"dates"`
	MShowData  *MetadataShowData    `json:"meta_show_data" xml:"show"`
	MWebData   *MetadataWebData     `json:"meta_web_data" xml:"web"`
	MOther     *MetadataOtherData   `json:"meta_other_data" xml:"other"`

	JSONFileRW JSONFileRW
	NFOFileRW  NFOFileRW
	NFOData    *NFOData
}
package models

import "os"

// Metadata read/write interface
type JSONFileRW interface {
	DecodeMetadata(file *os.File) (map[string]interface{}, error)
	RefreshMetadata() (map[string]interface{}, error)
	WriteMetadata(fieldMap map[string]*string) (map[string]interface{}, error)
	MakeMetaEdits(data map[string]interface{}, file *os.File, wd *MetadataWebData) (bool, error)
}

// Metadata read/write interface
type NFOFileRW interface {
	DecodeMetadata(file *os.File) (*NFOData, error)
	RefreshMetadata() (*NFOData, error)
	MakeMetaEdits(data string, file *os.File, wd *MetadataWebData) (bool, error)
}
package models

import (
	enums "Metarr/internal/domain/enums"
	"net/http"
)

func NewMetaReplaceSuffix(f, s, r string) *MetaReplaceSuffix {
	return &MetaReplaceSuffix{
		Field:       f,
		Suffix:      s,
		Replacement: r,
	}
}

type MetaReplaceSuffix struct {
	Field       string
	Suffix      string
	Replacement string
}

type MetaReplacePrefix struct {
	Field       string
	Prefix      string
	Replacement string
}

type MetaNewField struct {
	Field string
	Value string
}

type FilenameDatePrefix struct {
	YearLength  int
	MonthLength int
	DayLength   int
	Order       enums.FilenameDateFormat
}

func NewFilenameReplaceSuffix(s, r string) *FilenameReplaceSuffix {
	return &FilenameReplaceSuffix{
		Suffix:      s,
		Replacement: r,
	}
}

type FilenameReplaceSuffix struct {
	Suffix      string
	Replacement string
}

type MetadataCredits struct {
	Actor      string `json:"actor" xml:"actor"`
	Actors     []string
	Author     string `json:"author" xml:"author"`
	Artist     string `json:"artist" xml:"artist"`
	Artists    []string
	Channel    string `json:"channel" xml:"channel"`
	Creator    string `json:"creator" xml:"creator"`
	Studio     string `json:"studio" xml:"studio"`
	Studios    []string
	Publisher  string `json:"publisher" xml:"publisher"`
	Publishers []string
	Producer   string `json:"producer" xml:"producer"`
	Producers  []string
	Performer  string `json:"performer" xml:"performer"`
	Performers []string
	Uploader   string `json:"uploader" xml:"uploader"`
	Composer   string `json:"composer" xml:"composer"`
	Composers  []string
	Director   string `json:"director" xml:"director"`
	Directors  []string
	Writer     string `json:"writer" xml:"writer"`
	Writers    []string
}

type MetadataTitlesDescs struct {
	Title            string `json:"fulltitle" xml:"title"`
	FallbackTitle    string `json:"title" xml:"originaltitle"`
	Subtitle         string `json:"subtitle" xml:"subtitle"`
	Description      string `json:"description" xml:"description"`
	LongDescription  string `json:"longdescription" xml:"plot"`
	Long_Description string `json:"long_description" xml:"long_description"`
	Synopsis         string `json:"synopsis" xml:"synopsis"`
	Summary          string `json:"summary" xml:"summary"`
	Comment          string `json:"comment" xml:"comment"`
}

type MetadataDates struct {
	FormattedDate           string `json:"-" xml:"-"`
	UploadDate              string `json:"upload_date" xml:"upload_date"`
	ReleaseDate             string `json:"release_date" xml:"release_date"`
	Date                    string `json:"date" xml:"date"`
	Year                    string `json:"year" xml:"year"`
	Originally_Available_At string `json:"originally_available_at" xml:"originally_available_at"`
	Creation_Time           string `json:"creation_time" xml:"creation_time"`
	StringDate              string `json:"-"`
}

type MetadataWebData struct {
	WebpageURL string         `json:"webpage_url" xml:"webpage_url"`
	VideoURL   string         `json:"url" xml:"url"`
	Domain     string         `json:"webpage_url_domain" xml:"domain"`
	Referer    string         `json:"referer" xml:"referer"`
	Cookies    []*http.Cookie `json:"-" xml:"-"`
	TryURLs    []string       `json:"-"`
}

type MetadataShowData struct {
	Show          string `json:"show" xml:"show"`
	Episode_ID    string `json:"episode_id" xml:"episode_id"`
	Episode_Sort  string `json:"episode_sort" xml:"episode_sort"`
	Season_Number string `json:"season_number" xml:"season_number"`
	Season_Title  string `json:"season_title" xml:"seasontitle"`
}

type MetadataOtherData struct {
	Language string `json:"language" xml:"language"`
	Genre    string `json:"genre" xml:"genre"`
	HD_Video string `json:"hd_video" xml:"hd_video"`
}
package models

import "encoding/xml"

// NFOData represents the complete NFO file structure
type NFOData struct {
	XMLName     xml.Name    `xml:"movie"`
	Title       Title       `xml:"title"`
	Plot        string      `xml:"plot"`
	Description string      `xml:"description"`
	Actors      []Person    `xml:"cast>actor"`
	Directors   []string    `xml:"director"`
	Producers   []string    `xml:"producer"`
	Publishers  []string    `xml:"publisher"`
	Writers     []string    `xml:"writer"`
	Studios     []string    `xml:"studio"`
	Year        string      `xml:"year"`
	Premiered   string      `xml:"premiered"`
	ReleaseDate string      `xml:"releasedate"`
	ShowInfo    ShowInfo    `xml:"showinfo"`
	WebpageInfo WebpageInfo `xml:"web"`
}

// Title represents nested title information
type Title struct {
	Main      string `xml:"main"`
	Original  string `xml:"original"`
	Sort      string `xml:"sort"`
	Sub       string `xml:"sub"`
	PlainText string `xml:",chardata"` // For non-nested titles
}

// Person represents a credited person with optional role
type Person struct {
	Name  string `xml:"name"`
	Role  string `xml:"role"`
	Order int    `xml:"order"`
	Thumb string `xml:"thumb"`
}

// ShowInfo represents TV show specific information
type ShowInfo struct {
	Show         string `xml:"show"`
	SeasonNumber string `xml:"season>number"`
	EpisodeID    string `xml:"episode>number"`
	EpisodeTitle string `xml:"episode>title"`
}

// ShowInfo represents TV show specific information
type WebpageInfo struct {
	URL    string `xml:"url"`
	Fanart string `xml:"fanart"`
	Thumb  string `xml:"thumb"`
}
package models

// SelectorRule holds rules for specific websites for use in scrapers
type SelectorRule struct {
	Selector string
	Attr     string // empty for text content, otherwise attribute name
	Process  func(string) string
	JsonPath []string
}
package processing

import (
	"Metarr/internal/config"
	enums "Metarr/internal/domain/enums"
	keys "Metarr/internal/domain/keys"
	reader "Metarr/internal/metadata/reader"
	writer "Metarr/internal/metadata/writer"
	"Metarr/internal/models"
	"Metarr/internal/transformations"
	fsRead "Metarr/internal/utils/fs/read"
	logging "Metarr/internal/utils/logging"
	"context"
	"fmt"
	"os"
	"sync"
	"sync/atomic"
)

var (
	totalMetaFiles,
	totalVideoFiles,
	processedMetaFiles,
	processedVideoFiles int32

	processedDataArray []*models.FileData
)

// processFiles is the main program function to process folder entries
func ProcessFiles(ctx context.Context, cancel context.CancelFunc, wg *sync.WaitGroup, cleanupChan chan os.Signal, openVideo, openMeta *os.File) {

	skipVideos := config.GetBool(keys.SkipVideos)

	var (
		videoMap,
		metaMap,
		matchedFiles map[string]*models.FileData

		err error
	)

	// Process metadata, checking if its a directory or a single file
	if openMeta != nil {
		fileInfo, _ := openMeta.Stat()
		if fileInfo.IsDir() {
			metaMap, err = fsRead.GetMetadataFiles(openMeta)
		} else {
			metaMap, err = fsRead.GetSingleMetadataFile(openMeta)
		}
		if err != nil {
			logging.PrintE(0, "Error: %v", err)
			os.Exit(1)
		}
	}
	// Process video files, checking if its a directory or a single file
	if openVideo != nil {
		fileInfo, _ := openVideo.Stat()
		if fileInfo.IsDir() {
			videoMap, err = fsRead.GetVideoFiles(openVideo)
		} else if !skipVideos {
			videoMap, err = fsRead.GetSingleVideoFile(openVideo)
		}
		if err != nil {
			logging.PrintE(0, "Error fetching video files: %v", err)
			os.Exit(1)
		}

		// Match video and metadata files
		if !skipVideos {
			matchedFiles, err = fsRead.MatchVideoWithMetadata(videoMap, metaMap)
			if err != nil {
				logging.PrintE(0, "Error matching videos with metadata: %v", err)
				os.Exit(1)
			}
		} else {
			matchedFiles = metaMap
		}
	}

	config.Set(keys.VideoMap, videoMap)
	config.Set(keys.MetaMap, metaMap)

	atomic.StoreInt32(&totalMetaFiles, int32(len(metaMap)))
	atomic.StoreInt32(&totalVideoFiles, int32(len(videoMap)))

	fmt.Printf("\nFound %d file(s) to process in the directory\n", totalMetaFiles+totalVideoFiles)

	logging.PrintD(3, "Matched metafiles: %v", matchedFiles)

	for _, fileData := range matchedFiles {

		var (
			processedData *models.FileData
			err           error
		)

		if !config.IsSet(keys.SkipVideos) || metaChanges() {
			switch fileData.MetaFileType {
			case enums.METAFILE_JSON:
				logging.PrintD(3, "File: %s: Meta file type in model as %v", fileData.JSONFilePath, fileData.MetaFileType)
				processedData, err = reader.ProcessJSONFile(fileData)

			case enums.METAFILE_NFO:
				logging.PrintD(3, "File: %s: Meta file type in model as %v", fileData.NFOFilePath, fileData.MetaFileType)
				processedData, err = reader.ProcessNFOFiles(fileData)
			}
			if err != nil {
				logging.ErrorArray = append(logging.ErrorArray, err)
				errMsg := fmt.Errorf("error processing metadata for file: %w", err)
				logging.PrintE(0, errMsg.Error())
				return
			}
			processedDataArray = append(processedDataArray, processedData)
		} else {
			processedDataArray = append(processedDataArray, fileData)
		}
	}

	// Goroutine to handle signals and cleanup
	go func() {
		<-cleanupChan

		fmt.Println("\nSignal received, cleaning up temporary files...")

		cancel()

		err = cleanupTempFiles(videoMap)
		if err != nil {
			logging.ErrorArray = append(logging.ErrorArray, err)
			fmt.Printf("\nFailed to cleanup temp files: %v", err)
			logging.PrintE(0, "Failed to cleanup temp files", err)
		}

		logging.PrintI("Process was interrupted by a syscall", nil)

		wg.Wait()
		os.Exit(0)
	}()

	sem := make(chan struct{}, config.GetInt(keys.Concurrency))

	for fileName, fileData := range matchedFiles {

		executeFile(ctx, wg, sem, fileName, fileData)
	}

	wg.Wait()

	err = cleanupTempFiles(videoMap)
	if err != nil {
		logging.ErrorArray = append(logging.ErrorArray, err)
		logging.PrintE(0, "Failed to cleanup temp files: %v", err)
	}

	replaceToStyle := config.Get(keys.Rename).(enums.ReplaceToStyle)
	inputVideoDir := config.GetString(keys.JsonDir)

	err = transformations.FileRename(processedDataArray, replaceToStyle)
	if err != nil {
		logging.ErrorArray = append(logging.ErrorArray, err)
		logging.PrintE(0, "Failed to rename files: %v", err)
	} else {
		logging.PrintS(0, "Successfully formatted file names in directory: %v", inputVideoDir)
	}

	if len(logging.ErrorArray) == 0 || logging.ErrorArray == nil {

		logging.PrintS(0, "Successfully processed all videos in directory (%v) with no errors.", inputVideoDir)
		fmt.Println()
	} else {

		logging.PrintE(0, "Program finished, but some errors were encountered: %v", logging.ErrorArray)
		fmt.Println()
	}
}

// processFile handles processing for both video and metadata files
func executeFile(ctx context.Context, wg *sync.WaitGroup, sem chan struct{}, fileName string, fileData *models.FileData) {
	wg.Add(1)
	go func(fileName string, fileData *models.FileData) {

		defer wg.Done()

		currentFile := atomic.AddInt32(&processedMetaFiles, 1)
		total := atomic.LoadInt32(&totalMetaFiles)

		fmt.Printf("\n====================================================\n")
		fmt.Printf("    Processed metafile %d of %d\n", currentFile, total)
		fmt.Printf("    Remaining: %d\n", total-currentFile)
		fmt.Printf("====================================================\n\n")

		sem <- struct{}{}
		defer func() {
			<-sem
		}()

		select {
		case <-ctx.Done():
			fmt.Printf("Skipping processing for %s due to cancellation\n", fileName)
			return
		default:
		}

		sysResourceLoop(fileName)

		skipVideos := config.GetBool(keys.SkipVideos)
		isVideoFile := fileData.OriginalVideoPath != ""

		if isVideoFile {
			logging.PrintI("Processing file: %s", fileName)
		} else {
			logging.PrintI("Processing metadata file: %s", fileName)
		}

		if isVideoFile && !skipVideos {
			err := writer.WriteMetadata(fileData)
			if err != nil {
				logging.ErrorArray = append(logging.ErrorArray, err)
				errMsg := fmt.Errorf("failed to process video '%v': %w", fileName, err)
				logging.PrintE(0, errMsg.Error())
			} else {
				logging.PrintS(0, "Successfully processed video %s", fileName)
			}
		} else {
			logging.PrintS(0, "Successfully processed metadata for %s", fileName)
		}

		currentFile = atomic.AddInt32(&processedVideoFiles, 1)
		total = atomic.LoadInt32(&totalVideoFiles)

		fmt.Printf("\n====================================================\n")
		fmt.Printf("    Processed video file %d of %d\n", currentFile, total)
		fmt.Printf("    Remaining: %d\n", total-currentFile)
		fmt.Printf("====================================================\n\n")

	}(fileName, fileData)
}
package processing

import (
	"Metarr/internal/config"
	keys "Metarr/internal/domain/keys"
	"Metarr/internal/models"
	logging "Metarr/internal/utils/logging"
	"fmt"
	"os"
	"time"

	"github.com/shirou/gopsutil/cpu"
	"github.com/shirou/gopsutil/mem"
)

// sysResourceLoop checks the system resources, controlling whether a new routine should be spawned
func sysResourceLoop(fileStr string) {

	var resourceMsg bool
	audioMemoryThreshold := config.GetUint64(keys.MinMemMB)

	for {
		// Fetch system resources and determine if processing can proceed
		proceed, availableMemory, CPUUsage, err := checkSysResources(audioMemoryThreshold)
		if err != nil {
			logging.ErrorArray = append(logging.ErrorArray, err)
			logging.PrintE(0, "Error checking system resources: %v", err)
		}
		if proceed {
			resourceMsg = false
			break
		}

		// Log resource info only once when insufficient resources are detected
		if !resourceMsg {
			logging.PrintI("Not enough system resources to process %s, waiting...", fileStr)
			logging.PrintD(1, "Memory available: %.2f MB\tCPU usage: %.2f%%\n", float64(availableMemory)/(1024*1024), CPUUsage)
			resourceMsg = true
		}
		time.Sleep(1 * time.Second) // Wait before checking again
	}
}

// checkAvailableMemory checks if enough memory is available (at least the threshold).
func checkSysResources(requiredMemory uint64) (bool, uint64, float64, error) {
	vMem, err := mem.VirtualMemory()
	if err != nil {
		return false, 0, 0, err
	}

	cpuPct, err := cpu.Percent(0, false)
	if err != nil {
		return false, 0, 0, err
	}

	maxCpuUsage := config.GetFloat64(keys.MaxCPU)
	return (vMem.Available >= requiredMemory && cpuPct[0] <= maxCpuUsage), vMem.Available, cpuPct[0], nil
}

// cleanupTempFiles removes temporary files
func cleanupTempFiles(files map[string]*models.FileData) error {

	var (
		errReturn error
		path      string
	)

	for _, data := range files {
		path = data.TempOutputFilePath
		if _, err := os.Stat(path); err == nil {
			fmt.Printf("Removing temp file: %s\n", path)
			err = os.Remove(path)
			if err != nil {
				errReturn = fmt.Errorf("error removing temp file: %w", err)
			}
		}
	}
	return errReturn
}

// metaChanges determines if metadatashould be processed
func metaChanges() bool {
	response := false
	if config.IsSet(keys.MReplacePfx) {
		response = true
	}
	if config.IsSet(keys.MReplaceSfx) {
		response = true
	}
	if config.IsSet(keys.MNewField) {
		response = true
	}
	if config.IsSet(keys.FileDateFmt) {
		response = true
	}
	return response
}
package transformations

import (
	config "Metarr/internal/config"
	keys "Metarr/internal/domain/keys"
	"Metarr/internal/models"
	logging "Metarr/internal/utils/logging"
)

// CensoredTvTransformations adds preset transformations to
// files for censored.tv videos
func CensoredTvTransformations() {

	logging.PrintI("Making preset censored.tv meta replacements")

	censoredTvMSuffixes()
	censoredTvFSuffixes()
}

// censoredTvMSuffixes adds meta suffix replacements
func censoredTvMSuffixes() {

	var (
		sfx []*models.MetaReplaceSuffix
		ok  bool
	)

	flagSet := config.IsSet(keys.MReplaceSfx)

	if !flagSet {
		sfx, ok = config.Get(keys.MReplaceSfx).([]*models.MetaReplaceSuffix)
		if !ok {
			logging.PrintE(0, "Got type %T", sfx)
		}
	}

	var new []*models.MetaReplaceSuffix
	new = append(new, models.NewMetaReplaceSuffix("title", " (1)", ""))
	new = append(new, models.NewMetaReplaceSuffix("fulltitle", " (1)", ""))
	new = append(new, models.NewMetaReplaceSuffix("id", "-1", ""))
	new = append(new, models.NewMetaReplaceSuffix("display_id", "-1", ""))

	for _, newSuffix := range new {
		exists := false
		for _, existingSuffix := range sfx {
			if existingSuffix.Field == newSuffix.Field {
				exists = true
				break
			}
		}
		if !exists {
			logging.PrintI("Adding new censored.tv meta suffix replacement: %v", newSuffix)
			sfx = append(sfx, newSuffix)
		}
	}

	config.Set(keys.MReplaceSfx, sfx)
}

// censoredTvFSuffixes adds filename suffix replacements
func censoredTvFSuffixes() {

	var (
		sfx []*models.FilenameReplaceSuffix
		ok  bool
	)

	flagSet := config.IsSet(keys.MReplaceSfx)

	if !flagSet {
		sfx, ok = config.Get(keys.FilenameReplaceSfx).([]*models.FilenameReplaceSuffix)
		if !ok {
			logging.PrintE(0, "Wrong type sent in, got %T", sfx)
		}
	}

	for _, existingSuffix := range sfx {
		if existingSuffix.Suffix != "_1" &&
			existingSuffix.Replacement != "" {

			sfx = append(sfx, models.NewFilenameReplaceSuffix("_1", ""))
			logging.PrintI("Set censored.tv filename suffix replacement.")
			break
		}
	}

	config.Set(keys.MReplaceSfx, sfx)
}
package transformations

import (
	"Metarr/internal/config"
	enums "Metarr/internal/domain/enums"
	keys "Metarr/internal/domain/keys"
	"Metarr/internal/models"
	writefs "Metarr/internal/utils/fs/write"
	logging "Metarr/internal/utils/logging"
	"fmt"
	"path/filepath"
	"strings"
)

// FileRename formats the file names
func FileRename(dataArray []*models.FileData, style enums.ReplaceToStyle) error {

	var vidExt string

	skipVideos := config.GetBool(keys.SkipVideos)

	for _, fd := range dataArray {
		metaBase, metaDir, originalMPath := getMetafileData(fd)
		metaExt := filepath.Ext(originalMPath)

		videoBase := fd.FinalVideoBaseName
		originalVPath := fd.FinalVideoPath
		vidExt = filepath.Ext(fd.OriginalVideoPath)

		renamedVideo := ""
		renamedMeta := ""

		if !skipVideos {
			renamedVideo = renameVideo(videoBase, style)
			renamedMeta = renamedVideo // Use video name as base to ensure best filename consistency
		} else {
			renamedMeta = renameMeta(metaBase, style)
		}

		var err error
		if renamedVideo, renamedMeta, err = fixContractions(renamedVideo, renamedMeta, style); err != nil {
			return fmt.Errorf("failed to fix contractions for %s. error: %v", renamedVideo, err)
		}

		// Add the metatag to the front of the filenames
		renamedVideo, renamedMeta = addTags(renamedVideo, renamedMeta, fd)

		// Trim trailing spaces
		renamedVideo = strings.TrimSpace(renamedVideo)
		renamedMeta = strings.TrimSpace(renamedMeta)

		logging.PrintD(2, "Rename replacements:\n\nVideo: %v\nMetafile: %v\n\n", renamedVideo, renamedMeta)

		// Construct final output filepaths
		renamedVPath := filepath.Join(fd.VideoDirectory, renamedVideo+vidExt)
		renamedMPath := filepath.Join(metaDir, renamedMeta+metaExt)

		// Save into model. May want to save to FinalVideoPath (etc) instead, but currently saves to new field
		fd.RenamedVideoPath = renamedVPath
		fd.RenamedMetaPath = renamedMPath

		fsWriter := writefs.NewFSFileWriter(skipVideos, renamedVPath, originalVPath, renamedMPath, originalMPath)

		if err := fsWriter.WriteResults(); err != nil {
			return err
		}
		if config.IsSet(keys.MoveOnComplete) {
			if err := fsWriter.MoveFile(); err != nil {
				logging.PrintE(0, "Failed to move to destination folder: %v", err)
			}
		}
	}
	return nil
}

// Performs name transformations for video files
func renameVideo(videoBase string, style enums.ReplaceToStyle) string {
	logging.PrintD(2, "Processing video base name: %q", videoBase)

	if !config.IsSet(keys.FilenameReplaceSfx) && style == enums.RENAMING_SKIP {
		return videoBase
	}

	// Transformations
	name := videoBase
	if config.IsSet(keys.FilenameReplaceSfx) {
		name = replaceSuffix(name)
	}

	if style != enums.RENAMING_SKIP {
		name = applyNamingStyle(style, name)
	}
	return name
}

// Performs name transformations for metafiles
func renameMeta(metaBase string, style enums.ReplaceToStyle) string {
	logging.PrintD(2, "Processing metafile base name: %q", metaBase)

	if !config.IsSet(keys.FilenameReplaceSfx) && style == enums.RENAMING_SKIP {
		return metaBase
	}

	// Transformations
	name := metaBase
	if config.IsSet(keys.FilenameReplaceSfx) {
		name = replaceSuffix(name)
	}

	if style != enums.RENAMING_SKIP {
		name = applyNamingStyle(style, name)
	}
	return name
}
package transformations

import (
	"Metarr/internal/config"
	consts "Metarr/internal/domain/constants"
	enums "Metarr/internal/domain/enums"
	keys "Metarr/internal/domain/keys"
	"Metarr/internal/models"
	presets "Metarr/internal/transformations/presets"
	logging "Metarr/internal/utils/logging"
	"fmt"
	"path/filepath"
	"regexp"
	"strings"
	"unicode"
)

func TryTransPresets(urls []string) (found bool) {

	for _, url := range urls {
		switch {
		case strings.Contains(url, "censored.tv"):
			presets.CensoredTvTransformations()
			found = true
		}
	}
	return found
}

// getMetafileData retrieves meta type specific data
func getMetafileData(m *models.FileData) (string, string, string) {

	switch m.MetaFileType {
	case enums.METAFILE_JSON:
		return m.JSONBaseName, m.JSONDirectory, m.JSONFilePath
	case enums.METAFILE_NFO:
		return m.NFOBaseName, m.NFODirectory, m.NFOFilePath
	default:
		logging.PrintE(0, "No metafile type set in model %v", m)
		return "", "", ""
	}
}

// Renaming conventions
func applyNamingStyle(style enums.ReplaceToStyle, input string) (output string) {

	switch style {
	case enums.RENAMING_SPACES:
		output = strings.ReplaceAll(input, "_", " ")
	case enums.RENAMING_UNDERSCORES:
		output = strings.ReplaceAll(input, " ", "_")
	default:
		logging.PrintI("Skipping space or underscore renaming conventions...")
		output = input
	}
	return output
}

// addTags handles the tagging of the video files where necessary
func addTags(renamedVideo, renamedMeta string, m *models.FileData) (string, string) {

	if len(m.FilenameMetaPrefix) > 2 {
		renamedVideo = fmt.Sprintf("%s %s", m.FilenameMetaPrefix, renamedVideo)
		renamedMeta = fmt.Sprintf("%s %s", m.FilenameMetaPrefix, renamedMeta)
	}

	if len(m.FilenameDateTag) > 2 {
		renamedVideo = fmt.Sprintf("%s %s", m.FilenameDateTag, renamedVideo)
		renamedMeta = fmt.Sprintf("%s %s", m.FilenameDateTag, renamedMeta)
	}

	return renamedVideo, renamedMeta
}

// fixContractions fixes the contractions created by FFmpeg's restrict-filenames flag
func fixContractions(videoFilename, metaFilename string, style enums.ReplaceToStyle) (string, string, error) {

	contractionsMap := make(map[string]string, len(consts.ContractionsSpaced))

	// Rename style map to use
	switch style {
	case enums.RENAMING_SPACES:
		contractionsMap = consts.ContractionsSpaced
	case enums.RENAMING_UNDERSCORES:
		contractionsMap = consts.ContractionsUnderscored
	default:
		// Skip or other unsupported parameter returns unchanged
		return videoFilename, metaFilename, nil
	}

	// Function to replace contractions in a filename
	replaceContractions := func(filename string) string {
		for contraction, replacement := range contractionsMap {
			re := regexp.MustCompile(`\b` + regexp.QuoteMeta(contraction) + `\b`)
			repIdx := re.FindStringIndex(strings.ToLower(filename))
			if repIdx == nil {
				continue
			}
			originalContraction := filename[repIdx[0]:repIdx[1]]
			restoredReplacement := ""

			// Match original case for each character in the replacement
			for i, char := range replacement {
				if i < len(originalContraction) && unicode.IsUpper(rune(originalContraction[i])) {
					restoredReplacement += strings.ToUpper(string(char))
				} else {
					restoredReplacement += string(char)
				}
			}
			// Replace in filename with adjusted case
			filename = filename[:repIdx[0]] + restoredReplacement + filename[repIdx[1]:]
		}
		logging.PrintD(2, "Made contraction replacements for file '%s'", filename)
		return filename
	}
	// Replace contractions in both filenames
	videoFilename = replaceContractions(videoFilename)
	videoFilename = strings.TrimSpace(videoFilename)

	metaFilename = replaceContractions(metaFilename)
	metaFilename = strings.TrimSpace(metaFilename)

	return videoFilename, metaFilename, nil
}

// replaceSuffix applies configured suffix replacements to a filename
func replaceSuffix(filename string) string {
	suffixes, ok := config.Get(keys.FilenameReplaceSfx).([]*models.FilenameReplaceSuffix)
	if !ok || suffixes == nil {
		logging.PrintD(1, "No suffix replacements configured, keeping original filename: %q", filename)
		return filename
	}

	ext := getCompoundExtension(filename)
	baseName := strings.TrimSuffix(filename, ext)

	logging.PrintD(2, "Processing filename %q with suffixes: %v", filename, suffixes)

	for _, suffix := range suffixes {
		if strings.HasSuffix(baseName, suffix.Suffix) {
			baseName = strings.TrimSuffix(baseName, suffix.Suffix) + suffix.Replacement
			logging.PrintD(2, "Applied suffix replacement: %q -> %q", suffix.Suffix, suffix.Replacement)
		}
	}

	result := baseName + ext
	logging.PrintD(2, "Suffix replacement complete: %q -> %q", filename, result)
	return result
}

// getCompoundExtension returns compound extensions like .info.json or regular extension
func getCompoundExtension(filename string) string {
	switch {
	case strings.HasSuffix(filename, ".info.json"):
		return ".info.json"
	case strings.HasSuffix(filename, ".metadata.json"):
		return ".metadata.json"
	case strings.HasSuffix(filename, ".model.json"):
		return ".model.json"
	default:
		return filepath.Ext(filename)
	}
}
package utils

import (
	logging "Metarr/internal/utils/logging"
	"fmt"
	"net/http"
	"net/url"
	"strings"

	"github.com/browserutils/kooky"
	_ "github.com/browserutils/kooky/browser/all"
)

var (
	allStores  []kooky.CookieStore
	allCookies []*http.Cookie
)

func initializeCookies() {
	allStores = kooky.FindAllCookieStores()
	allCookies = []*http.Cookie{}
}

// GetBrowserCookies checks user browsers for cookies corresponding to
// a given URL
func getBrowserCookies(url string) ([]*http.Cookie, error) {

	baseURL, err := extractBaseDomain(url)
	if err != nil {
		return nil, fmt.Errorf("failed to extract base domain: %v", err)
	}

	// Find all cookie stores
	if allStores == nil || allCookies == nil || len(allCookies) == 0 {
		initializeCookies()
	}

	attemptedBrowsers := make(map[string]bool, len(allStores))

	for _, store := range allStores {
		browserName := store.Browser()
		logging.PrintD(2, "Attempting to read cookies from %s", browserName)
		attemptedBrowsers[browserName] = true

		cookies, err := store.ReadCookies(kooky.Valid, kooky.Domain(baseURL))
		if err != nil {
			logging.PrintD(2, "Failed to read cookies from %s: %v", browserName, err)
			continue
		}

		if len(cookies) > 0 {
			logging.PrintI("Successfully read %d cookies from %s for domain %s", len(cookies), browserName, baseURL)
			// Append to the Go http.Cookie structure
			for _, c := range cookies {
				allCookies = append(allCookies, &http.Cookie{
					Name:   c.Name,
					Value:  c.Value,
					Path:   c.Path,
					Domain: c.Domain,
					Secure: c.Secure,
				})
			}
		} else {
			logging.PrintD(2, "No cookies found for %s", browserName)
		}
	}

	// Log summary of attempted browsers
	logging.PrintI("Attempted to read cookies from the following browsers: %v", keysFromMap(attemptedBrowsers))

	if len(allCookies) == 0 {
		logging.PrintI("No cookies found for '%s', proceeding without cookies", url)
	} else {
		logging.PrintI("Found a total of %d cookies for '%s'", len(allCookies), url)
	}

	return allCookies, nil
}

// extractBaseDomain helper function to parse a domain as just it's base.
// Useful for the purpose of scraping for cookies.
func extractBaseDomain(urlString string) (string, error) {
	parsedURL, err := url.Parse(urlString)
	if err != nil {
		return "", err
	}

	parts := strings.Split(parsedURL.Hostname(), ".")
	if len(parts) > 2 {
		return strings.Join(parts[len(parts)-2:], "."), nil
	}
	return parsedURL.Hostname(), nil
}

// keysForMap helper function to get keys from a map
func keysFromMap(m map[string]bool) []string {
	keys := make([]string, 0, len(m))
	for k := range m {
		keys = append(keys, k)
	}
	return keys
}
package utils

import (
	enums "Metarr/internal/domain/enums"
	"Metarr/internal/models"
	logging "Metarr/internal/utils/logging"
	"strconv"
	"strings"
	"time"
)

// BitchuteComRules holds rules for scraping bitchute.com
var BitchuteComRules = map[enums.WebClassTags][]*models.SelectorRule{
	enums.WEBCLASS_CREDITS: {

		{Selector: "q-item__label ellipsis text-subtitle1 ellipsis", Process: strings.TrimSpace},
	},
	enums.WEBCLASS_DATE: {
		{
			Selector: "span[data-v-3c3cf957]",
			Attr:     "data-v-3c3cf957",
			Process:  BitchuteComParseDate,
		},
	},
	enums.WEBCLASS_DESCRIPTION: {

		{Selector: `meta[name="description"]`, Attr: "content", Process: strings.TrimSpace},
		{Selector: `meta[property="og:description"]`, Attr: "content", Process: strings.TrimSpace},
		{
			Selector: `meta[itemprop="name"]`,
			Attr:     "content",
			Process:  strings.TrimSpace,
		},
	},
	enums.WEBCLASS_TITLE: {
		{
			Selector: `meta[itemprop="name"]`,
			Attr:     "content",
			Process:  strings.TrimSpace,
		},
	},
}

// BitchuteComParseDate attempts to parse dates like "9 hours ago" (etc.)
func BitchuteComParseDate(date string) string {
	date = strings.TrimSpace(date)

	dateSplit := strings.Split(date, " ")

	var (
		unit  string
		digit int
		err   error
	)

	if len(dateSplit) >= 3 {
		digit, err = strconv.Atoi(dateSplit[0])
		if err != nil {
			logging.PrintE(0, "Failed to convert string to digits: %v", err)
		}
		unit = strings.TrimSuffix(strings.ToLower(dateSplit[1]), "s") // handles both "hour" and "hours"

		var duration time.Duration
		now := time.Now()

		switch unit {
		case "second":
			duration = time.Duration(digit) * time.Second
			return now.Add(-duration).Format(time.RFC3339)
		case "minute":
			duration = time.Duration(digit) * time.Minute
			return now.Add(-duration).Format(time.RFC3339)
		case "hour":
			duration = time.Duration(digit) * time.Hour
			return now.Add(-duration).Format(time.RFC3339)
		case "day":
			duration = time.Duration(digit) * time.Hour * 24
			return now.Add(-duration).Format(time.RFC3339)
		case "week":
			duration = time.Duration(digit) * time.Hour * 24 * 7
			return now.Add(-duration).Format(time.RFC3339)
		case "month":
			return now.AddDate(0, -digit, 0).Format(time.RFC3339)
		case "year":
			return now.AddDate(-digit, 0, 0).Format(time.RFC3339)
		default:
			logging.PrintE(0, "Unknown time unit: %s", unit)
			return ""
		}
	}
	logging.PrintE(0, "Wrong date length passed in")
	return ""
}
package utils

import (
	enums "Metarr/internal/domain/enums"
	"Metarr/internal/models"
	logging "Metarr/internal/utils/logging"
	"strings"

	"golang.org/x/text/cases"
	"golang.org/x/text/language"
)

var CensoredTvRules = map[enums.WebClassTags][]*models.SelectorRule{
	enums.WEBCLASS_DATE: {
		{Selector: ".main-episode-player-container p.text-muted.text-right.text-date.mb-0", Process: strings.TrimSpace},
		{Selector: ".text-date", Process: strings.TrimSpace},
	},
	enums.WEBCLASS_DESCRIPTION: {
		{Selector: ".p-3 check-for-urls", Process: strings.TrimSpace},
		{Selector: `meta[name="description"]`, Attr: "content", Process: strings.TrimSpace},
	},
	enums.WEBCLASS_TITLE: {
		{Selector: ".p-3 h4", Process: strings.TrimSpace},
		{Selector: "[title]", Attr: "title", Process: strings.TrimSpace},
	},
}

// censoredTvChannelName gets the channel name from the URL string
func CensoredTvChannelName(url string) string {
	if url == "" {
		logging.PrintE(0, "url passed in empty")
		return ""
	}
	urlSplit := strings.Split(url, "/")

	var channel string
	for i, seg := range urlSplit {
		if strings.HasSuffix(seg, "shows") && len(urlSplit) > i+1 {
			channel = urlSplit[i+1]
		}
	}

	if channel == "" {
		logging.PrintE(0, "failed to fill channel name from url, out of bounds?")
	}
	channel = strings.ReplaceAll(channel, "-", " ")

	caser := cases.Title(language.English)
	channel = caser.String(channel)

	if strings.ToLower(channel) == "atheism is unstoppable" {
		channel = "Atheism-is-Unstoppable"
	}
	return channel
}
package utils

import (
	enums "Metarr/internal/domain/enums"
	"Metarr/internal/models"
	"strings"
)

// OdyseeComRules holds rules for scraping odysee.com
var OdyseeComRules = map[enums.WebClassTags][]*models.SelectorRule{
	enums.WEBCLASS_CREDITS: {
		{
			Selector: "script[type='application/ld+json']",
			JsonPath: []string{"author", "name"},
			Process:  strings.TrimSpace,
		},
	},
	enums.WEBCLASS_DATE: {
		{
			Selector: "script[type='application/ld+json']",
			JsonPath: []string{"uploadDate"},
			Process:  strings.TrimSpace,
		},
		{Selector: `meta[property="og:video:release_date"]`, Attr: "content", Process: strings.TrimSpace},
	},
	enums.WEBCLASS_DESCRIPTION: {
		{
			Selector: "script[type='application/ld+json']",
			JsonPath: []string{"description"},
			Process:  strings.TrimSpace,
		},
		{Selector: `meta[name="description"]`, Attr: "content", Process: strings.TrimSpace},
		{Selector: `meta[property="og:description"]`, Attr: "content", Process: strings.TrimSpace},
	},
	enums.WEBCLASS_TITLE: {

		{Selector: "title", Process: strings.TrimSpace},
		{
			Selector: "script[type='application/ld+json']",
			JsonPath: []string{"name"},
			Process:  strings.TrimSpace,
		},
	},
}
package utils

import (
	enums "Metarr/internal/domain/enums"
	"Metarr/internal/models"
	"strings"
)

// RumbleComRules holds rules for scraping rumble.com
var RumbleComRules = map[enums.WebClassTags][]*models.SelectorRule{
	enums.WEBCLASS_CREDITS: {

		{Selector: ".media-subscribe-and-notify", Attr: "data-title", Process: strings.TrimSpace},
		{Selector: ".media-by--a .media-heading-name", Process: strings.TrimSpace},
	},
	enums.WEBCLASS_DATE: {
		{Selector: "time", Attr: "datetime", Process: strings.TrimSpace},
		{
			Selector: "script[type='application/ld+json']",
			JsonPath: []string{"uploadDate"},
			Process:  strings.TrimSpace,
		},
	},
	enums.WEBCLASS_DESCRIPTION: {
		{
			Selector: "script[type='application/ld+json']",
			JsonPath: []string{"description"},
			Process:  strings.TrimSpace,
		},
		{Selector: `meta[name="description"]`, Attr: "content", Process: strings.TrimSpace},
		{Selector: `meta[property="og:description"]`, Attr: "content", Process: strings.TrimSpace},
	},
	enums.WEBCLASS_TITLE: {

		{Selector: "title", Process: strings.TrimSpace},
		{
			Selector: "script[type='application/ld+json']",
			JsonPath: []string{"name"},
			Process:  strings.TrimSpace,
		},
	},
}
package utils

import (
	consts "Metarr/internal/domain/constants"
	enums "Metarr/internal/domain/enums"
	"Metarr/internal/models"
	presets "Metarr/internal/utils/browser/presets"
	logging "Metarr/internal/utils/logging"
	"encoding/json"
	"fmt"
	"net/http"
	"regexp"
	"strings"
	"time"

	"github.com/gocolly/colly"
)

// scrapeMeta gets cookies for a given URL and returns a grabbed string
func ScrapeMeta(w *models.MetadataWebData, find enums.WebClassTags) string {

	var (
		err  error
		data string
	)

	w.Cookies, err = getBrowserCookies(w.WebpageURL)
	if err != nil {
		logging.PrintE(2, "Was unable to grab browser cookies: %v", err)
	}
	for _, try := range w.TryURLs {
		data, err = scrape(try, w.Cookies, find, false)
		if err != nil {
			logging.PrintE(0, "Failed to scrape '%s' for requested metadata: %v", try, err)
		} else {
			break
		}
	}
	return data
}

// scrape searches relevant URLs to try and fill missing metadata
func scrape(url string, cookies []*http.Cookie, tag enums.WebClassTags, skipPresets bool) (string, error) {

	var (
		result      string
		scrapeError error
		custom      bool
	)

	// Initialize the collector
	c := colly.NewCollector(
		colly.AllowURLRevisit(),
		colly.MaxDepth(1),
		colly.Async(true),
	)
	c.SetRequestTimeout(15 * time.Second)

	if len(cookies) > 0 {
		c.SetCookies(url, cookies)
	}

	// Define preset scraping rules if the URL matches a known pattern
	switch {
	case strings.Contains(url, "bitchute.com") && !skipPresets:

		custom = true
		logging.PrintI("Using bitchute.com preset scraper")
		setupPresetScraping(c, tag, presets.BitchuteComRules, &result, url)

	case strings.Contains(url, "censored.tv") && !skipPresets:

		custom = true
		logging.PrintI("Using censored.tv preset scraper")
		if tag == enums.WEBCLASS_CREDITS {
			return presets.CensoredTvChannelName(url), nil
		}
		setupPresetScraping(c, tag, presets.CensoredTvRules, &result, url)

	case strings.Contains(url, "rumble.com") && !skipPresets:

		custom = true
		logging.PrintI("Using rumble.com preset scraper")
		setupPresetScraping(c, tag, presets.RumbleComRules, &result, url)

	case strings.Contains(url, "odysee.com") && !skipPresets:

		custom = true
		logging.PrintI("Using odysee.com preset scraper")
		setupPresetScraping(c, tag, presets.OdyseeComRules, &result, url)

	default:
		logging.PrintI("Generic scrape attempt...")
		setupGenericScraping(c, tag, &result, url)
	}

	// Error handler
	c.OnError(func(r *colly.Response, err error) {
		scrapeError = fmt.Errorf("failed to scrape %s: %v", r.Request.URL, err)
	})

	// Attempt visit and wait for async scraping
	if err := c.Visit(url); err != nil {
		return "", fmt.Errorf("unable to visit given web page")
	}
	c.Wait()

	if scrapeError != nil {
		switch result {
		case "":
			return "", scrapeError
		default:
			logging.PrintE(0, "Error during scrape (%v) but got result anyway. Returning result '%s'...", scrapeError, result)
			return result, nil
		}
	}

	// If custom preset was used and failed, try again with default
	if result == "" && custom {
		return scrape(url, cookies, tag, true)
	}

	return result, nil
}

// setupPresetScraping applies specific scraping rules for known sites
func setupPresetScraping(c *colly.Collector, tag enums.WebClassTags, rules map[enums.WebClassTags][]*models.SelectorRule, result *string, url string) {
	if result == nil {
		return
	}
	if ruleSet, exists := rules[tag]; exists {
		for _, rule := range ruleSet {
			c.OnHTML(rule.Selector, func(h *colly.HTMLElement) {
				if *result != "" {
					return
				}
				var value string
				if len(rule.JsonPath) > 0 {
					if jsonVal, err := jsonExtractor([]byte(h.Text), rule.JsonPath); err == nil {
						value = jsonVal
					}
				} else if rule.Attr != "" {
					value = h.Attr(rule.Attr)
				} else {
					value = h.Text
				}

				if value != "" {
					logging.PrintS(0, "Grabbed value '%s' for URL '%s' using preset scraper", value, url)
					*result = rule.Process(value)
				}
			})
		}
	}
}

// setupGenericScraping defines a generic scraping approach for non-preset sites
func setupGenericScraping(c *colly.Collector, tag enums.WebClassTags, result *string, url string) {
	if result == nil {
		return
	}

	var tags []string

	// Determine the appropriate tags based on the metadata being fetched
	switch tag {
	case enums.WEBCLASS_DATE:
		tags = consts.WebDateTags[:]
	case enums.WEBCLASS_DESCRIPTION:
		tags = consts.WebDescriptionTags[:]
	case enums.WEBCLASS_CREDITS:
		tags = consts.WebCreditsTags[:]
	case enums.WEBCLASS_TITLE:
		tags = consts.WebTitleTags[:]
	default:
		return
	}

	// Set up the HTML scraper for each tag
	c.OnHTML("*", func(e *colly.HTMLElement) {
		if *result != "" {
			return
		}

		classAttr := strings.ToLower(e.Attr("class"))
		idAttr := strings.ToLower(e.Attr("id"))
		text := strings.TrimSpace(e.Text)

		if classAttr != "" {
			logging.PrintD(2, "Checking element with class: '%s'", classAttr)
		}

		for _, t := range tags {
			if (e.Name == "p" && strings.Contains(idAttr, t)) ||
				strings.Contains(classAttr, t) ||
				strings.Contains(idAttr, t) {

				if tag == enums.WEBCLASS_DATE && !looksLikeDate(text) {
					continue
				}

				*result = text
				logging.PrintI("Found '%s' in element with class '%s' and id '%s' for URL '%s'",
					result, classAttr, idAttr, url)
				return
			}
		}
	})
}

// jsonExtractor helps extract values from nested JSON structures
func jsonExtractor(data []byte, path []string) (string, error) {
	var result map[string]interface{}
	if err := json.Unmarshal(data, &result); err != nil {
		return "", err
	}
	current := result
	for _, key := range path[:len(path)-1] {
		if next, ok := current[key].(map[string]interface{}); ok {
			current = next
		} else {
			return "", fmt.Errorf("invalid JSON path at %s", key)
		}
	}
	if val, ok := current[path[len(path)-1]].(string); ok {
		return val, nil
	}
	return "", fmt.Errorf("value at path is not a string")
}

// looksLikeDate validates if the text appears to be a date
func looksLikeDate(text string) bool {
	text = strings.TrimSpace(strings.ToLower(text))

	// Common date patterns
	datePatterns := []string{
		`\d{4}-\d{2}-\d{2}`,       // YYYY-MM-DD
		`\d{1,2}/\d{1,2}/\d{2,4}`, // M/D/YY or MM/DD/YYYY
		`(?i)(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\s+\d{1,2},?\s+\d{4}`, // Month DD, YYYY
	}

	for _, pattern := range datePatterns {
		matched, err := regexp.MatchString(pattern, text)
		if err == nil && matched {
			return true
		}
	}

	// Additional date indicators
	dateIndicators := []string{"uploaded", "published", "created", "date:", "on"}
	for _, indicator := range dateIndicators {
		if strings.Contains(text, indicator) {
			return true
		}
	}

	return false
}
package utils

import (
	consts "Metarr/internal/domain/constants"
	logging "Metarr/internal/utils/logging"
	"fmt"
	"io"
	"os"
	"path/filepath"
	"strings"
)

// createBackup creates a backup copy of the original file before modifying it.
func BackupFile(file *os.File) error {

	originalFilePath := file.Name()

	backupFilePath := generateBackupFilename(originalFilePath)
	logging.PrintD(3, "Creating backup of file '%s' as '%s'", originalFilePath, backupFilePath)

	// Open the backup file for writing
	backupFile, err := os.Create(backupFilePath)
	if err != nil {
		return fmt.Errorf("failed to create backup file: %w", err)
	}
	defer backupFile.Close()

	// Seek to the beginning of the original file (not the backup file)
	_, err = file.Seek(0, io.SeekStart)
	if err != nil {
		return fmt.Errorf("failed to seek to beginning of original file: %w", err)
	}

	// Copy the content of the original file to the backup file
	buf := make([]byte, 32*1024)
	_, err = io.CopyBuffer(backupFile, file, buf)
	if err != nil {
		return fmt.Errorf("failed to copy content to backup file: %w", err)
	}

	logging.PrintD(3, "Backup successfully created at '%s'", backupFilePath)
	return nil
}

// generateBackupFilename creates a backup filename by appending "_backup" to the original filename
func generateBackupFilename(originalFilePath string) string {
	ext := filepath.Ext(originalFilePath)
	base := strings.TrimSuffix(originalFilePath, ext)
	return fmt.Sprintf(base + consts.OldTag + ext)
}

// RenameToBackup renames the passed in file
func RenameToBackup(filename string) error {

	if filename == "" {
		logging.PrintE(0, "filename was passed in to backup empty")
	}

	backupName := generateBackupFilename(filename)

	if err := os.Rename(filename, backupName); err != nil {
		return fmt.Errorf("failed to backup filename '%s' to '%s'", filename, backupName)
	}
	return nil
}
package utils

import (
	"Metarr/internal/config"
	consts "Metarr/internal/domain/constants"
	enums "Metarr/internal/domain/enums"
	keys "Metarr/internal/domain/keys"
	"Metarr/internal/models"
	logging "Metarr/internal/utils/logging"
	"fmt"
	"os"
	"path/filepath"
	"regexp"
	"strings"
)

// Variable cache
var (
	videoExtensions,
	metaExtensions,
	inputPrefixes []string
)

// InitFetchFilesVars sets up the cached variables to be used in file fetching ops
func InitFetchFilesVars() error {

	if inVExts, ok := config.Get(keys.InputVExtsEnum).([]enums.ConvertFromFiletype); ok {
		logging.PrintD(2, "Received video extensions enum: %v", inVExts)
		videoExtensions = setVideoExtensions(inVExts)
	} else {
		return fmt.Errorf("wrong type sent in. Received type %T", inVExts)
	}

	if inMExts, ok := config.Get(keys.InputMExtsEnum).([]enums.MetaFiletypeFilter); ok {
		logging.PrintD(2, "Received video extensions enum: %v", inMExts)
		metaExtensions = setMetaExtensions(inMExts)
	} else {
		return fmt.Errorf("wrong type sent in. Received type %T", inMExts)
	}

	inputPrefixes = SetPrefixFilter(config.GetStringSlice(keys.FilePrefixes))
	logging.PrintD(2, "Setting prefix filter: %v", inputPrefixes)

	return nil
}

// GetVideoFiles fetches video files from a directory
func GetVideoFiles(videoDir *os.File) (map[string]*models.FileData, error) {
	files, err := videoDir.ReadDir(-1)
	if err != nil {
		return nil, fmt.Errorf("error reading video directory: %w", err)
	}

	logging.Print("\n\nFiltering directory '%s':\n\nFile extensions: %v\nFile prefixes: %v\n\n", videoDir.Name(), videoExtensions, inputPrefixes)

	videoFiles := make(map[string]*models.FileData, len(files))

	for _, file := range files {
		if !file.IsDir() && HasFileExtension(file.Name(), videoExtensions) {

			if config.IsSet(keys.FilePrefixes) {
				if !HasPrefix(file.Name(), inputPrefixes) {
					continue
				}
			}
			filenameBase := filepath.Base(file.Name())

			m := models.NewFileData()
			m.OriginalVideoPath = filepath.Join(videoDir.Name(), file.Name())
			m.OriginalVideoBaseName = strings.TrimSuffix(filenameBase, filepath.Ext(file.Name()))
			m.VideoDirectory = videoDir.Name()

			if !strings.HasSuffix(m.OriginalVideoBaseName, consts.OldTag) {
				videoFiles[file.Name()] = m
				logging.PrintI("Added video to queue: %v", filenameBase)
			} else {
				logging.PrintI("Skipping file '%s' containing backup tag ('%s')", m.OriginalVideoBaseName, consts.OldTag)
			}
		}
	}

	if len(videoFiles) == 0 {
		return nil, fmt.Errorf("no video files with extensions: %v and prefixes: %v found in directory: %s", videoExtensions, inputPrefixes, videoDir.Name())
	}
	return videoFiles, nil
}

// GetMetadataFiles fetches metadata files from a directory
func GetMetadataFiles(metaDir *os.File) (map[string]*models.FileData, error) {
	files, err := metaDir.ReadDir(-1)
	if err != nil {
		return nil, fmt.Errorf("error reading metadata directory: %w", err)
	}

	metaFiles := make(map[string]*models.FileData, len(files))

	for _, file := range files {
		if !file.IsDir() {
			ext := filepath.Ext(file.Name())

			logging.PrintD(3, "Checking file '%s' with extension '%s'", file.Name(), ext)

			if config.IsSet(keys.FilePrefixes) {
				if !HasPrefix(file.Name(), inputPrefixes) {
					continue
				}
			}

			var match bool
			for _, mExt := range metaExtensions {
				if ext != mExt {
					logging.PrintD(3, "Extension '%s' does not match '%s'", ext, mExt)
					continue
				}
				logging.PrintS(3, "Extension '%s' matches input meta extensions '%s'", ext, mExt)
				match = true
				break
			}
			if !match {
				continue
			}

			filenameBase := filepath.Base(file.Name())
			baseName := strings.TrimSuffix(filenameBase, ext)

			m := models.NewFileData()
			filePath := filepath.Join(metaDir.Name(), file.Name())

			switch ext {
			case ".json":
				logging.PrintD(1, "Detected JSON file '%s'", file.Name())
				m.JSONFilePath = filePath
				m.JSONBaseName = baseName
				m.JSONDirectory = metaDir.Name()
				m.MetaFileType = enums.METAFILE_JSON

			case ".nfo":
				logging.PrintD(1, "Detected NFO file '%s'", file.Name())
				m.NFOFilePath = filePath
				m.NFOBaseName = baseName
				m.NFODirectory = metaDir.Name()
				m.MetaFileType = enums.METAFILE_NFO
			}

			if !strings.Contains(baseName, consts.OldTag) {
				metaFiles[file.Name()] = m
			} else {
				logging.PrintI("Skipping file '%s' containing backup tag ('%s')", baseName, consts.OldTag)
			}
		}
	}

	if len(metaFiles) == 0 {
		return nil, fmt.Errorf("no meta files with extensions: %v and prefixes: %v found in directory: %s", metaExtensions, inputPrefixes, metaDir.Name())
	}

	logging.PrintD(3, "Returning meta files %v", metaFiles)
	return metaFiles, nil
}

// GetSingleVideoFile handles a single video file
func GetSingleVideoFile(videoFile *os.File) (map[string]*models.FileData, error) {
	videoMap := make(map[string]*models.FileData, 1)
	filename := filepath.Base(videoFile.Name())

	videoData := models.NewFileData()
	videoData.OriginalVideoPath = videoFile.Name()
	videoData.OriginalVideoBaseName = strings.TrimSuffix(filename, filepath.Ext(filename))
	videoData.VideoDirectory = filepath.Dir(videoFile.Name())
	videoData.VideoFile = videoFile

	logging.PrintD(3, "Created video file data for single file: %s", filename)

	videoMap[filename] = videoData
	return videoMap, nil
}

// GetSingleMetadataFile handles a single metadata file
func GetSingleMetadataFile(metaFile *os.File) (map[string]*models.FileData, error) {
	metaMap := make(map[string]*models.FileData, 1)
	filename := filepath.Base(metaFile.Name())

	fileData := models.NewFileData()
	ext := filepath.Ext(metaFile.Name())

	switch ext {
	case ".json":
		fileData.MetaFileType = enums.METAFILE_JSON
		fileData.JSONFilePath = metaFile.Name()
		fileData.JSONBaseName = strings.TrimSuffix(filename, ext)
		fileData.JSONDirectory = filepath.Dir(metaFile.Name())
		logging.PrintD(3, "Created JSON metadata file data for single file: %s", filename)

	case ".nfo":
		fileData.MetaFileType = enums.METAFILE_NFO
		fileData.NFOFilePath = metaFile.Name()
		fileData.NFOBaseName = strings.TrimSuffix(filename, ext)
		fileData.NFODirectory = filepath.Dir(metaFile.Name())
		logging.PrintD(3, "Created NFO metadata file data for single file: %s", filename)

	default:
		return nil, fmt.Errorf("unsupported metadata file type: %s", ext)
	}

	metaMap[filename] = fileData
	return metaMap, nil
}

// MatchVideoWithMetadata matches video files with their corresponding metadata files
func MatchVideoWithMetadata(videoFiles, metaFiles map[string]*models.FileData) (map[string]*models.FileData, error) {
	logging.PrintD(3, "Entering metadata and video file matching loop...")

	matchedFiles := make(map[string]*models.FileData, len(videoFiles))

	specialChars := regexp.MustCompile(`[^\w\s-]`)
	extraSpaces := regexp.MustCompile(`\s+`)

	// Pre-process metaFiles into a lookup map
	metaLookup := make(map[string]*models.FileData, len(metaFiles))
	for metaName, metaData := range metaFiles {
		baseKey := NormalizeFilename(TrimMetafileSuffixes(metaName, ""), specialChars, extraSpaces)
		metaLookup[baseKey] = metaData
	}

	for videoName := range videoFiles {
		videoBase := strings.TrimSuffix(videoName, filepath.Ext(videoName))
		normalizedVideoBase := NormalizeFilename(videoBase, specialChars, extraSpaces)

		if metaData, exists := metaLookup[normalizedVideoBase]; exists { // This checks if the key exists in the metaLookup map
			matchedFiles[videoName] = videoFiles[videoName]
			matchedFiles[videoName].MetaFileType = metaData.MetaFileType

			switch metaData.MetaFileType {
			case enums.METAFILE_JSON:
				matchedFiles[videoName].JSONFilePath = metaData.JSONFilePath
				matchedFiles[videoName].JSONBaseName = metaData.JSONBaseName
				matchedFiles[videoName].JSONDirectory = metaData.JSONDirectory

			case enums.METAFILE_NFO:
				matchedFiles[videoName].NFOFilePath = metaData.NFOFilePath
				matchedFiles[videoName].NFOBaseName = metaData.NFOBaseName
				matchedFiles[videoName].NFODirectory = metaData.NFODirectory
			}
		}
	}

	if len(matchedFiles) == 0 {
		return nil, fmt.Errorf("no matching metadata files found for any videos")
	}

	return matchedFiles, nil
}
package utils

import (
	consts "Metarr/internal/domain/constants"
	enums "Metarr/internal/domain/enums"
	logging "Metarr/internal/utils/logging"
	"os"
	"path/filepath"
	"regexp"
	"strings"
)

// hasVideoExtension checks if the file has a valid video extension
func HasFileExtension(fileName string, extensions []string) bool {

	if extensions == nil {
		logging.PrintE(0, "No extensions picked.")
		return false
	}

	for _, ext := range extensions {
		if strings.HasSuffix(strings.ToLower(fileName), strings.ToLower(ext)) {
			return true
		}
	}

	// No matches
	return false
}

// hasPrefix determines if the input file has the desired prefix
func HasPrefix(fileName string, prefixes []string) bool {

	if prefixes == nil {
		prefixes = append(prefixes, "")
	}

	for _, data := range prefixes {
		if strings.HasPrefix(strings.ToLower(fileName), strings.ToLower(data)) {
			return true
		}
	}

	// No matches
	return false
}

// setVideoExtensions creates a list of extensions to filter
func setVideoExtensions(exts []enums.ConvertFromFiletype) []string {

	videoExtensions := make([]string, 0, len(consts.AllVidExtensions))

	for _, arg := range exts {
		switch arg {
		case enums.VID_EXTS_MKV:
			videoExtensions = append(videoExtensions, ".mkv")
		case enums.VID_EXTS_MP4:
			videoExtensions = append(videoExtensions, ".mp4")
		case enums.VID_EXTS_WEBM:
			videoExtensions = append(videoExtensions, ".webm")
		case enums.VID_EXTS_ALL:
			return consts.AllVidExtensions[:]
		}
	}

	if len(videoExtensions) == 0 {
		return consts.AllVidExtensions[:]
	}

	return videoExtensions
}

// setMetaExtensions creates a lists of meta extensions to filter
func setMetaExtensions(exts []enums.MetaFiletypeFilter) []string {

	metaExtensions := make([]string, 0, len(consts.AllMetaExtensions))

	for _, arg := range exts {
		switch arg {
		case enums.META_EXTS_JSON:
			metaExtensions = append(metaExtensions, ".json")
		case enums.META_EXTS_NFO:
			metaExtensions = append(metaExtensions, ".nfo")
		case enums.META_EXTS_ALL:
			return consts.AllMetaExtensions[:]
		}
	}

	if len(metaExtensions) == 0 {
		return consts.AllMetaExtensions[:]
	}

	return metaExtensions
}

// setPrefixFilter sets a list of prefixes to filter
func SetPrefixFilter(inputPrefixFilters []string) []string {

	prefixFilters := make([]string, 0, len(inputPrefixFilters))
	prefixFilters = append(prefixFilters, inputPrefixFilters...)

	return prefixFilters
}

// GetDirStats returns the number of video or metadata files in a directory, so maps/slices can be suitable sized
func GetDirStats(dir string) (vidCount, metaCount int) {
	// Quick initial scan just counting files, not storing anything
	entries, err := os.ReadDir(dir)
	if err != nil {
		return 0, 0
	}
	for _, entry := range entries {
		if !entry.IsDir() {
			ext := strings.ToLower(filepath.Ext(entry.Name()))

			for _, entry := range consts.AllVidExtensions {
				if ext == entry {
					vidCount++
					continue
				}
				switch ext {
				case ".json", ".nfo":
					metaCount++
					continue
				}
			}
		}
	}
	return vidCount, metaCount
}

// normalizeFilename removes special characters and normalizes spacing
func NormalizeFilename(filename string, specialChars, extraSpaces *regexp.Regexp) string {

	normalized := strings.ToLower(filename)
	normalized = specialChars.ReplaceAllString(normalized, "")
	normalized = extraSpaces.ReplaceAllString(normalized, " ")
	normalized = strings.TrimSpace(normalized)

	return normalized
}

// trimJsonSuffixes normalizes away common json string suffixes
// e.g. ".info" for yt-dlp outputted JSON files
func TrimMetafileSuffixes(metaBase, videoBase string) string {

	switch {

	case strings.HasSuffix(metaBase, ".info.json"): // FFmpeg
		if !strings.HasSuffix(videoBase, ".info") {
			metaBase = strings.TrimSuffix(metaBase, ".info.json")
		} else {
			metaBase = strings.TrimSuffix(metaBase, ".json")
		}

	case strings.HasSuffix(metaBase, ".metadata.json"): // Angular
		if !strings.HasSuffix(videoBase, ".metadata") {
			metaBase = strings.TrimSuffix(metaBase, ".metadata.json")
		} else {
			metaBase = strings.TrimSuffix(metaBase, ".json")
		}

	case strings.HasSuffix(metaBase, ".model.json"):
		if !strings.HasSuffix(videoBase, ".model") {
			metaBase = strings.TrimSuffix(metaBase, ".model.json")
		} else {
			metaBase = strings.TrimSuffix(metaBase, ".json")
		}

	case strings.HasSuffix(metaBase, ".manifest.cdfd.json"):
		if !strings.HasSuffix(videoBase, ".manifest.cdm") {
			metaBase = strings.TrimSuffix(metaBase, ".manifest.cdfd.json")
		} else {
			metaBase = strings.TrimSuffix(metaBase, ".json")
		}

	default:
		switch {
		case !strings.HasSuffix(videoBase, ".json"): // Edge cases where metafile extension is in the suffix of the video file
			metaBase = strings.TrimSuffix(metaBase, ".json")
		case !strings.HasSuffix(videoBase, ".nfo"):
			metaBase = strings.TrimSuffix(metaBase, ".nfo")
		default:
			logging.PrintD(1, "Common suffix not found for metafile (%s)", metaBase)
		}
	}
	return metaBase
}
package utils

import (
	"Metarr/internal/config"
	keys "Metarr/internal/domain/keys"
	logging "Metarr/internal/utils/logging"
	"bufio"
	"bytes"
	"fmt"
	"io"
	"os"
	"path/filepath"
	"strings"
	"sync"
)

type FSFileWriter struct {
	SkipVids  bool
	DestVideo string
	SrcVideo  string
	DestMeta  string
	SrcMeta   string
	muFs      sync.RWMutex
}

func NewFSFileWriter(skipVids bool, destVideo, srcVideo, destMeta, srcMeta string) *FSFileWriter {
	same := 0
	if destVideo != srcVideo {
		same++
	}
	if destMeta != srcMeta {
		same++
	}

	logging.PrintD(2, "Made FSFileWriter with parameters:\n\nSkip videos? %v\n\nOriginal Video: %s\nRenamed Video:  %s\n\nOriginal Metafile: %s\nRenamed Metafile:  %s\n\n%d file names will be changed...\n\n",
		skipVids, srcVideo, destVideo, srcMeta, destMeta, same)
	return &FSFileWriter{
		SkipVids:  skipVids,
		DestVideo: destVideo,
		SrcVideo:  srcVideo,
		DestMeta:  destMeta,
		SrcMeta:   srcMeta,
	}
}

// WriteResults executes the final commands to write the transformed files
func (fs *FSFileWriter) WriteResults() error {
	fs.muFs.Lock()
	defer fs.muFs.Unlock()

	if !fs.SkipVids {
		if fs.SrcVideo != fs.DestVideo && fs.SrcVideo != "" && fs.DestVideo != "" {
			if err := os.Rename(fs.SrcVideo, fs.DestVideo); err != nil {
				return fmt.Errorf("failed to rename %s to %s. error: %v", fs.SrcVideo, fs.DestVideo, err)
			}
		}
	}
	if fs.SrcMeta != fs.DestMeta && fs.SrcMeta != "" && fs.DestMeta != "" {
		if err := os.Rename(fs.SrcMeta, fs.DestMeta); err != nil {
			return fmt.Errorf("failed to rename %s to %s. error: %v", fs.SrcMeta, fs.DestMeta, err)
		}
	}

	logging.PrintD(1, "Rename function final commands:\n\nVideo: Replacing '%v' with '%v'\nMetafile: Replacing '%v' with '%v'\n\n", fs.SrcVideo, fs.DestVideo,
		fs.SrcMeta, fs.DestMeta)
	return nil
}

// MoveFile moves files to specified location
func (fs *FSFileWriter) MoveFile() error {
	fs.muFs.Lock()
	defer fs.muFs.Unlock()

	if !config.IsSet(keys.MoveOnComplete) {
		return nil
	}

	if fs.DestVideo == "" && fs.DestMeta == "" {
		return fmt.Errorf("video and metafile source strings both empty")
	}

	dst := config.GetString(keys.MoveOnComplete)
	dst = filepath.Clean(dst)

	// Check destination directory
	check, err := os.Stat(dst)
	if err != nil {
		return fmt.Errorf("unable to stat destination folder '%s': %w", dst, err)
	}
	if !check.IsDir() {
		return fmt.Errorf("destination path must be a folder: '%s'", dst)
	}

	// Move/copy video and metadata file
	if fs.DestVideo != "" {
		destVBase := filepath.Base(fs.DestVideo)
		destVTarget := filepath.Join(dst, destVBase)
		if err := fs.moveOrCopyFile(fs.DestVideo, destVTarget); err != nil {
			return fmt.Errorf("failed to move video file: %w", err)
		}
	}
	if fs.DestMeta != "" {
		destMBase := filepath.Base(fs.DestMeta)
		destMTarget := filepath.Join(dst, destMBase)
		if err := fs.moveOrCopyFile(fs.DestMeta, destMTarget); err != nil {
			return fmt.Errorf("failed to move metadata file: %w", err)
		}
	}
	return nil
}

// copyFile copies a file to a target destination
func (fs *FSFileWriter) copyFile(src, dst string) error {
	src = filepath.Clean(src)
	dst = filepath.Clean(dst)

	if src == dst {
		return fmt.Errorf("entered source file '%s' and destination '%s' file as the same name and same path", src, dst)
	}

	logging.PrintI("Copying:\n'%s'\nto\n'%s'...", src, dst)

	// Validate source file
	sourceInfo, err := os.Stat(src)
	if err != nil {
		return fmt.Errorf("failed to stat source file: %w", err)
	}
	if !sourceInfo.Mode().IsRegular() {
		return fmt.Errorf("source is not a regular file: %s", src)
	}
	if sourceInfo.Size() == 0 {
		return fmt.Errorf("source file is empty: %s", src)
	}

	// Check destination
	if destInfo, err := os.Stat(dst); err == nil {
		if os.SameFile(sourceInfo, destInfo) {
			return nil // Same file
		}
		return fmt.Errorf("aborting move, destination file '%s' is equal to source file '%s'", dst, src)
	} else if !os.IsNotExist(err) {
		return fmt.Errorf("error checking destination file: %w", err)
	}

	// Ensure destination directory exists
	if err := os.MkdirAll(filepath.Dir(dst), 0755); err != nil {
		return fmt.Errorf("failed to create destination directory: %w", err)
	}

	// Open source file
	sourceFile, err := os.Open(src)
	if err != nil {
		return fmt.Errorf("failed to open source file: %w", err)
	}
	defer sourceFile.Close()

	// Create destination file
	destFile, err := os.Create(dst)
	if err != nil {
		return fmt.Errorf("failed to create destination file, do you have adequate permissions on the destination folder?: %w", err)
	}
	defer func() {
		destFile.Close()
		if err != nil {
			os.Remove(dst) // Clean up on error
		}
	}()

	// Copy contents with buffer
	bufferedSource := bufio.NewReaderSize(sourceFile, 4*1024*1024) // 4MB: 1024 * 1024 is 1 MB
	bufferedDest := bufio.NewWriterSize(destFile, 4*1024*1024)
	defer bufferedDest.Flush()

	buf := make([]byte, 4*1024*1024)

	if _, err = io.CopyBuffer(bufferedDest, bufferedSource, buf); err != nil {
		return fmt.Errorf("failed to copy file contents: %w", err)
	}

	// Sync to ensure write is complete
	if err = destFile.Sync(); err != nil {
		return fmt.Errorf("failed to sync destination file: %w", err)
	}

	// Set same permissions as source
	if err = os.Chmod(dst, sourceInfo.Mode()); err != nil {
		logging.PrintI("Failed to set file permissions, is destination folder remote? (%v)", err)
	}

	// Verify destination file
	check, err := destFile.Stat()
	if err != nil {
		return fmt.Errorf("error statting destination file: %w", err)
	}
	if check.Size() != sourceInfo.Size() {
		return fmt.Errorf("destination file size (%d) does not match source size (%d)",
			check.Size(), sourceInfo.Size())
	}
	return nil
}

// moveOrCopyFile attempts rename first, falls back to copy+delete for cross-device moves
func (fs *FSFileWriter) moveOrCopyFile(src, dst string) error {
	src = filepath.Clean(src)
	dst = filepath.Clean(dst)

	if src == dst {
		return nil // Same file, nothing to do
	}

	srcHash, err := fs.calculateFileHash(src)
	if err != nil {
		return fmt.Errorf("failed to calculate initial source hash: %w", err)
	}

	// Try rename (pure move) first
	err = os.Rename(src, dst)
	if err == nil {
		dstHash, verifyErr := fs.calculateFileHash(dst)
		if verifyErr != nil {
			return fmt.Errorf("move verification failed: %w", verifyErr)
		}
		if !bytes.Equal(srcHash, dstHash) {
			return fmt.Errorf("hash mismatch after move")
		}
		return nil
	}

	// If cross-device error, fall back to copy+delete
	if strings.Contains(err.Error(), "invalid cross-device link") {
		logging.PrintD(1, "Falling back to copy for moving %s to %s", src, dst)

		// Copy the file
		if err := fs.copyFile(src, dst); err != nil {
			os.Remove(dst)
			return fmt.Errorf("failed to copy file: %w", err)
		}

		// Verify copy with hash comparison
		dstHash, verifyErr := fs.calculateFileHash(dst)
		if verifyErr != nil {
			os.Remove(dst)
			return fmt.Errorf("copy verification failed: %w", verifyErr)
		}
		if !bytes.Equal(srcHash, dstHash) {
			os.Remove(dst)
			return fmt.Errorf("hash mismatch after copy")
		}

		// Remove source after successful copy and verification
		if err := os.Remove(src); err != nil {
			logging.PrintE(0, "Failed to remove source file after verified copy: %v", err)
			// Operation successful, do not return error, just log the error
		}
		return nil
	}
	return fmt.Errorf("failed to move file: %w", err)
}
package utils

import (
	"bufio"
	"crypto/sha256"
	"fmt"
	"io"
	"os"
)

// calculateFileHash computes SHA-256 hash of a file
func (fs *FSFileWriter) calculateFileHash(filepath string) ([]byte, error) {
	file, err := os.Open(filepath)
	if err != nil {
		return nil, fmt.Errorf("failed to open file for hashing: %w", err)
	}
	defer file.Close()

	hash := sha256.New()
	buf := make([]byte, 4*1024*1024) // 4MB buffer
	reader := bufio.NewReaderSize(file, 4*1024*1024)

	for {
		n, err := reader.Read(buf)
		if n > 0 {
			if _, err := hash.Write(buf[:n]); err != nil {
				return nil, fmt.Errorf("error writing to hash: %w", err)
			}
		}
		if err == io.EOF {
			break
		}
		if err != nil {
			return nil, fmt.Errorf("error reading file for hash: %w", err)
		}
	}

	return hash.Sum(nil), nil
}
package utils

import (
	consts "Metarr/internal/domain/constants"
	keys "Metarr/internal/domain/keys"
	"fmt"
	"path/filepath"
	"runtime"
	"sync"

	"github.com/spf13/viper"
)

var (
	Level int = -1 // Pre initialization
	mu    sync.Mutex
)

func PrintE(l int, format string, args ...interface{}) string {

	mu.Lock()
	defer mu.Unlock()
	var msg string

	_, file, line, _ := runtime.Caller(1)
	file = filepath.Base(file)
	tag := fmt.Sprintf("[File: %s : Line: %d] ", file, line)

	if Level < 0 {
		Level = viper.GetInt(keys.DebugLevel)
	}
	if l <= viper.GetInt(keys.DebugLevel) {

		if len(args) != 0 && args != nil {
			msg = fmt.Sprintf(consts.RedError+format+tag+"\n", args...)
		} else {
			msg = fmt.Sprintf(consts.RedError + format + tag + "\n")
		}
		fmt.Print(msg)
		Write(msg, l)
	}

	return msg
}

func PrintS(l int, format string, args ...interface{}) string {

	mu.Lock()
	defer mu.Unlock()
	var msg string

	_, file, line, _ := runtime.Caller(1)
	file = filepath.Base(file)
	tag := fmt.Sprintf("[File: %s : Line: %d] ", file, line)

	if Level < 0 {
		Level = viper.GetInt(keys.DebugLevel)
	}
	if l <= viper.GetInt(keys.DebugLevel) {

		if len(args) != 0 && args != nil {
			msg = fmt.Sprintf(consts.GreenSuccess+format+tag+"\n", args...)
		} else {
			msg = fmt.Sprintf(consts.GreenSuccess + format + tag + "\n")
		}
		fmt.Print(msg)
		Write(msg, l)
	}
	return msg
}

func PrintD(l int, format string, args ...interface{}) string {

	mu.Lock()
	defer mu.Unlock()
	var msg string

	_, file, line, _ := runtime.Caller(1)
	file = filepath.Base(file)
	tag := fmt.Sprintf("[File: %s : Line: %d] ", file, line)

	if Level < 0 {
		Level = viper.GetInt(keys.DebugLevel)
	}
	if l <= viper.GetInt(keys.DebugLevel) && l != 0 { // Debug messages don't appear by default

		if len(args) != 0 && args != nil {
			msg = fmt.Sprintf(consts.YellowDebug+format+tag+"\n", args...)
		} else {
			msg = fmt.Sprintf(consts.YellowDebug + format + tag + "\n")
		}
		fmt.Print(msg)
		Write(msg, l)
	}
	return msg
}

func PrintI(format string, args ...interface{}) string {

	mu.Lock()
	defer mu.Unlock()
	var msg string

	if len(args) != 0 && args != nil {
		msg = fmt.Sprintf(consts.BlueInfo+format+"\n", args...)
	} else {
		msg = fmt.Sprintf(consts.BlueInfo + format + "\n")
	}
	fmt.Print(msg)
	Write(msg, 0)

	return msg
}

func Print(format string, args ...interface{}) string {

	mu.Lock()
	defer mu.Unlock()
	var msg string

	if len(args) != 0 && args != nil {
		msg = fmt.Sprintf(format+"\n", args...)
	} else {
		msg = fmt.Sprintf(format + "\n")
	}
	fmt.Print(msg)
	Write(msg, 0)

	return msg
}
package utils

import (
	"fmt"
	"log"
	"path/filepath"
	"regexp"
	"strings"
	"time"

	"gopkg.in/natefinch/lumberjack.v2"
)

var (
	ErrorArray []error
	Loggable   bool = false
	Logger     *log.Logger

	// Matches ANSI escape codes
	ansiEscape = regexp.MustCompile(`\x1b\[[0-9;]*m`)
)

// SetupLogging creates and/or opens the log file
func SetupLogging(targetDir string) error {

	logFile := &lumberjack.Logger{
		Filename:   filepath.Join(targetDir, "/metarr.log"), // Log file path
		MaxSize:    1,                                       // Max size in MB before rotation
		MaxBackups: 3,                                       // Number of backups to retain
		Compress:   true,                                    // Gzip compression
	}

	// Assign lumberjack logger to standard log output
	Logger = log.New(logFile, "", log.LstdFlags)
	Loggable = true

	Logger.Printf(":\n=========== %v ===========\n\n", time.Now().Format(time.RFC1123Z))
	return nil
}

// Write writes error information to the log file
func Write(msg string, level int) {
	// Do not add mutex, only called by callers which themselves use mutex
	if Loggable && level < 2 {
		if !strings.HasPrefix(msg, "\n") {
			msg += "\n"
		}
		Logger.Print(ansiEscape.ReplaceAllString(msg, ""))
	}
}

// WriteArray writes an array of error information to the log file
func WriteArray(msgs []string, args ...interface{}) {
	if Loggable {
		if len(msgs) != 0 {
			var msg string
			for i, entry := range msgs {
				switch i {
				case len(msgs) - 1:
					msg += fmt.Sprintf(entry, args...)
				default:
					msg += fmt.Sprintf(entry+", ", args...)
				}
				Logger.Print(ansiEscape.ReplaceAllString(msg, ""))
			}
		}
	}
}
package print

import (
	consts "Metarr/internal/domain/constants"
	"Metarr/internal/models"
	logging "Metarr/internal/utils/logging"
	"fmt"
	"reflect"
	"sync"
)

var muPrint sync.Mutex

// CreateModelPrintout prints out the values stored in a struct.
// taskName allows you to enter your own identifier for this task.
func CreateModelPrintout(model any, filename, taskName string, args ...interface{}) {
	muPrint.Lock()
	defer muPrint.Unlock()

	output := "\n\n================= " + consts.ColorCyan + "Printing metadata fields for:" + consts.ColorReset + " '" + consts.ColorReset + filename + "' =================\n"

	if taskName != "" {
		str := fmt.Sprintf("'"+taskName+"'", args...)
		output += "\n" + consts.ColorGreen + "Printing model at point of task " + consts.ColorReset + str + "\n"
	}

	// Add fields from the struct
	output += consts.ColorYellow + "\nFile Information:\n" + consts.ColorReset
	output += printStructFields(model)

	if m, ok := model.(*models.FileData); ok {
		output += consts.ColorYellow + "\nCredits:\n" + consts.ColorReset
		output += printStructFields(m.MCredits)

		output += consts.ColorYellow + "\nTitles and descriptions:\n" + consts.ColorReset
		output += printStructFields(m.MTitleDesc)

		output += consts.ColorYellow + "\nDates and timestamps:\n" + consts.ColorReset
		output += printStructFields(m.MDates)

		output += consts.ColorYellow + "\nWebpage data:\n" + consts.ColorReset
		output += printStructFields(m.MWebData)

		output += consts.ColorYellow + "\nShow data:\n" + consts.ColorReset
		output += printStructFields(m.MShowData)

		output += consts.ColorYellow + "\nOther data:\n" + consts.ColorReset
		output += printStructFields(m.MOther)
	} else if n, ok := model.(*models.NFOData); ok {
		output += consts.ColorYellow + "\nCredits:\n" + consts.ColorReset
		for _, actor := range n.Actors {
			output += printStructFields(actor.Name)
		}
		for _, director := range n.Directors {
			output += printStructFields(director)
		}
		for _, producer := range n.Producers {
			output += printStructFields(producer)
		}
		for _, publisher := range n.Publishers {
			output += printStructFields(publisher)
		}
		for _, studio := range n.Studios {
			output += printStructFields(studio)
		}
		for _, writer := range n.Writers {
			output += printStructFields(writer)
		}

		output += consts.ColorYellow + "\nTitles and descriptions:\n" + consts.ColorReset
		output += printStructFields(n.Title)
		output += printStructFields(n.Description)
		output += printStructFields(n.Plot)

		output += consts.ColorYellow + "\nWebpage data:\n" + consts.ColorReset
		output += printStructFields(n.WebpageInfo)

		output += consts.ColorYellow + "\nShow data:\n" + consts.ColorReset
		output += printStructFields(n.ShowInfo.Show)
		output += printStructFields(n.ShowInfo.EpisodeID)
		output += printStructFields(n.ShowInfo.EpisodeTitle)
		output += printStructFields(n.ShowInfo.SeasonNumber)
	}

	output += "\n\n================= " + consts.ColorYellow + "End metadata fields for:" + consts.ColorReset + " '" + filename + "' =================\n\n"

	logging.Print(output)
}

// Function to print the fields of a struct using reflection
func printStructFields(s interface{}) string {
	val := reflect.ValueOf(s)

	// Dereference pointer
	if val.Kind() == reflect.Ptr {
		val = val.Elem()
	}

	if val.Kind() != reflect.Struct {
		return fmt.Sprintf("Expected a struct, got %s\n", val.Kind())
	}

	typ := val.Type()
	output := ""

	for i := 0; i < val.NumField(); i++ {
		field := typ.Field(i)      // Get field metadata
		fieldValue := val.Field(i) // Get field value

		// Skip zero or empty fields
		if fieldValue.IsZero() {
			output += field.Name + consts.ColorRed + " [empty]\n" + consts.ColorReset
			continue
		}

		fieldName := field.Name
		fieldValueStr := fmt.Sprintf("%v", fieldValue.Interface()) // Convert the value to a string

		// Append the field name and value in key-value format
		output += fmt.Sprintf("%s: %s\n", fieldName, fieldValueStr)
	}

	return output
}

// Print out the fetched fields
func PrintGrabbedFields(fieldType string, p *map[string]string) {

	printMap := *p

	muPrint.Lock()
	defer muPrint.Unlock()

	fmt.Println()
	logging.PrintI("Found and stored %s metadata fields from metafile:", fieldType)
	fmt.Println()

	for printKey, printVal := range printMap {
		if printKey != "" && printVal != "" {
			fmt.Printf(consts.ColorGreen + "Key: " + consts.ColorReset + printKey + consts.ColorYellow + "\nValue: " + consts.ColorReset + printVal + "\n")
		}
	}
	fmt.Println()
}
package utils

import (
	logging "Metarr/internal/utils/logging"
	"bufio"
	"context"
	"fmt"
	"os"
	"strings"
)

var (
	userInputChan = make(chan string) // Channel for user input
	decisionMade  bool
)

// InitUserInputReader initializes a user input reading function in a goroutine
func InitUserInputReader() {
	go func() {
		reader := bufio.NewReader(os.Stdin)
		for {
			input, _ := reader.ReadString('\n')
			userInputChan <- strings.TrimSpace(input)
		}
	}()
}

// PromptMetaReplace displays a prompt message and waits for valid user input.
// The option can be used to tell the program to overwrite all in the queue,
// preserve all in the queue, or move through value by value
func PromptMetaReplace(promptMsg string, ow, ps bool) (string, error) {

	logging.PrintD(3, "Entering PromptUser dialogue...")
	ctx := context.Background()

	if decisionMade {
		// If overwriteAll, return "Y" without waiting
		if ow {

			logging.PrintD(3, "Overwrite all is set...")
			return "Y", nil
		} else if ps {

			logging.PrintD(3, "Preserve all is set...")
			return "N", nil
		}
	}

	fmt.Println()
	logging.PrintI(promptMsg)

	// Wait for user input
	select {
	case response := <-userInputChan:
		if response == "Y" {
			ow = true
		}
		decisionMade = true
		return response, nil

	case <-ctx.Done():
		logging.PrintI("Operation canceled during input.")
		return "", fmt.Errorf("operation canceled")
	}
}
package main

import (
	"Metarr/internal/config"
	keys "Metarr/internal/domain/keys"
	"Metarr/internal/processing"
	fsRead "Metarr/internal/utils/fs/read"
	logging "Metarr/internal/utils/logging"
	prompt "Metarr/internal/utils/prompt"
	"context"
	"fmt"
	"log"
	"os"
	"os/signal"
	"path/filepath"
	"runtime/pprof"
	"runtime/trace"
	"sync"
	"syscall"
	"time"
)

var startTime time.Time

func init() {
	startTime = time.Now()
	logging.PrintI("Metarr started at: %v", startTime.Format("2006-01-02 15:04:05.00 MST"))
}

func main() {

	var (
		err       error
		directory string
	)

	// TESTING FUNCTIONS
	if config.GetBool(keys.Benchmarking) {
		// CPU profile
		cpuFile, err := os.Create("cpu.prof")
		if err != nil {
			log.Fatal("could not create CPU profile: ", err)
		}
		defer cpuFile.Close() // Don't forget to close the file
		if err := pprof.StartCPUProfile(cpuFile); err != nil {
			log.Fatal("could not start CPU profile: ", err)
		}
		defer pprof.StopCPUProfile()

		// Memory profile
		memFile, err := os.Create("mem.prof")
		if err != nil {
			log.Fatal("could not create memory profile: ", err)
		}
		defer memFile.Close()
		defer func() {
			if config.GetBool(keys.Benchmarking) {
				if err := pprof.WriteHeapProfile(memFile); err != nil {
					log.Fatal("could not write memory profile: ", err)
				}
			}
		}()

		// Trace
		traceFile, err := os.Create("trace.out")
		if err != nil {
			log.Fatal("could not create trace file: ", err)
		}
		defer traceFile.Close()
		if err := trace.Start(traceFile); err != nil {
			log.Fatal("could not start trace: ", err)
		}
		defer trace.Stop()
	}
	// END OF TESTING FUNCTIONS: MEM TEST WRITE AT BOTTOM

	if err := config.Execute(); err != nil {
		fmt.Fprintln(os.Stderr, err)
		fmt.Println()
		os.Exit(1)
	}

	if !config.GetBool("execute") {
		fmt.Println()
		logging.PrintI(`(Separate fields supporting multiple entries by commas with no spaces e.g. "title:example,date:20240101")`)
		fmt.Println()
		return // Exit early if not meant to execute
	}

	// Handle cleanup on interrupt or termination signals
	ctx, cancel := context.WithCancel(context.Background())
	config.Set(keys.Context, ctx)
	defer cancel()

	var (
		inputVideoDir,
		inputVideo string

		openVideo *os.File
	)
	if config.IsSet(keys.VideoDir) {

		inputVideoDir = config.GetString(keys.VideoDir)
		openVideo, err = os.Open(inputVideoDir)
		if err != nil {
			logging.PrintE(0, "Error: %v", err)
			os.Exit(1)
		}
		defer openVideo.Close()
		directory = inputVideoDir

	} else if config.IsSet(keys.VideoFile) {

		inputVideo = config.GetString(keys.VideoFile)
		openVideo, err = os.Open(inputVideo)
		if err != nil {
			logging.PrintE(0, "Error: %v", err)
			os.Exit(1)
		}
		defer openVideo.Close()
		directory = filepath.Dir(inputVideo)
	}
	config.Set(keys.OpenVideo, openVideo)

	var (
		inputMetaDir,
		inputMeta string

		openJson *os.File
	)
	if config.IsSet(keys.JsonDir) {

		inputMetaDir = config.GetString(keys.JsonDir)
		openJson, err = os.Open(inputMetaDir)
		if err != nil {
			logging.PrintE(0, "Error: %v", err)
			os.Exit(1)
		}
		defer openJson.Close()
		if directory == "" {
			directory = inputMetaDir
		}

	} else if config.IsSet(keys.JsonFile) {

		inputMeta = config.GetString(keys.JsonFile)
		openJson, err = os.Open(inputMeta)
		if err != nil {
			logging.PrintE(0, "Error: %v", err)
			os.Exit(1)
		}
		defer openJson.Close()
		if directory == "" {
			directory = filepath.Dir(inputMeta)
		}
	}
	config.Set(keys.OpenJson, openJson)

	// Setup logging
	if directory != "" {
		err = logging.SetupLogging(directory)
		if err != nil {
			fmt.Printf("\n\nNotice: Log file was not created\nReason: %s\n\n", err)
		}
	} else {
		logging.PrintI("Directory and file strings were entered empty. Exiting...")
		os.Exit(1)
	}

	if err := fsRead.InitFetchFilesVars(); err != nil {
		logging.PrintE(0, "Failed to initialize variables to fetch files. Exiting...")
		os.Exit(1)
	}

	// Program control
	var wg sync.WaitGroup
	config.Set(keys.WaitGroup, &wg)

	cleanupChan := make(chan os.Signal, 1)
	signal.Notify(cleanupChan, syscall.SIGINT, syscall.SIGTERM)

	// Removed: prompt.SetMetaOverwritePreserve(config.GetBool(keys.MOverwrite), config.GetBool(keys.MPreserve))
	prompt.InitUserInputReader()

	// Proceed to process files (videos, metadata files, etc...)
	processing.ProcessFiles(ctx, cancel, &wg, cleanupChan, openVideo, openJson)

	endTime := time.Now()
	logging.PrintI("Metarr finished at: %v", endTime.Format("2006-01-02 15:04:05.00 MST"))
	logging.PrintI("Time elapsed: %.2f seconds", endTime.Sub(startTime).Seconds())
}
